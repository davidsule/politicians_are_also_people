{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn_extra.cluster import KMedoids\n",
    "import math\n",
    "import word_dist_script as ws\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading finished\n"
     ]
    }
   ],
   "source": [
    "twitEmbs = ws.load_twitter_embs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading word2vec model...\n",
      "This can take several minutes...\n"
     ]
    }
   ],
   "source": [
    "load_word2vec = True\n",
    "\n",
    "if load_word2vec:\n",
    "    word2vec = ws.load_word2vec_embs()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# every_emb = ws.get_every_embeddings(word2vec)\n",
    "# clf = KMeans(n_clusters=5, random_state=0)\n",
    "\n",
    "# clf_kmoids = KMeans(n_clusters=5, random_state=0, metric='precomputed', method='pam')\n",
    "\n",
    "# clf_kmoids.fit(every_emb)\n",
    "\n",
    "# pred_emb = clf_kmoids.predict(every_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "every_dist = ws.get_every_distance(word2vec)\n",
    "import random\n",
    "random.seed(4012)\n",
    "\n",
    "clf_kmoids = KMedoids(n_clusters=5, random_state=4012, metric='precomputed', method='pam')\n",
    "\n",
    "\n",
    "clf_kmoids.fit(every_dist)\n",
    "\n",
    "pred_dist = clf_kmoids.predict(every_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 3, 4, 1, 0, 1, 3, 1, 2, 2, 2, 3, 1, 3, 0, 0, 1, 0, 1, 0, 0,\n",
       "       1, 2, 0, 2, 2, 3, 0, 3, 4, 4, 0, 4, 1, 4, 2], dtype=int64)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_dist = ws.sort_zip_labels(pred_dist)\n",
    "groups_dist = ws.group_by_cluster(sorted_dist)\n",
    "\n",
    "# sorted_emb = ws.sort_zip_labels(pred_emb)\n",
    "# groups_emb = ws.group_by_cluster(sorted_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 ['album', 'band', 'genre', 'location', 'metrics', 'artist', 'instrument', 'poem', 'java', 'song']\n",
      "1 ['journal', 'award', 'book', 'conference', 'event', 'magazine', 'misc', 'Organisation', 'theory']\n",
      "2 ['country', 'discipline', 'election', 'person', 'politics', 'politician', 'writer']\n",
      "3 ['algorithm', 'chemical', 'enzyme', 'field', 'product', 'protein']\n",
      "4 ['astronomer', 'researcher', 'scientist', 'task', 'university']\n"
     ]
    }
   ],
   "source": [
    "for i, group in enumerate(groups_dist):\n",
    "    print(i, group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'groups_emb' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[39mfor\u001b[39;00m i, group \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(groups_emb):\n\u001b[0;32m      2\u001b[0m     \u001b[39mprint\u001b[39m(i, group)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'groups_emb' is not defined"
     ]
    }
   ],
   "source": [
    "# for i, group in enumerate(groups_emb):\n",
    "#     print(i, group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "genre ['album', 'band', 'genre', 'location', 'metrics', 'artist', 'instrument', 'poem', 'java', 'song']\n",
      "magazine ['journal', 'award', 'book', 'conference', 'event', 'magazine', 'misc', 'Organisation', 'theory']\n",
      "politician ['country', 'discipline', 'election', 'person', 'politics', 'politician', 'writer']\n",
      "enzyme ['algorithm', 'chemical', 'enzyme', 'field', 'product', 'protein']\n",
      "astronomer ['astronomer', 'researcher', 'scientist', 'task', 'university']\n"
     ]
    }
   ],
   "source": [
    "for group in groups_dist:\n",
    "    ## find distance to centroid\n",
    "    label = ws.find_label_for_cluster(word2vec, group)\n",
    "    print(label, group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "event ['award', 'conference', 'event', 'location']\n",
      "country ['band', 'country', 'discipline', 'election', 'field', 'genre', 'misc', 'instrument', 'Organisation', 'politics', 'researcher', 'task', 'university']\n",
      "person ['astronomer', 'artist', 'person', 'politician', 'scientist']\n",
      "book ['journal', 'album', 'book', 'magazine', 'poem', 'song', 'theory', 'writer']\n",
      "product ['algorithm', 'chemical', 'enzyme', 'metrics', 'product', 'java', 'protein']\n"
     ]
    }
   ],
   "source": [
    "# for group in groups_emb:\n",
    "#     ## find distance to centroid\n",
    "#     label = ws.find_label_for_cluster_emb(word2vec, group)\n",
    "#     print(label, group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('event', 0.687100887298584)\n",
      "('Lisa_Vorderbrueggen_covers', 0.5613850355148315)\n",
      "('scientist', 0.7774226069450378)\n",
      "('poem', 0.7118237018585205)\n",
      "('enzyme', 0.7614861726760864)\n"
     ]
    }
   ],
   "source": [
    "# for group in groups_emb:\n",
    "#     # find most similar word\n",
    "#     centroid = ws.find_vector_for_cluster(word2vec, group)\n",
    "#     most_sim = word2vec.most_similar(centroid)[0]\n",
    "#     print(most_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity2cluster_dist = ws.create_entity2cluster_dict(pred_dist)\n",
    "entity2cluster_emb = ws.create_entity2cluster_dict(pred_emb)\n",
    "\n",
    "# cluster2label_dist = ws.create_cluster2label_dict(word2vec, pred_dist)\n",
    "# cluster2label_emb = ws.create_cluster2label_dict(word2vec, pred_emb)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mapping entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load csv\n",
    "df = pd.read_csv('../data/category_mappings.csv')\n",
    "## get entity names\n",
    "entities = df[\"entity_name\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_problematic_words(word2vec_model, entities):\n",
    "    PROBLEMATIC_WORDS = []\n",
    "    for entity in entities:\n",
    "        try:\n",
    "            word2vec_model[entity]\n",
    "        except:\n",
    "            PROBLEMATIC_WORDS.append(entity)\n",
    "    return PROBLEMATIC_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## find the problematic words\n",
    "problem_words = find_problematic_words(word2vec_model=word2vec, entities=entities)\n",
    "problem2work = {\"musicalartist\": \"artist\", \"organisation\": \"Organisation\", \"politicalparty\": \"politics\", \"academicjournal\": \"journal\", \"chemicalcompound\": \"chemical\", \"chemicalelement\": \"chemical\", \"astronomicalobject\": \"astronomer\", \"musicgenre\": \"genre\", \"literarygenre\": \"genre\", \"programlang\": \"java\", \"musicalinstrument\": \"instrument\"  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## map the problematic entities to working ones\n",
    "working = entities.map(lambda entity: problem2work[entity] if entity in problem2work else entity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## map the words to their cluster(word) label\n",
    "mapped_words_dist = working.map(lambda entity: cluster2label_dist[entity2cluster_dist[entity]])\n",
    "# mapped_words_emb = working.map(lambda entity: cluster2label_emb[entity2cluster_emb[entity]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## add the new columns to the dataframe\n",
    "df[\"word_dist\"] = mapped_words_dist\n",
    "# df[\"word_emb\"] = mapped_words_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## write to csv\n",
    "df.to_csv('../data/category_mappings.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = pd.read_csv(\"../data/category_mappings.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'organisation'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file[\"ours\"][\"politicalparty\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_to_use = file[\"ours\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "syp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
