{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn_extra.cluster import KMedoids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading finished\n"
     ]
    }
   ],
   "source": [
    "import gensim.models\n",
    "from gensim.models import Word2Vec\n",
    "######### download the twitter.bin file from google drive and place it in the same folder (word_dist) ###########\n",
    "######### https://drive.google.com/drive/folders/12vBvFPjpHx3gwkaCeuBf5MjBHdAnyk4N ###########\n",
    "twitEmbs = gensim.models.KeyedVectors.load_word2vec_format(\n",
    "                                'twitter.bin', binary=True)\n",
    "print('loading finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "from  gensim import downloader\n",
    "## This will take a very long time to download\n",
    "Word2Vec_google_news = downloader.load('word2vec-google-news-300')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Nicklas\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENTITY_LABELS = \"journal album algorithm astronomer award band book chemical conference country discipline election enzyme event field genre location magazine metrics misc artist instrument Organisation person poem politics politician product java protein researcher scientist song task theory university writer\"\n",
    "ENTITY_LABELS_SPLIT = ENTITY_LABELS.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance_to_every_word(word, ENTITY_LABELS_SPLIT):\n",
    "    \n",
    "    distance_list = []\n",
    "    \n",
    "    for i in range(len(ENTITY_LABELS_SPLIT)):\n",
    "        distance = Word2Vec_google_news.distance(word, ENTITY_LABELS_SPLIT[i])\n",
    "        distance_list.append(distance)\n",
    "    \n",
    "    return distance_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_every_distance(ENTITY_LABELS_SPLIT):\n",
    "    \n",
    "    every_distance = []\n",
    "    for current_word in ENTITY_LABELS_SPLIT:\n",
    "        distance_list = distance_to_every_word(current_word, ENTITY_LABELS_SPLIT)\n",
    "        every_distance.append(distance_list)\n",
    "    return every_distance    \n",
    "\n",
    "def get_every_embeddings(ENTITY_LABELS_SPLIT):\n",
    "        \n",
    "        embeddings = []\n",
    "        for current_word in ENTITY_LABELS_SPLIT:\n",
    "            embedding = Word2Vec_google_news[current_word]\n",
    "            embeddings.append(embedding)\n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 4, 1, 3, 1, 4, 4, 0, 3, 3, 1, 3, 0, 2, 3, 4, 2, 3, 1, 1, 4, 2,\n",
       "       3, 2, 4, 3, 2, 1, 1, 0, 3, 3, 4, 2, 3, 3, 2], dtype=int64)"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "every_dist = get_every_distance(ENTITY_LABELS_SPLIT)\n",
    "every_emb = get_every_embeddings(ENTITY_LABELS_SPLIT)\n",
    "\n",
    "clf_kmedoids = KMedoids(n_clusters=5)\n",
    "\n",
    "clf_kmedoids.fit(every_dist)\n",
    "\n",
    "pred_kmediods = clf_kmedoids.predict(every_dist)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_using_distance(ENTITY_LABELS_SPLIT, n_clusters=5):\n",
    "        \n",
    "    ## make classifier\n",
    "    clf = KMeans(n_clusters=n_clusters, random_state=0)\n",
    "    ## get distances\n",
    "    every_distance = get_every_distance(ENTITY_LABELS_SPLIT)\n",
    "    ## fit classifier\n",
    "    clf.fit(every_distance)\n",
    "    ## predict labels\n",
    "    predicted_labels = []\n",
    "    for i,current_word in enumerate(ENTITY_LABELS_SPLIT):\n",
    "        current_dist = every_distance[i]\n",
    "        predicted_label = clf.predict([current_dist])\n",
    "        predicted_labels.append(predicted_label)\n",
    "    return predicted_labels\n",
    "\n",
    "def predict_using_embeddings(ENTITY_LABELS_SPLIT, n_clusters=5):\n",
    "        \n",
    "    ## make classifier\n",
    "    clf = KMeans(n_clusters=n_clusters, random_state=0)\n",
    "    ## get embeddings\n",
    "    every_embeddings = get_every_embeddings(ENTITY_LABELS_SPLIT)\n",
    "    ## fit classifier\n",
    "    clf.fit(every_embeddings)\n",
    "    ## predict labels\n",
    "    predicted_labels = []\n",
    "    for i,current_word in enumerate(ENTITY_LABELS_SPLIT):\n",
    "        current_emb = every_embeddings[i]\n",
    "        predicted_label = clf.predict([current_emb])\n",
    "        predicted_labels.append(predicted_label)\n",
    "    return predicted_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_zip_labels(ENTITY_LABELS_SPLIT, predictions):\n",
    "    zipped = zip(ENTITY_LABELS_SPLIT, predictions)\n",
    "    sorted_zipped = sorted(zipped, key=lambda x: x[1])\n",
    "    return sorted_zipped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Nicklas\\anaconda3\\envs\\syp\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1332: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Nicklas\\anaconda3\\envs\\syp\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1332: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('algorithm', array([0])),\n",
       " ('award', array([0])),\n",
       " ('book', array([0])),\n",
       " ('conference', array([0])),\n",
       " ('country', array([0])),\n",
       " ('event', array([0])),\n",
       " ('field', array([0])),\n",
       " ('location', array([0])),\n",
       " ('magazine', array([0])),\n",
       " ('metrics', array([0])),\n",
       " ('misc', array([0])),\n",
       " ('instrument', array([0])),\n",
       " ('Organisation', array([0])),\n",
       " ('person', array([0])),\n",
       " ('product', array([0])),\n",
       " ('java', array([0])),\n",
       " ('task', array([0])),\n",
       " ('election', array([1])),\n",
       " ('politics', array([1])),\n",
       " ('politician', array([1])),\n",
       " ('journal', array([2])),\n",
       " ('discipline', array([2])),\n",
       " ('theory', array([2])),\n",
       " ('university', array([2])),\n",
       " ('album', array([3])),\n",
       " ('band', array([3])),\n",
       " ('genre', array([3])),\n",
       " ('artist', array([3])),\n",
       " ('poem', array([3])),\n",
       " ('song', array([3])),\n",
       " ('writer', array([3])),\n",
       " ('chemical', array([4])),\n",
       " ('enzyme', array([4])),\n",
       " ('protein', array([4])),\n",
       " ('astronomer', array([5])),\n",
       " ('researcher', array([5])),\n",
       " ('scientist', array([5]))]"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_labels_dist = predict_using_distance(ENTITY_LABELS_SPLIT, n_clusters=6)\n",
    "predicted_labels_emb = predict_using_embeddings(ENTITY_LABELS_SPLIT, n_clusters=6)\n",
    "\n",
    "sorted_zipped_dist = sort_zip_labels(ENTITY_LABELS_SPLIT, predicted_labels_dist)\n",
    "sorted_zipped_emb = sort_zip_labels(ENTITY_LABELS_SPLIT, predicted_labels_emb)\n",
    "\n",
    "sorted_zipped_emb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "syp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
