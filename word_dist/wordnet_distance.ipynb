{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/davidsule/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from typing import Union\n",
    "\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENTITY_LABELS = \"journal album algorithm astronomer award band book chemical conference country discipline election enzyme event field genre location magazine metrics miscellaneous artist instrument Organisation person poem politics politician product java protein researcher scientist song task theory university writer\"\n",
    "ENTITY_LABELS_SPLIT = ENTITY_LABELS.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_frequent_synset(entities: list) -> list:\n",
    "    \"\"\"Return list of most frequent synset for each word in entities.\"\"\"\n",
    "    return list(map(lambda w: wordnet.synsets(w)[0], entities))\n",
    "\n",
    "def minpath(synset) -> list:\n",
    "    \"\"\"Return the path with the minimum length to the root.\"\"\"\n",
    "    hypernym_paths = synset.hypernym_paths()\n",
    "    minlen = len(hypernym_paths[0])\n",
    "    minlen_idx = 0\n",
    "    for idx, path in enumerate(hypernym_paths):\n",
    "        if len(path) < minlen:\n",
    "            minlen = len(path)\n",
    "            minlen_idx = idx\n",
    "    return hypernym_paths[minlen_idx]\n",
    "\n",
    "def category_dict(synset_list: list, level: int = 2, words: Union[list, None] = None) -> dict:\n",
    "    \"\"\"Create dictionary with desired level of hypernyms as keys and\n",
    "    synsets as values.  Optionally pass the corresponding list of words\n",
    "    to return the words as values.\n",
    "\n",
    "    The function searches for all the possible paths to the root and\n",
    "    chooses the shortes among those for each synset in synset_list.  The\n",
    "    level means the distance from the root in the path of hypernyms;\n",
    "    level 0 is the root.  If the path for a synset is shorter than the\n",
    "    selected level, the last level is returned.\n",
    "\n",
    "    Each hypernym category is a key in the returned dictionary, whose\n",
    "    keys are the synsets belonging to it from synset_list (or the\n",
    "    corresponding words if words in not None).\n",
    "    \"\"\"\n",
    "    categories = {}\n",
    "    for i, syn in enumerate(synset_list):\n",
    "        path = minpath(syn)\n",
    "        try:\n",
    "            cat = path[level]\n",
    "        except:\n",
    "            cat = path[-1]\n",
    "        if cat in categories:\n",
    "            if words is not None:\n",
    "                categories[cat].append(words[i])\n",
    "            else:\n",
    "                categories[cat].append(syn)\n",
    "        else:\n",
    "            if words is not None:\n",
    "                categories[cat] = [words[i]]\n",
    "            else:\n",
    "                categories[cat] = [syn]\n",
    "    return categories\n",
    "\n",
    "# Get categories with most frequent word meanings\n",
    "# category_dict(most_frequent_synset(ENTITY_LABELS_SPLIT), 2, ENTITY_LABELS_SPLIT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36 writer\n",
      "0 Synset('writer.n.01') writes (books or stories or articles or the like) professionally (for pay)\n",
      "1 Synset('writer.n.02') a person who is able to write and has written something\n"
     ]
    }
   ],
   "source": [
    "# Check meanings and definitions for manual correction\n",
    "idx = 36\n",
    "synlist = wordnet.synsets(ENTITY_LABELS_SPLIT[idx])\n",
    "print(idx, ENTITY_LABELS_SPLIT[idx])\n",
    "for i, syn in enumerate(synlist):\n",
    "    print(i, syn, syn.definition())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manual corrections of meanings. Key: idx in Entity_labels, value: idx of meaning in wordnet.synsets(word)\n",
    "# ENTITY_LABELS = \"journal album algorithm astronomer award band book chemical conference country discipline election enzyme event field genre location magazine metrics miscellaneous artist instrument Organisation person poem politics politician product java protein researcher scientist song task theory university writer\"\n",
    "meaning_dict = {0: 1, 1: 0, 2: 0, 3: 0, 4: 1, 5: 0, 6: 0, 7: 0, 8: 0, 9: 0, 10: 0, 11: 0, 12: 0, 13: 0, 14: 3, 15: 2, 16: 0, 17: 0, 18: 3, 19: 0, 20: 0, 21: 5, 22: 0, 23: 0, 24: 0, 25: 0, 26: 0, 27: 0, 28: 2, 29: 0, 30: 0, 31: 0, 32: 0, 33: 1, 34: 0, 35: 2, 36: 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{Synset('object.n.01'): ['journal',\n",
       "  'album',\n",
       "  'book',\n",
       "  'location',\n",
       "  'magazine',\n",
       "  'instrument',\n",
       "  'product'],\n",
       " Synset('psychological_feature.n.01'): ['algorithm',\n",
       "  'discipline',\n",
       "  'election',\n",
       "  'event',\n",
       "  'field',\n",
       "  'task',\n",
       "  'theory'],\n",
       " Synset('causal_agent.n.01'): ['astronomer',\n",
       "  'artist',\n",
       "  'person',\n",
       "  'politician',\n",
       "  'researcher',\n",
       "  'scientist',\n",
       "  'writer'],\n",
       " Synset('communication.n.02'): ['award', 'genre', 'poem', 'java', 'song'],\n",
       " Synset('group.n.01'): ['band',\n",
       "  'conference',\n",
       "  'country',\n",
       "  'Organisation',\n",
       "  'university'],\n",
       " Synset('matter.n.03'): ['chemical', 'enzyme'],\n",
       " Synset('measure.n.02'): ['metrics'],\n",
       " Synset('assorted.s.01'): ['miscellaneous'],\n",
       " Synset('relation.n.01'): ['politics'],\n",
       " Synset('thing.n.12'): ['protein']}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get categories with associated words\n",
    "ENTITY_SYNS = []\n",
    "for i, word in enumerate(ENTITY_LABELS_SPLIT):\n",
    "    ENTITY_SYNS.append(wordnet.synsets(word)[meaning_dict[i]])\n",
    "category_dict(ENTITY_SYNS, 2, ENTITY_LABELS_SPLIT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Wu-Palmer Similarities\n",
    "def get_wup_sim(entity_syns):\n",
    "    \"\"\"Get pairwise Wu-Palmer Similarities of the synsets from the list\n",
    "    in the argument.  Returns a NumPy Array of shape (len(entity_syns),\n",
    "    len(entity_syns)).\n",
    "    \"\"\"\n",
    "    similarities = np.zeros((len(entity_syns), len(entity_syns)))\n",
    "    for i, syn1 in enumerate(entity_syns):\n",
    "        for j, syn2 in enumerate(entity_syns):\n",
    "            if j > i:\n",
    "                continue\n",
    "            sim = syn1.wup_similarity(syn2)\n",
    "            similarities[i, j] = sim\n",
    "            similarities[j, i] = sim\n",
    "    return similarities\n",
    "\n",
    "similarities = get_wup_sim(ENTITY_SYNS)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "2yp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
