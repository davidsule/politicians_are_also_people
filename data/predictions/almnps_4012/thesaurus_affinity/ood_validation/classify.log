Saved arguments to data/predictions/almnps_4012/thesaurus_affinity/ood_validation/args.json.
loading projection weights from /home/davidsule/gensim-data/word2vec-google-news-300/word2vec-google-news-300.gz
KeyedVectors lifecycle event {'msg': 'loaded (3000000, 300) matrix of type float32 from /home/davidsule/gensim-data/word2vec-google-news-300/word2vec-google-news-300.gz', 'binary': True, 'encoding': 'utf8', 'datetime': '2023-05-14T02:02:50.447476', 'gensim': '4.3.0', 'python': '3.9.16 | packaged by conda-forge | (main, Feb  1 2023, 21:39:03) \n[GCC 11.3.0]', 'platform': 'Linux-5.15.90.1-microsoft-standard-WSL2-x86_64-with-glibc2.31', 'event': 'load_word2vec_format'}
Saved category mapping to data/predictions/almnps_4012/thesaurus_affinity/ood_validation/entity2category_mapping.json.
Loaded category mapping: thesaurus_affinity.
Loaded <torch.utils.data.dataloader.DataLoader object at 0x7f62b5be59d0> (train).
Loaded <torch.utils.data.dataloader.DataLoader object at 0x7f62b5be53a0> (dev).
Starting training on ['literature', 'music', 'news', 'politics', 'science'] data with validation domain ai.
Read data from folder data/crossre_data/
Loaded <TransformerEmbeddings: dim=768>.
Using classifier: <LinearClassifier: emb_model = <TransformerEmbeddings: dim=768>>
Using criterion: <LabelLoss: loss=XEnt>.
Optimizing using: AdamW with learning rate 2e-05.
[Epoch 1/50] Train completed with Micro-f1: 0.4154, Macro-f1: 0.2391, Weighted-f1: 0.3460, Loss: 1.9626
[Epoch 1/50] Evaluation completed with Micro-f1: 0.5326, Macro-f1: 0.3620, Weighted-f1: 0.4808, Loss: 1.6497
Saved models from epoch 1 to 'data/predictions/almnps_4012/thesaurus_affinity/ood_validation/ai/newest.pt'.
Saved model with best loss 1.6497 to 'data/predictions/almnps_4012/thesaurus_affinity/ood_validation/ai/best.pt'.
[Epoch 2/50] Train completed with Micro-f1: 0.7448, Macro-f1: 0.6052, Weighted-f1: 0.7140, Loss: 0.9634
[Epoch 2/50] Evaluation completed with Micro-f1: 0.5515, Macro-f1: 0.3866, Weighted-f1: 0.5037, Loss: 1.4647
Saved models from epoch 2 to 'data/predictions/almnps_4012/thesaurus_affinity/ood_validation/ai/newest.pt'.
Saved model with best loss 1.4647 to 'data/predictions/almnps_4012/thesaurus_affinity/ood_validation/ai/best.pt'.
[Epoch 3/50] Train completed with Micro-f1: 0.8670, Macro-f1: 0.7821, Weighted-f1: 0.8514, Loss: 0.5670
[Epoch 3/50] Evaluation completed with Micro-f1: 0.5596, Macro-f1: 0.3956, Weighted-f1: 0.5145, Loss: 1.4027
Saved models from epoch 3 to 'data/predictions/almnps_4012/thesaurus_affinity/ood_validation/ai/newest.pt'.
Saved model with best loss 1.4027 to 'data/predictions/almnps_4012/thesaurus_affinity/ood_validation/ai/best.pt'.
[Epoch 4/50] Train completed with Micro-f1: 0.9231, Macro-f1: 0.8569, Weighted-f1: 0.9181, Loss: 0.3430
[Epoch 4/50] Evaluation completed with Micro-f1: 0.5694, Macro-f1: 0.4071, Weighted-f1: 0.5280, Loss: 1.4163
Saved models from epoch 4 to 'data/predictions/almnps_4012/thesaurus_affinity/ood_validation/ai/newest.pt'.
[Epoch 5/50] Train completed with Micro-f1: 0.9584, Macro-f1: 0.9234, Weighted-f1: 0.9543, Loss: 0.2066
[Epoch 5/50] Evaluation completed with Micro-f1: 0.5759, Macro-f1: 0.4172, Weighted-f1: 0.5382, Loss: 1.5386
Saved models from epoch 5 to 'data/predictions/almnps_4012/thesaurus_affinity/ood_validation/ai/newest.pt'.
[Epoch 6/50] Train completed with Micro-f1: 0.9782, Macro-f1: 0.9596, Weighted-f1: 0.9769, Loss: 0.1335
[Epoch 6/50] Evaluation completed with Micro-f1: 0.5804, Macro-f1: 0.4246, Weighted-f1: 0.5453, Loss: 1.5986
Saved models from epoch 6 to 'data/predictions/almnps_4012/thesaurus_affinity/ood_validation/ai/newest.pt'.
No improvement since 3 epochs (1.4027 loss). Early stop.
OOD training completed for test topic ai after 6 epochs.
Loaded <torch.utils.data.dataloader.DataLoader object at 0x7f629c3962b0> (train).
Loaded <torch.utils.data.dataloader.DataLoader object at 0x7f62b5be5670> (dev).
Starting training on ['ai', 'music', 'news', 'politics', 'science'] data with validation domain literature.
Read data from folder data/crossre_data/
Loaded <TransformerEmbeddings: dim=768>.
Using classifier: <LinearClassifier: emb_model = <TransformerEmbeddings: dim=768>>
Using criterion: <LabelLoss: loss=XEnt>.
Optimizing using: AdamW with learning rate 2e-05.
[Epoch 1/50] Train completed with Micro-f1: 0.4037, Macro-f1: 0.2575, Weighted-f1: 0.3448, Loss: 2.0324
[Epoch 1/50] Evaluation completed with Micro-f1: 0.4705, Macro-f1: 0.3229, Weighted-f1: 0.4286, Loss: 1.7450
Saved models from epoch 1 to 'data/predictions/almnps_4012/thesaurus_affinity/ood_validation/literature/newest.pt'.
Saved model with best loss 1.7450 to 'data/predictions/almnps_4012/thesaurus_affinity/ood_validation/literature/best.pt'.
[Epoch 2/50] Train completed with Micro-f1: 0.7068, Macro-f1: 0.5928, Weighted-f1: 0.6803, Loss: 1.0691
[Epoch 2/50] Evaluation completed with Micro-f1: 0.5061, Macro-f1: 0.3547, Weighted-f1: 0.4690, Loss: 1.4756
Saved models from epoch 2 to 'data/predictions/almnps_4012/thesaurus_affinity/ood_validation/literature/newest.pt'.
Saved model with best loss 1.4756 to 'data/predictions/almnps_4012/thesaurus_affinity/ood_validation/literature/best.pt'.
[Epoch 3/50] Train completed with Micro-f1: 0.8342, Macro-f1: 0.7293, Weighted-f1: 0.8225, Loss: 0.6371
[Epoch 3/50] Evaluation completed with Micro-f1: 0.5263, Macro-f1: 0.3783, Weighted-f1: 0.4949, Loss: 1.4290
Saved models from epoch 3 to 'data/predictions/almnps_4012/thesaurus_affinity/ood_validation/literature/newest.pt'.
Saved model with best loss 1.4290 to 'data/predictions/almnps_4012/thesaurus_affinity/ood_validation/literature/best.pt'.
[Epoch 4/50] Train completed with Micro-f1: 0.9137, Macro-f1: 0.8614, Weighted-f1: 0.9033, Loss: 0.3929
[Epoch 4/50] Evaluation completed with Micro-f1: 0.5367, Macro-f1: 0.3919, Weighted-f1: 0.5085, Loss: 1.4609
Saved models from epoch 4 to 'data/predictions/almnps_4012/thesaurus_affinity/ood_validation/literature/newest.pt'.
[Epoch 5/50] Train completed with Micro-f1: 0.9542, Macro-f1: 0.9169, Weighted-f1: 0.9494, Loss: 0.2538
[Epoch 5/50] Evaluation completed with Micro-f1: 0.5460, Macro-f1: 0.4024, Weighted-f1: 0.5199, Loss: 1.4632
Saved models from epoch 5 to 'data/predictions/almnps_4012/thesaurus_affinity/ood_validation/literature/newest.pt'.
[Epoch 6/50] Train completed with Micro-f1: 0.9737, Macro-f1: 0.9585, Weighted-f1: 0.9727, Loss: 0.1607
[Epoch 6/50] Evaluation completed with Micro-f1: 0.5509, Macro-f1: 0.4083, Weighted-f1: 0.5262, Loss: 1.4772
Saved models from epoch 6 to 'data/predictions/almnps_4012/thesaurus_affinity/ood_validation/literature/newest.pt'.
No improvement since 3 epochs (1.4290 loss). Early stop.
OOD training completed for test topic literature after 6 epochs.
Loaded <torch.utils.data.dataloader.DataLoader object at 0x7f630e75aa90> (train).
Loaded <torch.utils.data.dataloader.DataLoader object at 0x7f629c305370> (dev).
Starting training on ['ai', 'literature', 'news', 'politics', 'science'] data with validation domain music.
Read data from folder data/crossre_data/
Loaded <TransformerEmbeddings: dim=768>.
Using classifier: <LinearClassifier: emb_model = <TransformerEmbeddings: dim=768>>
Using criterion: <LabelLoss: loss=XEnt>.
Optimizing using: AdamW with learning rate 2e-05.
[Epoch 1/50] Train completed with Micro-f1: 0.3439, Macro-f1: 0.2069, Weighted-f1: 0.2811, Loss: 2.1795
[Epoch 1/50] Evaluation completed with Micro-f1: 0.4553, Macro-f1: 0.3068, Weighted-f1: 0.4146, Loss: 1.8612
Saved models from epoch 1 to 'data/predictions/almnps_4012/thesaurus_affinity/ood_validation/music/newest.pt'.
Saved model with best loss 1.8612 to 'data/predictions/almnps_4012/thesaurus_affinity/ood_validation/music/best.pt'.
[Epoch 2/50] Train completed with Micro-f1: 0.6864, Macro-f1: 0.5508, Weighted-f1: 0.6523, Loss: 1.1977
[Epoch 2/50] Evaluation completed with Micro-f1: 0.4824, Macro-f1: 0.3323, Weighted-f1: 0.4472, Loss: 1.6534
Saved models from epoch 2 to 'data/predictions/almnps_4012/thesaurus_affinity/ood_validation/music/newest.pt'.
Saved model with best loss 1.6534 to 'data/predictions/almnps_4012/thesaurus_affinity/ood_validation/music/best.pt'.
[Epoch 3/50] Train completed with Micro-f1: 0.8161, Macro-f1: 0.7088, Weighted-f1: 0.7952, Loss: 0.7362
[Epoch 3/50] Evaluation completed with Micro-f1: 0.4960, Macro-f1: 0.3489, Weighted-f1: 0.4637, Loss: 1.6325
Saved models from epoch 3 to 'data/predictions/almnps_4012/thesaurus_affinity/ood_validation/music/newest.pt'.
Saved model with best loss 1.6325 to 'data/predictions/almnps_4012/thesaurus_affinity/ood_validation/music/best.pt'.
[Epoch 4/50] Train completed with Micro-f1: 0.8904, Macro-f1: 0.8224, Weighted-f1: 0.8788, Loss: 0.4834
[Epoch 4/50] Evaluation completed with Micro-f1: 0.5084, Macro-f1: 0.3620, Weighted-f1: 0.4801, Loss: 1.5440
Saved models from epoch 4 to 'data/predictions/almnps_4012/thesaurus_affinity/ood_validation/music/newest.pt'.
Saved model with best loss 1.5440 to 'data/predictions/almnps_4012/thesaurus_affinity/ood_validation/music/best.pt'.
[Epoch 5/50] Train completed with Micro-f1: 0.9372, Macro-f1: 0.9003, Weighted-f1: 0.9306, Loss: 0.3168
[Epoch 5/50] Evaluation completed with Micro-f1: 0.5160, Macro-f1: 0.3723, Weighted-f1: 0.4901, Loss: 1.5586
Saved models from epoch 5 to 'data/predictions/almnps_4012/thesaurus_affinity/ood_validation/music/newest.pt'.
[Epoch 6/50] Train completed with Micro-f1: 0.9711, Macro-f1: 0.9524, Weighted-f1: 0.9682, Loss: 0.2025
[Epoch 6/50] Evaluation completed with Micro-f1: 0.5206, Macro-f1: 0.3770, Weighted-f1: 0.4960, Loss: 1.5850
Saved models from epoch 6 to 'data/predictions/almnps_4012/thesaurus_affinity/ood_validation/music/newest.pt'.
[Epoch 7/50] Train completed with Micro-f1: 0.9815, Macro-f1: 0.9668, Weighted-f1: 0.9815, Loss: 0.1338
[Epoch 7/50] Evaluation completed with Micro-f1: 0.5260, Macro-f1: 0.3827, Weighted-f1: 0.5021, Loss: 1.6060
Saved models from epoch 7 to 'data/predictions/almnps_4012/thesaurus_affinity/ood_validation/music/newest.pt'.
No improvement since 3 epochs (1.5440 loss). Early stop.
OOD training completed for test topic music after 7 epochs.
Loaded <torch.utils.data.dataloader.DataLoader object at 0x7f62af5ee4c0> (train).
Loaded <torch.utils.data.dataloader.DataLoader object at 0x7f630e8150a0> (dev).
Starting training on ['ai', 'literature', 'music', 'politics', 'science'] data with validation domain news.
Read data from folder data/crossre_data/
Loaded <TransformerEmbeddings: dim=768>.
Using classifier: <LinearClassifier: emb_model = <TransformerEmbeddings: dim=768>>
Using criterion: <LabelLoss: loss=XEnt>.
Optimizing using: AdamW with learning rate 2e-05.
[Epoch 1/50] Train completed with Micro-f1: 0.4337, Macro-f1: 0.2731, Weighted-f1: 0.3758, Loss: 1.9844
[Epoch 1/50] Evaluation completed with Micro-f1: 0.5319, Macro-f1: 0.3648, Weighted-f1: 0.4878, Loss: 1.6179
Saved models from epoch 1 to 'data/predictions/almnps_4012/thesaurus_affinity/ood_validation/news/newest.pt'.
Saved model with best loss 1.6179 to 'data/predictions/almnps_4012/thesaurus_affinity/ood_validation/news/best.pt'.
[Epoch 2/50] Train completed with Micro-f1: 0.7541, Macro-f1: 0.6295, Weighted-f1: 0.7266, Loss: 0.9751
[Epoch 2/50] Evaluation completed with Micro-f1: 0.5525, Macro-f1: 0.3923, Weighted-f1: 0.5140, Loss: 1.4468
Saved models from epoch 2 to 'data/predictions/almnps_4012/thesaurus_affinity/ood_validation/news/newest.pt'.
Saved model with best loss 1.4468 to 'data/predictions/almnps_4012/thesaurus_affinity/ood_validation/news/best.pt'.
[Epoch 3/50] Train completed with Micro-f1: 0.8609, Macro-f1: 0.7744, Weighted-f1: 0.8468, Loss: 0.5638
[Epoch 3/50] Evaluation completed with Micro-f1: 0.5643, Macro-f1: 0.4074, Weighted-f1: 0.5294, Loss: 1.3906
Saved models from epoch 3 to 'data/predictions/almnps_4012/thesaurus_affinity/ood_validation/news/newest.pt'.
Saved model with best loss 1.3906 to 'data/predictions/almnps_4012/thesaurus_affinity/ood_validation/news/best.pt'.
[Epoch 4/50] Train completed with Micro-f1: 0.9224, Macro-f1: 0.8658, Weighted-f1: 0.9171, Loss: 0.3426
[Epoch 4/50] Evaluation completed with Micro-f1: 0.5733, Macro-f1: 0.4208, Weighted-f1: 0.5427, Loss: 1.3701
Saved models from epoch 4 to 'data/predictions/almnps_4012/thesaurus_affinity/ood_validation/news/newest.pt'.
Saved model with best loss 1.3701 to 'data/predictions/almnps_4012/thesaurus_affinity/ood_validation/news/best.pt'.
[Epoch 5/50] Train completed with Micro-f1: 0.9576, Macro-f1: 0.9260, Weighted-f1: 0.9528, Loss: 0.2268
[Epoch 5/50] Evaluation completed with Micro-f1: 0.5819, Macro-f1: 0.4299, Weighted-f1: 0.5528, Loss: 1.4139
Saved models from epoch 5 to 'data/predictions/almnps_4012/thesaurus_affinity/ood_validation/news/newest.pt'.
[Epoch 6/50] Train completed with Micro-f1: 0.9809, Macro-f1: 0.9666, Weighted-f1: 0.9807, Loss: 0.1374
[Epoch 6/50] Evaluation completed with Micro-f1: 0.5878, Macro-f1: 0.4370, Weighted-f1: 0.5607, Loss: 1.4511
Saved models from epoch 6 to 'data/predictions/almnps_4012/thesaurus_affinity/ood_validation/news/newest.pt'.
[Epoch 7/50] Train completed with Micro-f1: 0.9909, Macro-f1: 0.9849, Weighted-f1: 0.9901, Loss: 0.0860
[Epoch 7/50] Evaluation completed with Micro-f1: 0.5929, Macro-f1: 0.4428, Weighted-f1: 0.5665, Loss: 1.4631
Saved models from epoch 7 to 'data/predictions/almnps_4012/thesaurus_affinity/ood_validation/news/newest.pt'.
No improvement since 3 epochs (1.3701 loss). Early stop.
OOD training completed for test topic news after 7 epochs.
Loaded <torch.utils.data.dataloader.DataLoader object at 0x7f61d56ddbe0> (train).
Loaded <torch.utils.data.dataloader.DataLoader object at 0x7f61d5d5efa0> (dev).
Starting training on ['ai', 'literature', 'music', 'news', 'science'] data with validation domain politics.
Read data from folder data/crossre_data/
Loaded <TransformerEmbeddings: dim=768>.
Using classifier: <LinearClassifier: emb_model = <TransformerEmbeddings: dim=768>>
Using criterion: <LabelLoss: loss=XEnt>.
Optimizing using: AdamW with learning rate 2e-05.
[Epoch 1/50] Train completed with Micro-f1: 0.3986, Macro-f1: 0.2498, Weighted-f1: 0.3410, Loss: 2.1349
[Epoch 1/50] Evaluation completed with Micro-f1: 0.5070, Macro-f1: 0.3231, Weighted-f1: 0.4512, Loss: 1.6967
Saved models from epoch 1 to 'data/predictions/almnps_4012/thesaurus_affinity/ood_validation/politics/newest.pt'.
Saved model with best loss 1.6967 to 'data/predictions/almnps_4012/thesaurus_affinity/ood_validation/politics/best.pt'.
[Epoch 2/50] Train completed with Micro-f1: 0.6994, Macro-f1: 0.5651, Weighted-f1: 0.6673, Loss: 1.1285
[Epoch 2/50] Evaluation completed with Micro-f1: 0.5339, Macro-f1: 0.3535, Weighted-f1: 0.4884, Loss: 1.4465
Saved models from epoch 2 to 'data/predictions/almnps_4012/thesaurus_affinity/ood_validation/politics/newest.pt'.
Saved model with best loss 1.4465 to 'data/predictions/almnps_4012/thesaurus_affinity/ood_validation/politics/best.pt'.
[Epoch 3/50] Train completed with Micro-f1: 0.8319, Macro-f1: 0.7238, Weighted-f1: 0.8154, Loss: 0.6817
[Epoch 3/50] Evaluation completed with Micro-f1: 0.5516, Macro-f1: 0.3776, Weighted-f1: 0.5113, Loss: 1.3630
Saved models from epoch 3 to 'data/predictions/almnps_4012/thesaurus_affinity/ood_validation/politics/newest.pt'.
Saved model with best loss 1.3630 to 'data/predictions/almnps_4012/thesaurus_affinity/ood_validation/politics/best.pt'.
[Epoch 4/50] Train completed with Micro-f1: 0.9029, Macro-f1: 0.8315, Weighted-f1: 0.8933, Loss: 0.4183
[Epoch 4/50] Evaluation completed with Micro-f1: 0.5637, Macro-f1: 0.3910, Weighted-f1: 0.5271, Loss: 1.3408
Saved models from epoch 4 to 'data/predictions/almnps_4012/thesaurus_affinity/ood_validation/politics/newest.pt'.
Saved model with best loss 1.3408 to 'data/predictions/almnps_4012/thesaurus_affinity/ood_validation/politics/best.pt'.
[Epoch 5/50] Train completed with Micro-f1: 0.9457, Macro-f1: 0.9010, Weighted-f1: 0.9396, Loss: 0.2662
[Epoch 5/50] Evaluation completed with Micro-f1: 0.5728, Macro-f1: 0.4027, Weighted-f1: 0.5395, Loss: 1.3408
Saved models from epoch 5 to 'data/predictions/almnps_4012/thesaurus_affinity/ood_validation/politics/newest.pt'.
[Epoch 6/50] Train completed with Micro-f1: 0.9734, Macro-f1: 0.9515, Weighted-f1: 0.9722, Loss: 0.1737
[Epoch 6/50] Evaluation completed with Micro-f1: 0.5788, Macro-f1: 0.4110, Weighted-f1: 0.5471, Loss: 1.4311
Saved models from epoch 6 to 'data/predictions/almnps_4012/thesaurus_affinity/ood_validation/politics/newest.pt'.
[Epoch 7/50] Train completed with Micro-f1: 0.9854, Macro-f1: 0.9761, Weighted-f1: 0.9851, Loss: 0.1155
[Epoch 7/50] Evaluation completed with Micro-f1: 0.5837, Macro-f1: 0.4162, Weighted-f1: 0.5527, Loss: 1.4228
Saved models from epoch 7 to 'data/predictions/almnps_4012/thesaurus_affinity/ood_validation/politics/newest.pt'.
No improvement since 3 epochs (1.3408 loss). Early stop.
OOD training completed for test topic politics after 7 epochs.
Loaded <torch.utils.data.dataloader.DataLoader object at 0x7f6297f4a8e0> (train).
Loaded <torch.utils.data.dataloader.DataLoader object at 0x7f629c1e9cd0> (dev).
Starting training on ['ai', 'literature', 'music', 'news', 'politics'] data with validation domain science.
Read data from folder data/crossre_data/
Loaded <TransformerEmbeddings: dim=768>.
Using classifier: <LinearClassifier: emb_model = <TransformerEmbeddings: dim=768>>
Using criterion: <LabelLoss: loss=XEnt>.
Optimizing using: AdamW with learning rate 2e-05.
[Epoch 1/50] Train completed with Micro-f1: 0.4134, Macro-f1: 0.2511, Weighted-f1: 0.3475, Loss: 1.9752
[Epoch 1/50] Evaluation completed with Micro-f1: 0.5376, Macro-f1: 0.3679, Weighted-f1: 0.4965, Loss: 1.6444
Saved models from epoch 1 to 'data/predictions/almnps_4012/thesaurus_affinity/ood_validation/science/newest.pt'.
Saved model with best loss 1.6444 to 'data/predictions/almnps_4012/thesaurus_affinity/ood_validation/science/best.pt'.
[Epoch 2/50] Train completed with Micro-f1: 0.7140, Macro-f1: 0.5880, Weighted-f1: 0.6829, Loss: 1.0436
[Epoch 2/50] Evaluation completed with Micro-f1: 0.5557, Macro-f1: 0.3902, Weighted-f1: 0.5212, Loss: 1.4518
Saved models from epoch 2 to 'data/predictions/almnps_4012/thesaurus_affinity/ood_validation/science/newest.pt'.
Saved model with best loss 1.4518 to 'data/predictions/almnps_4012/thesaurus_affinity/ood_validation/science/best.pt'.
[Epoch 3/50] Train completed with Micro-f1: 0.8442, Macro-f1: 0.7576, Weighted-f1: 0.8330, Loss: 0.6461
[Epoch 3/50] Evaluation completed with Micro-f1: 0.5708, Macro-f1: 0.4085, Weighted-f1: 0.5383, Loss: 1.4174
Saved models from epoch 3 to 'data/predictions/almnps_4012/thesaurus_affinity/ood_validation/science/newest.pt'.
Saved model with best loss 1.4174 to 'data/predictions/almnps_4012/thesaurus_affinity/ood_validation/science/best.pt'.
[Epoch 4/50] Train completed with Micro-f1: 0.9037, Macro-f1: 0.8376, Weighted-f1: 0.8931, Loss: 0.4051
[Epoch 4/50] Evaluation completed with Micro-f1: 0.5799, Macro-f1: 0.4194, Weighted-f1: 0.5485, Loss: 1.4573
Saved models from epoch 4 to 'data/predictions/almnps_4012/thesaurus_affinity/ood_validation/science/newest.pt'.
[Epoch 5/50] Train completed with Micro-f1: 0.9392, Macro-f1: 0.8922, Weighted-f1: 0.9327, Loss: 0.2557
[Epoch 5/50] Evaluation completed with Micro-f1: 0.5880, Macro-f1: 0.4269, Weighted-f1: 0.5592, Loss: 1.4580
Saved models from epoch 5 to 'data/predictions/almnps_4012/thesaurus_affinity/ood_validation/science/newest.pt'.
[Epoch 6/50] Train completed with Micro-f1: 0.9719, Macro-f1: 0.9439, Weighted-f1: 0.9704, Loss: 0.1620
[Epoch 6/50] Evaluation completed with Micro-f1: 0.5942, Macro-f1: 0.4349, Weighted-f1: 0.5673, Loss: 1.5207
Saved models from epoch 6 to 'data/predictions/almnps_4012/thesaurus_affinity/ood_validation/science/newest.pt'.
No improvement since 3 epochs (1.4174 loss). Early stop.
OOD training completed for test topic science after 6 epochs.
TRAINING COMPLETED with:
	Domains:		['ai', 'literature', 'music', 'news', 'politics', 'science']
	OOD validaation:	True
	Mapping type:		thesaurus_affinity
