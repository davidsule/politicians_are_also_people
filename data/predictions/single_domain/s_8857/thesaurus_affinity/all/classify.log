Saved arguments to data/predictions/single_domain/s_8857/thesaurus_affinity/all/args.json.
loading projection weights from /home/davidsule/gensim-data/word2vec-google-news-300/word2vec-google-news-300.gz
KeyedVectors lifecycle event {'msg': 'loaded (3000000, 300) matrix of type float32 from /home/davidsule/gensim-data/word2vec-google-news-300/word2vec-google-news-300.gz', 'binary': True, 'encoding': 'utf8', 'datetime': '2023-05-17T03:41:08.442272', 'gensim': '4.3.0', 'python': '3.9.16 | packaged by conda-forge | (main, Feb  1 2023, 21:39:03) \n[GCC 11.3.0]', 'platform': 'Linux-5.15.90.1-microsoft-standard-WSL2-x86_64-with-glibc2.31', 'event': 'load_word2vec_format'}
Saved category mapping to data/predictions/single_domain/s_8857/thesaurus_affinity/all/entity2category_mapping.json.
Loaded category mapping: thesaurus_affinity.
Loaded <torch.utils.data.dataloader.DataLoader object at 0x7f382b289340> (train).
Loaded <torch.utils.data.dataloader.DataLoader object at 0x7f382b0c8dc0> (dev).
Starting training on ['science'] data.
Read data from folder data/crossre_data/
Loaded <TransformerEmbeddings: dim=768>.
Using classifier: <LinearClassifier: emb_model = <TransformerEmbeddings: dim=768>>
Using criterion: <LabelLoss: loss=XEnt>.
Optimizing using: AdamW with learning rate 2e-05.
[Epoch 1/50] Train completed with Micro-f1: 0.2569, Macro-f1: 0.1194, Weighted-f1: 0.1845, Loss: 2.4994
[Epoch 1/50] Evaluation completed with Micro-f1: 0.1710, Macro-f1: 0.0584, Weighted-f1: 0.0984, Loss: 2.5278
Saved models from epoch 1 to 'data/predictions/single_domain/s_8857/thesaurus_affinity/all/newest.pt'.
Saved model with best loss 2.5278 to 'data/predictions/single_domain/s_8857/thesaurus_affinity/all/best.pt'.
[Epoch 2/50] Train completed with Micro-f1: 0.4089, Macro-f1: 0.1987, Weighted-f1: 0.3193, Loss: 1.9800
[Epoch 2/50] Evaluation completed with Micro-f1: 0.2026, Macro-f1: 0.0803, Weighted-f1: 0.1307, Loss: 2.4531
Saved models from epoch 2 to 'data/predictions/single_domain/s_8857/thesaurus_affinity/all/newest.pt'.
Saved model with best loss 2.4531 to 'data/predictions/single_domain/s_8857/thesaurus_affinity/all/best.pt'.
[Epoch 3/50] Train completed with Micro-f1: 0.6745, Macro-f1: 0.5294, Weighted-f1: 0.6179, Loss: 1.3149
[Epoch 3/50] Evaluation completed with Micro-f1: 0.2294, Macro-f1: 0.1104, Weighted-f1: 0.1633, Loss: 2.3654
Saved models from epoch 3 to 'data/predictions/single_domain/s_8857/thesaurus_affinity/all/newest.pt'.
Saved model with best loss 2.3654 to 'data/predictions/single_domain/s_8857/thesaurus_affinity/all/best.pt'.
[Epoch 4/50] Train completed with Micro-f1: 0.7665, Macro-f1: 0.6405, Weighted-f1: 0.7219, Loss: 1.0103
[Epoch 4/50] Evaluation completed with Micro-f1: 0.2523, Macro-f1: 0.1329, Weighted-f1: 0.1906, Loss: 2.2484
Saved models from epoch 4 to 'data/predictions/single_domain/s_8857/thesaurus_affinity/all/newest.pt'.
Saved model with best loss 2.2484 to 'data/predictions/single_domain/s_8857/thesaurus_affinity/all/best.pt'.
[Epoch 5/50] Train completed with Micro-f1: 0.8438, Macro-f1: 0.7259, Weighted-f1: 0.8153, Loss: 0.7426
[Epoch 5/50] Evaluation completed with Micro-f1: 0.2714, Macro-f1: 0.1510, Weighted-f1: 0.2137, Loss: 2.1921
Saved models from epoch 5 to 'data/predictions/single_domain/s_8857/thesaurus_affinity/all/newest.pt'.
Saved model with best loss 2.1921 to 'data/predictions/single_domain/s_8857/thesaurus_affinity/all/best.pt'.
[Epoch 6/50] Train completed with Micro-f1: 0.8681, Macro-f1: 0.7918, Weighted-f1: 0.8421, Loss: 0.5964
[Epoch 6/50] Evaluation completed with Micro-f1: 0.2876, Macro-f1: 0.1662, Weighted-f1: 0.2333, Loss: 2.1049
Saved models from epoch 6 to 'data/predictions/single_domain/s_8857/thesaurus_affinity/all/newest.pt'.
Saved model with best loss 2.1049 to 'data/predictions/single_domain/s_8857/thesaurus_affinity/all/best.pt'.
[Epoch 7/50] Train completed with Micro-f1: 0.9453, Macro-f1: 0.9036, Weighted-f1: 0.9360, Loss: 0.4027
[Epoch 7/50] Evaluation completed with Micro-f1: 0.2987, Macro-f1: 0.1764, Weighted-f1: 0.2471, Loss: 2.1480
Saved models from epoch 7 to 'data/predictions/single_domain/s_8857/thesaurus_affinity/all/newest.pt'.
[Epoch 8/50] Train completed with Micro-f1: 0.9635, Macro-f1: 0.9277, Weighted-f1: 0.9586, Loss: 0.3115
[Epoch 8/50] Evaluation completed with Micro-f1: 0.3103, Macro-f1: 0.1881, Weighted-f1: 0.2615, Loss: 2.1000
Saved models from epoch 8 to 'data/predictions/single_domain/s_8857/thesaurus_affinity/all/newest.pt'.
Saved model with best loss 2.1000 to 'data/predictions/single_domain/s_8857/thesaurus_affinity/all/best.pt'.
[Epoch 9/50] Train completed with Micro-f1: 0.9870, Macro-f1: 0.9596, Weighted-f1: 0.9825, Loss: 0.2437
[Epoch 9/50] Evaluation completed with Micro-f1: 0.3201, Macro-f1: 0.1973, Weighted-f1: 0.2730, Loss: 2.0879
Saved models from epoch 9 to 'data/predictions/single_domain/s_8857/thesaurus_affinity/all/newest.pt'.
Saved model with best loss 2.0879 to 'data/predictions/single_domain/s_8857/thesaurus_affinity/all/best.pt'.
[Epoch 10/50] Train completed with Micro-f1: 0.9870, Macro-f1: 0.9699, Weighted-f1: 0.9839, Loss: 0.1928
[Epoch 10/50] Evaluation completed with Micro-f1: 0.3271, Macro-f1: 0.2031, Weighted-f1: 0.2809, Loss: 2.0925
Saved models from epoch 10 to 'data/predictions/single_domain/s_8857/thesaurus_affinity/all/newest.pt'.
[Epoch 11/50] Train completed with Micro-f1: 0.9922, Macro-f1: 0.9791, Weighted-f1: 0.9918, Loss: 0.1569
[Epoch 11/50] Evaluation completed with Micro-f1: 0.3334, Macro-f1: 0.2084, Weighted-f1: 0.2889, Loss: 2.0728
Saved models from epoch 11 to 'data/predictions/single_domain/s_8857/thesaurus_affinity/all/newest.pt'.
Saved model with best loss 2.0728 to 'data/predictions/single_domain/s_8857/thesaurus_affinity/all/best.pt'.
[Epoch 12/50] Train completed with Micro-f1: 0.9948, Macro-f1: 0.9823, Weighted-f1: 0.9928, Loss: 0.1502
[Epoch 12/50] Evaluation completed with Micro-f1: 0.3388, Macro-f1: 0.2135, Weighted-f1: 0.2959, Loss: 2.0812
Saved models from epoch 12 to 'data/predictions/single_domain/s_8857/thesaurus_affinity/all/newest.pt'.
[Epoch 13/50] Train completed with Micro-f1: 0.9974, Macro-f1: 0.9974, Weighted-f1: 0.9975, Loss: 0.1378
[Epoch 13/50] Evaluation completed with Micro-f1: 0.3440, Macro-f1: 0.2179, Weighted-f1: 0.3020, Loss: 2.0773
Saved models from epoch 13 to 'data/predictions/single_domain/s_8857/thesaurus_affinity/all/newest.pt'.
[Epoch 14/50] Train completed with Micro-f1: 0.9948, Macro-f1: 0.9833, Weighted-f1: 0.9949, Loss: 0.1062
[Epoch 14/50] Evaluation completed with Micro-f1: 0.3474, Macro-f1: 0.2211, Weighted-f1: 0.3063, Loss: 2.1196
Saved models from epoch 14 to 'data/predictions/single_domain/s_8857/thesaurus_affinity/all/newest.pt'.
No improvement since 3 epochs (2.0728 loss). Early stop.
Training completed after 14 epochs.
TRAINING COMPLETED with:
	Domains:		['science']
	OOD validaation:	False
	Mapping type:		thesaurus_affinity
