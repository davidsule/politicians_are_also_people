{"related-to": {"precision": 0.03431372549019608, "recall": 0.033816425120772944, "f1-score": 0.0340632603406326, "support": 414}, "artifact": {"precision": 0.06928104575163399, "recall": 0.08346456692913386, "f1-score": 0.07571428571428572, "support": 635}, "cause-effect": {"precision": 0.0, "recall": 0.0, "f1-score": 0.0, "support": 86}, "compare": {"precision": 0.0, "recall": 0.0, "f1-score": 0.0, "support": 82}, "general-affiliation": {"precision": 0.1481967213114754, "recall": 0.1824051654560129, "f1-score": 0.16353111432706222, "support": 1239}, "named": {"precision": 0.07231404958677685, "recall": 0.06272401433691756, "f1-score": 0.06717850287907869, "support": 558}, "opposite": {"precision": 0.061224489795918366, "recall": 0.0234375, "f1-score": 0.03389830508474576, "support": 128}, "origin": {"precision": 0.04918032786885246, "recall": 0.019736842105263157, "f1-score": 0.02816901408450704, "support": 304}, "part-of": {"precision": 0.08560311284046693, "recall": 0.09153952843273232, "f1-score": 0.08847184986595176, "support": 721}, "physical": {"precision": 0.16225390359809913, "recall": 0.16426116838487972, "f1-score": 0.1632513661202186, "support": 1455}, "role": {"precision": 0.22817173178967678, "recall": 0.2357926221335992, "f1-score": 0.23191958813434668, "support": 2006}, "social": {"precision": 0.0, "recall": 0.0, "f1-score": 0.0, "support": 105}, "temporal": {"precision": 0.05333333333333334, "recall": 0.058823529411764705, "f1-score": 0.05594405594405595, "support": 408}, "topic": {"precision": 0.05, "recall": 0.011764705882352941, "f1-score": 0.01904761904761905, "support": 170}, "type-of": {"precision": 0.04918032786885246, "recall": 0.017857142857142856, "f1-score": 0.026200873362445413, "support": 336}, "usage": {"precision": 0.0, "recall": 0.0, "f1-score": 0.0, "support": 200}, "win-defeat": {"precision": 0.036544850498338874, "recall": 0.03470031545741325, "f1-score": 0.03559870550161812, "support": 317}, "micro avg": {"precision": 0.1335948315643747, "recall": 0.12636403317328676, "f1-score": 0.12987886944818303, "support": 9164}, "macro avg": {"precision": 0.06468221292550709, "recall": 0.06001903097105797, "f1-score": 0.06017579649450397, "support": 9164}, "weighted avg": {"precision": 0.12209023941300606, "recall": 0.12636403317328676, "f1-score": 0.12307780098349194, "support": 9164}, "samples avg": {"precision": 0.1335948315643747, "recall": 0.12480772188894015, "f1-score": 0.1277303491770497, "support": 9164}}