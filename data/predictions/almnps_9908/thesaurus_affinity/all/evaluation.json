{"related-to": {"precision": 0.04113924050632911, "recall": 0.06280193236714976, "f1-score": 0.0497131931166348, "support": 414}, "artifact": {"precision": 0.08618654073199528, "recall": 0.11496062992125984, "f1-score": 0.09851551956815115, "support": 635}, "cause-effect": {"precision": 0.0, "recall": 0.0, "f1-score": 0.0, "support": 86}, "compare": {"precision": 0.0, "recall": 0.0, "f1-score": 0.0, "support": 82}, "general-affiliation": {"precision": 0.15727391874180865, "recall": 0.1937046004842615, "f1-score": 0.1735985533453888, "support": 1239}, "named": {"precision": 0.06728971962616823, "recall": 0.06451612903225806, "f1-score": 0.06587374199451053, "support": 558}, "opposite": {"precision": 0.0, "recall": 0.0, "f1-score": 0.0, "support": 128}, "origin": {"precision": 0.03723404255319149, "recall": 0.023026315789473683, "f1-score": 0.028455284552845527, "support": 304}, "part-of": {"precision": 0.09444444444444444, "recall": 0.09431345353675451, "f1-score": 0.09437890353920887, "support": 721}, "physical": {"precision": 0.1694915254237288, "recall": 0.17869415807560138, "f1-score": 0.17397122783539645, "support": 1455}, "role": {"precision": 0.2332695984703633, "recall": 0.18245264207377868, "f1-score": 0.20475524475524476, "support": 2006}, "social": {"precision": 0.014492753623188406, "recall": 0.009523809523809525, "f1-score": 0.011494252873563218, "support": 105}, "temporal": {"precision": 0.06079664570230608, "recall": 0.07107843137254902, "f1-score": 0.06553672316384179, "support": 408}, "topic": {"precision": 0.0, "recall": 0.0, "f1-score": 0.0, "support": 170}, "type-of": {"precision": 0.038461538461538464, "recall": 0.01488095238095238, "f1-score": 0.02145922746781116, "support": 336}, "usage": {"precision": 0.0, "recall": 0.0, "f1-score": 0.0, "support": 200}, "win-defeat": {"precision": 0.04746835443037975, "recall": 0.0473186119873817, "f1-score": 0.04739336492890995, "support": 317}, "micro avg": {"precision": 0.12990309183202584, "recall": 0.12287210824967264, "f1-score": 0.12628981606101392, "support": 9164}, "macro avg": {"precision": 0.061620489571496594, "recall": 0.062192450973248825, "f1-score": 0.06089089630244159, "support": 9164}, "weighted avg": {"precision": 0.12575622447240417, "recall": 0.12287210824967264, "f1-score": 0.12284257994909588, "support": 9164}, "samples avg": {"precision": 0.12990309183202584, "recall": 0.12232733425626825, "f1-score": 0.12484617751115214, "support": 9164}}