Saved arguments to data/predictions/almnps_9908/thesaurus_affinity/ood_validation/args.json.
loading projection weights from /home/davidsule/gensim-data/word2vec-google-news-300/word2vec-google-news-300.gz
KeyedVectors lifecycle event {'msg': 'loaded (3000000, 300) matrix of type float32 from /home/davidsule/gensim-data/word2vec-google-news-300/word2vec-google-news-300.gz', 'binary': True, 'encoding': 'utf8', 'datetime': '2023-05-11T04:38:56.447107', 'gensim': '4.3.0', 'python': '3.9.16 | packaged by conda-forge | (main, Feb  1 2023, 21:39:03) \n[GCC 11.3.0]', 'platform': 'Linux-5.15.90.1-microsoft-standard-WSL2-x86_64-with-glibc2.31', 'event': 'load_word2vec_format'}
Saved category mapping to data/predictions/almnps_9908/thesaurus_affinity/ood_validation/entity2category_mapping.json.
Loaded category mapping: thesaurus_affinity.
Loaded <torch.utils.data.dataloader.DataLoader object at 0x7f99df232760> (train).
Loaded <torch.utils.data.dataloader.DataLoader object at 0x7f99df2327c0> (dev).
Starting training on ['literature', 'music', 'news', 'politics', 'science'] data.
Read data from folder data/crossre_data/
Loaded <TransformerEmbeddings: dim=768>.
Using classifier: <LinearClassifier: emb_model = <TransformerEmbeddings: dim=768>>
Using criterion: <LabelLoss: loss=XEnt>.
Optimizing using: AdamW with learning rate 2e-05.
[Epoch 1/50] Train completed with Micro-f1: 0.4180, Macro-f1: 0.2403, Weighted-f1: 0.3445, Loss: 1.9939
[Epoch 1/50] Evaluation completed with Micro-f1: 0.5115, Macro-f1: 0.3280, Weighted-f1: 0.4519, Loss: 1.6806
Saved models from epoch 1 to 'data/predictions/almnps_9908/thesaurus_affinity/ood_validation/ai/newest.pt'.
Saved model with best loss 1.6806 to 'data/predictions/almnps_9908/thesaurus_affinity/ood_validation/ai/best.pt'.
[Epoch 2/50] Train completed with Micro-f1: 0.7329, Macro-f1: 0.6046, Weighted-f1: 0.6994, Loss: 0.9735
[Epoch 2/50] Evaluation completed with Micro-f1: 0.5351, Macro-f1: 0.3586, Weighted-f1: 0.4811, Loss: 1.4592
Saved models from epoch 2 to 'data/predictions/almnps_9908/thesaurus_affinity/ood_validation/ai/newest.pt'.
Saved model with best loss 1.4592 to 'data/predictions/almnps_9908/thesaurus_affinity/ood_validation/ai/best.pt'.
[Epoch 3/50] Train completed with Micro-f1: 0.8602, Macro-f1: 0.7582, Weighted-f1: 0.8471, Loss: 0.5577
[Epoch 3/50] Evaluation completed with Micro-f1: 0.5482, Macro-f1: 0.3756, Weighted-f1: 0.4979, Loss: 1.4793
Saved models from epoch 3 to 'data/predictions/almnps_9908/thesaurus_affinity/ood_validation/ai/newest.pt'.
[Epoch 4/50] Train completed with Micro-f1: 0.9231, Macro-f1: 0.8643, Weighted-f1: 0.9162, Loss: 0.3354
[Epoch 4/50] Evaluation completed with Micro-f1: 0.5578, Macro-f1: 0.3877, Weighted-f1: 0.5120, Loss: 1.4409
Saved models from epoch 4 to 'data/predictions/almnps_9908/thesaurus_affinity/ood_validation/ai/newest.pt'.
Saved model with best loss 1.4409 to 'data/predictions/almnps_9908/thesaurus_affinity/ood_validation/ai/best.pt'.
[Epoch 5/50] Train completed with Micro-f1: 0.9678, Macro-f1: 0.9397, Weighted-f1: 0.9646, Loss: 0.1961
[Epoch 5/50] Evaluation completed with Micro-f1: 0.5648, Macro-f1: 0.3979, Weighted-f1: 0.5211, Loss: 1.5487
Saved models from epoch 5 to 'data/predictions/almnps_9908/thesaurus_affinity/ood_validation/ai/newest.pt'.
[Epoch 6/50] Train completed with Micro-f1: 0.9781, Macro-f1: 0.9593, Weighted-f1: 0.9778, Loss: 0.1306
[Epoch 6/50] Evaluation completed with Micro-f1: 0.5703, Macro-f1: 0.4066, Weighted-f1: 0.5288, Loss: 1.5545
Saved models from epoch 6 to 'data/predictions/almnps_9908/thesaurus_affinity/ood_validation/ai/newest.pt'.
[Epoch 7/50] Train completed with Micro-f1: 0.9849, Macro-f1: 0.9694, Weighted-f1: 0.9850, Loss: 0.0874
[Epoch 7/50] Evaluation completed with Micro-f1: 0.5724, Macro-f1: 0.4085, Weighted-f1: 0.5308, Loss: 1.6412
Saved models from epoch 7 to 'data/predictions/almnps_9908/thesaurus_affinity/ood_validation/ai/newest.pt'.
No improvement since 3 epochs (1.4409 loss). Early stop.
OOD training completed for test topic ai after 7 epochs.
Loaded <torch.utils.data.dataloader.DataLoader object at 0x7f99c40a2130> (train).
Loaded <torch.utils.data.dataloader.DataLoader object at 0x7f99df232940> (dev).
Starting training on ['ai', 'music', 'news', 'politics', 'science'] data.
Read data from folder data/crossre_data/
Loaded <TransformerEmbeddings: dim=768>.
Using classifier: <LinearClassifier: emb_model = <TransformerEmbeddings: dim=768>>
Using criterion: <LabelLoss: loss=XEnt>.
Optimizing using: AdamW with learning rate 2e-05.
[Epoch 1/50] Train completed with Micro-f1: 0.4332, Macro-f1: 0.2663, Weighted-f1: 0.3640, Loss: 1.9582
[Epoch 1/50] Evaluation completed with Micro-f1: 0.5042, Macro-f1: 0.3516, Weighted-f1: 0.4658, Loss: 1.6641
Saved models from epoch 1 to 'data/predictions/almnps_9908/thesaurus_affinity/ood_validation/literature/newest.pt'.
Saved model with best loss 1.6641 to 'data/predictions/almnps_9908/thesaurus_affinity/ood_validation/literature/best.pt'.
[Epoch 2/50] Train completed with Micro-f1: 0.7329, Macro-f1: 0.6102, Weighted-f1: 0.7077, Loss: 1.0042
[Epoch 2/50] Evaluation completed with Micro-f1: 0.5207, Macro-f1: 0.3658, Weighted-f1: 0.4852, Loss: 1.5598
Saved models from epoch 2 to 'data/predictions/almnps_9908/thesaurus_affinity/ood_validation/literature/newest.pt'.
Saved model with best loss 1.5598 to 'data/predictions/almnps_9908/thesaurus_affinity/ood_validation/literature/best.pt'.
[Epoch 3/50] Train completed with Micro-f1: 0.8403, Macro-f1: 0.7547, Weighted-f1: 0.8257, Loss: 0.6205
[Epoch 3/50] Evaluation completed with Micro-f1: 0.5338, Macro-f1: 0.3839, Weighted-f1: 0.5007, Loss: 1.4801
Saved models from epoch 3 to 'data/predictions/almnps_9908/thesaurus_affinity/ood_validation/literature/newest.pt'.
Saved model with best loss 1.4801 to 'data/predictions/almnps_9908/thesaurus_affinity/ood_validation/literature/best.pt'.
[Epoch 4/50] Train completed with Micro-f1: 0.9111, Macro-f1: 0.8608, Weighted-f1: 0.9029, Loss: 0.3915
[Epoch 4/50] Evaluation completed with Micro-f1: 0.5438, Macro-f1: 0.3958, Weighted-f1: 0.5128, Loss: 1.4928
Saved models from epoch 4 to 'data/predictions/almnps_9908/thesaurus_affinity/ood_validation/literature/newest.pt'.
[Epoch 5/50] Train completed with Micro-f1: 0.9518, Macro-f1: 0.9182, Weighted-f1: 0.9475, Loss: 0.2443
[Epoch 5/50] Evaluation completed with Micro-f1: 0.5522, Macro-f1: 0.4052, Weighted-f1: 0.5229, Loss: 1.5136
Saved models from epoch 5 to 'data/predictions/almnps_9908/thesaurus_affinity/ood_validation/literature/newest.pt'.
[Epoch 6/50] Train completed with Micro-f1: 0.9679, Macro-f1: 0.9464, Weighted-f1: 0.9639, Loss: 0.1788
[Epoch 6/50] Evaluation completed with Micro-f1: 0.5573, Macro-f1: 0.4119, Weighted-f1: 0.5294, Loss: 1.5586
Saved models from epoch 6 to 'data/predictions/almnps_9908/thesaurus_affinity/ood_validation/literature/newest.pt'.
No improvement since 3 epochs (1.4801 loss). Early stop.
OOD training completed for test topic literature after 6 epochs.
Loaded <torch.utils.data.dataloader.DataLoader object at 0x7f99d26c50a0> (train).
Loaded <torch.utils.data.dataloader.DataLoader object at 0x7f99c40a2130> (dev).
Starting training on ['ai', 'literature', 'news', 'politics', 'science'] data.
Read data from folder data/crossre_data/
Loaded <TransformerEmbeddings: dim=768>.
Using classifier: <LinearClassifier: emb_model = <TransformerEmbeddings: dim=768>>
Using criterion: <LabelLoss: loss=XEnt>.
Optimizing using: AdamW with learning rate 2e-05.
[Epoch 1/50] Train completed with Micro-f1: 0.3649, Macro-f1: 0.2170, Weighted-f1: 0.2932, Loss: 2.1368
[Epoch 1/50] Evaluation completed with Micro-f1: 0.4621, Macro-f1: 0.3163, Weighted-f1: 0.4174, Loss: 1.8678
Saved models from epoch 1 to 'data/predictions/almnps_9908/thesaurus_affinity/ood_validation/music/newest.pt'.
Saved model with best loss 1.8678 to 'data/predictions/almnps_9908/thesaurus_affinity/ood_validation/music/best.pt'.
[Epoch 2/50] Train completed with Micro-f1: 0.7053, Macro-f1: 0.5687, Weighted-f1: 0.6727, Loss: 1.1245
[Epoch 2/50] Evaluation completed with Micro-f1: 0.4815, Macro-f1: 0.3361, Weighted-f1: 0.4417, Loss: 1.6134
Saved models from epoch 2 to 'data/predictions/almnps_9908/thesaurus_affinity/ood_validation/music/newest.pt'.
Saved model with best loss 1.6134 to 'data/predictions/almnps_9908/thesaurus_affinity/ood_validation/music/best.pt'.
[Epoch 3/50] Train completed with Micro-f1: 0.8289, Macro-f1: 0.7474, Weighted-f1: 0.8147, Loss: 0.6899
[Epoch 3/50] Evaluation completed with Micro-f1: 0.5017, Macro-f1: 0.3576, Weighted-f1: 0.4656, Loss: 1.5265
Saved models from epoch 3 to 'data/predictions/almnps_9908/thesaurus_affinity/ood_validation/music/newest.pt'.
Saved model with best loss 1.5265 to 'data/predictions/almnps_9908/thesaurus_affinity/ood_validation/music/best.pt'.
[Epoch 4/50] Train completed with Micro-f1: 0.9078, Macro-f1: 0.8388, Weighted-f1: 0.8950, Loss: 0.4313
[Epoch 4/50] Evaluation completed with Micro-f1: 0.5135, Macro-f1: 0.3707, Weighted-f1: 0.4811, Loss: 1.4900
Saved models from epoch 4 to 'data/predictions/almnps_9908/thesaurus_affinity/ood_validation/music/newest.pt'.
Saved model with best loss 1.4900 to 'data/predictions/almnps_9908/thesaurus_affinity/ood_validation/music/best.pt'.
[Epoch 5/50] Train completed with Micro-f1: 0.9493, Macro-f1: 0.9176, Weighted-f1: 0.9449, Loss: 0.2747
[Epoch 5/50] Evaluation completed with Micro-f1: 0.5229, Macro-f1: 0.3812, Weighted-f1: 0.4929, Loss: 1.5522
Saved models from epoch 5 to 'data/predictions/almnps_9908/thesaurus_affinity/ood_validation/music/newest.pt'.
[Epoch 6/50] Train completed with Micro-f1: 0.9756, Macro-f1: 0.9557, Weighted-f1: 0.9742, Loss: 0.1802
[Epoch 6/50] Evaluation completed with Micro-f1: 0.5289, Macro-f1: 0.3886, Weighted-f1: 0.5017, Loss: 1.5648
Saved models from epoch 6 to 'data/predictions/almnps_9908/thesaurus_affinity/ood_validation/music/newest.pt'.
[Epoch 7/50] Train completed with Micro-f1: 0.9906, Macro-f1: 0.9824, Weighted-f1: 0.9898, Loss: 0.1135
[Epoch 7/50] Evaluation completed with Micro-f1: 0.5336, Macro-f1: 0.3941, Weighted-f1: 0.5075, Loss: 1.5873
Saved models from epoch 7 to 'data/predictions/almnps_9908/thesaurus_affinity/ood_validation/music/newest.pt'.
No improvement since 3 epochs (1.4900 loss). Early stop.
OOD training completed for test topic music after 7 epochs.
Loaded <torch.utils.data.dataloader.DataLoader object at 0x7f99c3e73550> (train).
Loaded <torch.utils.data.dataloader.DataLoader object at 0x7f99d26bb610> (dev).
Starting training on ['ai', 'literature', 'music', 'politics', 'science'] data.
Read data from folder data/crossre_data/
Loaded <TransformerEmbeddings: dim=768>.
Using classifier: <LinearClassifier: emb_model = <TransformerEmbeddings: dim=768>>
Using criterion: <LabelLoss: loss=XEnt>.
Optimizing using: AdamW with learning rate 2e-05.
[Epoch 1/50] Train completed with Micro-f1: 0.4498, Macro-f1: 0.2946, Weighted-f1: 0.3974, Loss: 1.9468
[Epoch 1/50] Evaluation completed with Micro-f1: 0.5352, Macro-f1: 0.3754, Weighted-f1: 0.4966, Loss: 1.6310
Saved models from epoch 1 to 'data/predictions/almnps_9908/thesaurus_affinity/ood_validation/news/newest.pt'.
Saved model with best loss 1.6310 to 'data/predictions/almnps_9908/thesaurus_affinity/ood_validation/news/best.pt'.
[Epoch 2/50] Train completed with Micro-f1: 0.7494, Macro-f1: 0.6244, Weighted-f1: 0.7223, Loss: 0.9661
[Epoch 2/50] Evaluation completed with Micro-f1: 0.5551, Macro-f1: 0.4021, Weighted-f1: 0.5229, Loss: 1.4903
Saved models from epoch 2 to 'data/predictions/almnps_9908/thesaurus_affinity/ood_validation/news/newest.pt'.
Saved model with best loss 1.4903 to 'data/predictions/almnps_9908/thesaurus_affinity/ood_validation/news/best.pt'.
[Epoch 3/50] Train completed with Micro-f1: 0.8485, Macro-f1: 0.7546, Weighted-f1: 0.8375, Loss: 0.5771
[Epoch 3/50] Evaluation completed with Micro-f1: 0.5666, Macro-f1: 0.4161, Weighted-f1: 0.5363, Loss: 1.4085
Saved models from epoch 3 to 'data/predictions/almnps_9908/thesaurus_affinity/ood_validation/news/newest.pt'.
Saved model with best loss 1.4085 to 'data/predictions/almnps_9908/thesaurus_affinity/ood_validation/news/best.pt'.
[Epoch 4/50] Train completed with Micro-f1: 0.9233, Macro-f1: 0.8702, Weighted-f1: 0.9170, Loss: 0.3316
[Epoch 4/50] Evaluation completed with Micro-f1: 0.5752, Macro-f1: 0.4274, Weighted-f1: 0.5482, Loss: 1.4096
Saved models from epoch 4 to 'data/predictions/almnps_9908/thesaurus_affinity/ood_validation/news/newest.pt'.
[Epoch 5/50] Train completed with Micro-f1: 0.9700, Macro-f1: 0.9478, Weighted-f1: 0.9684, Loss: 0.1977
[Epoch 5/50] Evaluation completed with Micro-f1: 0.5808, Macro-f1: 0.4347, Weighted-f1: 0.5553, Loss: 1.5003
Saved models from epoch 5 to 'data/predictions/almnps_9908/thesaurus_affinity/ood_validation/news/newest.pt'.
[Epoch 6/50] Train completed with Micro-f1: 0.9828, Macro-f1: 0.9686, Weighted-f1: 0.9827, Loss: 0.1220
[Epoch 6/50] Evaluation completed with Micro-f1: 0.5862, Macro-f1: 0.4396, Weighted-f1: 0.5614, Loss: 1.5146
Saved models from epoch 6 to 'data/predictions/almnps_9908/thesaurus_affinity/ood_validation/news/newest.pt'.
No improvement since 3 epochs (1.4085 loss). Early stop.
OOD training completed for test topic news after 6 epochs.
Loaded <torch.utils.data.dataloader.DataLoader object at 0x7f98fed24280> (train).
Loaded <torch.utils.data.dataloader.DataLoader object at 0x7f99c3e6baf0> (dev).
Starting training on ['ai', 'literature', 'music', 'news', 'science'] data.
Read data from folder data/crossre_data/
Loaded <TransformerEmbeddings: dim=768>.
Using classifier: <LinearClassifier: emb_model = <TransformerEmbeddings: dim=768>>
Using criterion: <LabelLoss: loss=XEnt>.
Optimizing using: AdamW with learning rate 2e-05.
[Epoch 1/50] Train completed with Micro-f1: 0.3971, Macro-f1: 0.2436, Weighted-f1: 0.3366, Loss: 2.0517
[Epoch 1/50] Evaluation completed with Micro-f1: 0.5199, Macro-f1: 0.3491, Weighted-f1: 0.4743, Loss: 1.6289
Saved models from epoch 1 to 'data/predictions/almnps_9908/thesaurus_affinity/ood_validation/politics/newest.pt'.
Saved model with best loss 1.6289 to 'data/predictions/almnps_9908/thesaurus_affinity/ood_validation/politics/best.pt'.
[Epoch 2/50] Train completed with Micro-f1: 0.7261, Macro-f1: 0.5899, Weighted-f1: 0.7005, Loss: 1.0180
[Epoch 2/50] Evaluation completed with Micro-f1: 0.5456, Macro-f1: 0.3816, Weighted-f1: 0.5065, Loss: 1.4290
Saved models from epoch 2 to 'data/predictions/almnps_9908/thesaurus_affinity/ood_validation/politics/newest.pt'.
Saved model with best loss 1.4290 to 'data/predictions/almnps_9908/thesaurus_affinity/ood_validation/politics/best.pt'.
[Epoch 3/50] Train completed with Micro-f1: 0.8541, Macro-f1: 0.7686, Weighted-f1: 0.8423, Loss: 0.5948
[Epoch 3/50] Evaluation completed with Micro-f1: 0.5613, Macro-f1: 0.4004, Weighted-f1: 0.5274, Loss: 1.3607
Saved models from epoch 3 to 'data/predictions/almnps_9908/thesaurus_affinity/ood_validation/politics/newest.pt'.
Saved model with best loss 1.3607 to 'data/predictions/almnps_9908/thesaurus_affinity/ood_validation/politics/best.pt'.
[Epoch 4/50] Train completed with Micro-f1: 0.9209, Macro-f1: 0.8661, Weighted-f1: 0.9139, Loss: 0.3751
[Epoch 4/50] Evaluation completed with Micro-f1: 0.5747, Macro-f1: 0.4149, Weighted-f1: 0.5451, Loss: 1.3261
Saved models from epoch 4 to 'data/predictions/almnps_9908/thesaurus_affinity/ood_validation/politics/newest.pt'.
Saved model with best loss 1.3261 to 'data/predictions/almnps_9908/thesaurus_affinity/ood_validation/politics/best.pt'.
[Epoch 5/50] Train completed with Micro-f1: 0.9552, Macro-f1: 0.9215, Weighted-f1: 0.9521, Loss: 0.2459
[Epoch 5/50] Evaluation completed with Micro-f1: 0.5793, Macro-f1: 0.4242, Weighted-f1: 0.5533, Loss: 1.4062
Saved models from epoch 5 to 'data/predictions/almnps_9908/thesaurus_affinity/ood_validation/politics/newest.pt'.
[Epoch 6/50] Train completed with Micro-f1: 0.9723, Macro-f1: 0.9531, Weighted-f1: 0.9692, Loss: 0.1625
[Epoch 6/50] Evaluation completed with Micro-f1: 0.5865, Macro-f1: 0.4327, Weighted-f1: 0.5622, Loss: 1.4005
Saved models from epoch 6 to 'data/predictions/almnps_9908/thesaurus_affinity/ood_validation/politics/newest.pt'.
[Epoch 7/50] Train completed with Micro-f1: 0.9904, Macro-f1: 0.9872, Weighted-f1: 0.9903, Loss: 0.1015
[Epoch 7/50] Evaluation completed with Micro-f1: 0.5932, Macro-f1: 0.4395, Weighted-f1: 0.5699, Loss: 1.4061
Saved models from epoch 7 to 'data/predictions/almnps_9908/thesaurus_affinity/ood_validation/politics/newest.pt'.
No improvement since 3 epochs (1.3261 loss). Early stop.
OOD training completed for test topic politics after 7 epochs.
Loaded <torch.utils.data.dataloader.DataLoader object at 0x7f98ff3a2310> (train).
Loaded <torch.utils.data.dataloader.DataLoader object at 0x7f98fed0f7f0> (dev).
Starting training on ['ai', 'literature', 'music', 'news', 'politics'] data.
Read data from folder data/crossre_data/
Loaded <TransformerEmbeddings: dim=768>.
Using classifier: <LinearClassifier: emb_model = <TransformerEmbeddings: dim=768>>
Using criterion: <LabelLoss: loss=XEnt>.
Optimizing using: AdamW with learning rate 2e-05.
[Epoch 1/50] Train completed with Micro-f1: 0.4233, Macro-f1: 0.2657, Weighted-f1: 0.3594, Loss: 1.9935
[Epoch 1/50] Evaluation completed with Micro-f1: 0.5497, Macro-f1: 0.3804, Weighted-f1: 0.5028, Loss: 1.5687
Saved models from epoch 1 to 'data/predictions/almnps_9908/thesaurus_affinity/ood_validation/science/newest.pt'.
Saved model with best loss 1.5687 to 'data/predictions/almnps_9908/thesaurus_affinity/ood_validation/science/best.pt'.
[Epoch 2/50] Train completed with Micro-f1: 0.7124, Macro-f1: 0.5721, Weighted-f1: 0.6827, Loss: 1.0538
[Epoch 2/50] Evaluation completed with Micro-f1: 0.5662, Macro-f1: 0.4023, Weighted-f1: 0.5261, Loss: 1.4245
Saved models from epoch 2 to 'data/predictions/almnps_9908/thesaurus_affinity/ood_validation/science/newest.pt'.
Saved model with best loss 1.4245 to 'data/predictions/almnps_9908/thesaurus_affinity/ood_validation/science/best.pt'.
[Epoch 3/50] Train completed with Micro-f1: 0.8338, Macro-f1: 0.7141, Weighted-f1: 0.8213, Loss: 0.6410
[Epoch 3/50] Evaluation completed with Micro-f1: 0.5751, Macro-f1: 0.4151, Weighted-f1: 0.5407, Loss: 1.3999
Saved models from epoch 3 to 'data/predictions/almnps_9908/thesaurus_affinity/ood_validation/science/newest.pt'.
Saved model with best loss 1.3999 to 'data/predictions/almnps_9908/thesaurus_affinity/ood_validation/science/best.pt'.
[Epoch 4/50] Train completed with Micro-f1: 0.9091, Macro-f1: 0.8492, Weighted-f1: 0.9022, Loss: 0.4049
[Epoch 4/50] Evaluation completed with Micro-f1: 0.5841, Macro-f1: 0.4241, Weighted-f1: 0.5527, Loss: 1.3529
Saved models from epoch 4 to 'data/predictions/almnps_9908/thesaurus_affinity/ood_validation/science/newest.pt'.
Saved model with best loss 1.3529 to 'data/predictions/almnps_9908/thesaurus_affinity/ood_validation/science/best.pt'.
[Epoch 5/50] Train completed with Micro-f1: 0.9512, Macro-f1: 0.9050, Weighted-f1: 0.9467, Loss: 0.2489
[Epoch 5/50] Evaluation completed with Micro-f1: 0.5887, Macro-f1: 0.4310, Weighted-f1: 0.5601, Loss: 1.4672
Saved models from epoch 5 to 'data/predictions/almnps_9908/thesaurus_affinity/ood_validation/science/newest.pt'.
[Epoch 6/50] Train completed with Micro-f1: 0.9787, Macro-f1: 0.9580, Weighted-f1: 0.9784, Loss: 0.1509
[Epoch 6/50] Evaluation completed with Micro-f1: 0.5925, Macro-f1: 0.4365, Weighted-f1: 0.5651, Loss: 1.5254
Saved models from epoch 6 to 'data/predictions/almnps_9908/thesaurus_affinity/ood_validation/science/newest.pt'.
[Epoch 7/50] Train completed with Micro-f1: 0.9896, Macro-f1: 0.9799, Weighted-f1: 0.9888, Loss: 0.0928
[Epoch 7/50] Evaluation completed with Micro-f1: 0.5962, Macro-f1: 0.4417, Weighted-f1: 0.5705, Loss: 1.5574
Saved models from epoch 7 to 'data/predictions/almnps_9908/thesaurus_affinity/ood_validation/science/newest.pt'.
No improvement since 3 epochs (1.3529 loss). Early stop.
OOD training completed for test topic science after 7 epochs.
TRAINING COMPLETED with:
	Domains:		['ai', 'literature', 'music', 'news', 'politics', 'science']
	OOD validaation:	True
	Mapping type:		thesaurus_affinity
