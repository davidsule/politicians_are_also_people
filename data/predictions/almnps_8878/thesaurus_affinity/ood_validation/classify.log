Saved arguments to data/predictions/almnps_8878/thesaurus_affinity/ood_validation/args.json.
loading projection weights from /home/davidsule/gensim-data/word2vec-google-news-300/word2vec-google-news-300.gz
KeyedVectors lifecycle event {'msg': 'loaded (3000000, 300) matrix of type float32 from /home/davidsule/gensim-data/word2vec-google-news-300/word2vec-google-news-300.gz', 'binary': True, 'encoding': 'utf8', 'datetime': '2023-05-14T17:39:18.499050', 'gensim': '4.3.0', 'python': '3.9.16 | packaged by conda-forge | (main, Feb  1 2023, 21:39:03) \n[GCC 11.3.0]', 'platform': 'Linux-5.15.90.1-microsoft-standard-WSL2-x86_64-with-glibc2.31', 'event': 'load_word2vec_format'}
Saved category mapping to data/predictions/almnps_8878/thesaurus_affinity/ood_validation/entity2category_mapping.json.
Loaded category mapping: thesaurus_affinity.
Loaded <torch.utils.data.dataloader.DataLoader object at 0x7f97e371d5b0> (train).
Loaded <torch.utils.data.dataloader.DataLoader object at 0x7f97e371d0d0> (dev).
Starting training on ['literature', 'music', 'news', 'politics', 'science'] data with validation domain ai.
Read data from folder data/crossre_data/
Loaded <TransformerEmbeddings: dim=768>.
Using classifier: <LinearClassifier: emb_model = <TransformerEmbeddings: dim=768>>
Using criterion: <LabelLoss: loss=XEnt>.
Optimizing using: AdamW with learning rate 2e-05.
[Epoch 1/50] Train completed with Micro-f1: 0.4248, Macro-f1: 0.2532, Weighted-f1: 0.3470, Loss: 1.9768
[Epoch 1/50] Evaluation completed with Micro-f1: 0.5097, Macro-f1: 0.3345, Weighted-f1: 0.4512, Loss: 1.7353
Saved models from epoch 1 to 'data/predictions/almnps_8878/thesaurus_affinity/ood_validation/ai/newest.pt'.
Saved model with best loss 1.7353 to 'data/predictions/almnps_8878/thesaurus_affinity/ood_validation/ai/best.pt'.
[Epoch 2/50] Train completed with Micro-f1: 0.7397, Macro-f1: 0.5928, Weighted-f1: 0.7044, Loss: 0.9688
[Epoch 2/50] Evaluation completed with Micro-f1: 0.5386, Macro-f1: 0.3686, Weighted-f1: 0.4892, Loss: 1.4839
Saved models from epoch 2 to 'data/predictions/almnps_8878/thesaurus_affinity/ood_validation/ai/newest.pt'.
Saved model with best loss 1.4839 to 'data/predictions/almnps_8878/thesaurus_affinity/ood_validation/ai/best.pt'.
[Epoch 3/50] Train completed with Micro-f1: 0.8582, Macro-f1: 0.7598, Weighted-f1: 0.8438, Loss: 0.5592
[Epoch 3/50] Evaluation completed with Micro-f1: 0.5542, Macro-f1: 0.3870, Weighted-f1: 0.5100, Loss: 1.4468
Saved models from epoch 3 to 'data/predictions/almnps_8878/thesaurus_affinity/ood_validation/ai/newest.pt'.
Saved model with best loss 1.4468 to 'data/predictions/almnps_8878/thesaurus_affinity/ood_validation/ai/best.pt'.
[Epoch 4/50] Train completed with Micro-f1: 0.9204, Macro-f1: 0.8506, Weighted-f1: 0.9127, Loss: 0.3343
[Epoch 4/50] Evaluation completed with Micro-f1: 0.5657, Macro-f1: 0.4009, Weighted-f1: 0.5239, Loss: 1.3980
Saved models from epoch 4 to 'data/predictions/almnps_8878/thesaurus_affinity/ood_validation/ai/newest.pt'.
Saved model with best loss 1.3980 to 'data/predictions/almnps_8878/thesaurus_affinity/ood_validation/ai/best.pt'.
[Epoch 5/50] Train completed with Micro-f1: 0.9657, Macro-f1: 0.9362, Weighted-f1: 0.9638, Loss: 0.1972
[Epoch 5/50] Evaluation completed with Micro-f1: 0.5720, Macro-f1: 0.4102, Weighted-f1: 0.5342, Loss: 1.4746
Saved models from epoch 5 to 'data/predictions/almnps_8878/thesaurus_affinity/ood_validation/ai/newest.pt'.
[Epoch 6/50] Train completed with Micro-f1: 0.9781, Macro-f1: 0.9593, Weighted-f1: 0.9778, Loss: 0.1312
[Epoch 6/50] Evaluation completed with Micro-f1: 0.5780, Macro-f1: 0.4183, Weighted-f1: 0.5421, Loss: 1.5565
Saved models from epoch 6 to 'data/predictions/almnps_8878/thesaurus_affinity/ood_validation/ai/newest.pt'.
[Epoch 7/50] Train completed with Micro-f1: 0.9927, Macro-f1: 0.9859, Weighted-f1: 0.9919, Loss: 0.0770
[Epoch 7/50] Evaluation completed with Micro-f1: 0.5823, Macro-f1: 0.4232, Weighted-f1: 0.5464, Loss: 1.6081
Saved models from epoch 7 to 'data/predictions/almnps_8878/thesaurus_affinity/ood_validation/ai/newest.pt'.
No improvement since 3 epochs (1.3980 loss). Early stop.
OOD training completed for test topic ai after 7 epochs.
Loaded <torch.utils.data.dataloader.DataLoader object at 0x7f97c8d16910> (train).
Loaded <torch.utils.data.dataloader.DataLoader object at 0x7f97e371d550> (dev).
Starting training on ['ai', 'music', 'news', 'politics', 'science'] data with validation domain literature.
Read data from folder data/crossre_data/
Loaded <TransformerEmbeddings: dim=768>.
Using classifier: <LinearClassifier: emb_model = <TransformerEmbeddings: dim=768>>
Using criterion: <LabelLoss: loss=XEnt>.
Optimizing using: AdamW with learning rate 2e-05.
[Epoch 1/50] Train completed with Micro-f1: 0.3968, Macro-f1: 0.2465, Weighted-f1: 0.3358, Loss: 2.0422
[Epoch 1/50] Evaluation completed with Micro-f1: 0.4970, Macro-f1: 0.3546, Weighted-f1: 0.4556, Loss: 1.7100
Saved models from epoch 1 to 'data/predictions/almnps_8878/thesaurus_affinity/ood_validation/literature/newest.pt'.
Saved model with best loss 1.7100 to 'data/predictions/almnps_8878/thesaurus_affinity/ood_validation/literature/best.pt'.
[Epoch 2/50] Train completed with Micro-f1: 0.7166, Macro-f1: 0.5979, Weighted-f1: 0.6938, Loss: 1.0704
[Epoch 2/50] Evaluation completed with Micro-f1: 0.5181, Macro-f1: 0.3707, Weighted-f1: 0.4787, Loss: 1.5456
Saved models from epoch 2 to 'data/predictions/almnps_8878/thesaurus_affinity/ood_validation/literature/newest.pt'.
Saved model with best loss 1.5456 to 'data/predictions/almnps_8878/thesaurus_affinity/ood_validation/literature/best.pt'.
[Epoch 3/50] Train completed with Micro-f1: 0.8305, Macro-f1: 0.7281, Weighted-f1: 0.8173, Loss: 0.6491
[Epoch 3/50] Evaluation completed with Micro-f1: 0.5302, Macro-f1: 0.3850, Weighted-f1: 0.4968, Loss: 1.5069
Saved models from epoch 3 to 'data/predictions/almnps_8878/thesaurus_affinity/ood_validation/literature/newest.pt'.
Saved model with best loss 1.5069 to 'data/predictions/almnps_8878/thesaurus_affinity/ood_validation/literature/best.pt'.
[Epoch 4/50] Train completed with Micro-f1: 0.9011, Macro-f1: 0.8379, Weighted-f1: 0.8927, Loss: 0.4148
[Epoch 4/50] Evaluation completed with Micro-f1: 0.5384, Macro-f1: 0.3928, Weighted-f1: 0.5062, Loss: 1.5192
Saved models from epoch 4 to 'data/predictions/almnps_8878/thesaurus_affinity/ood_validation/literature/newest.pt'.
[Epoch 5/50] Train completed with Micro-f1: 0.9474, Macro-f1: 0.9137, Weighted-f1: 0.9462, Loss: 0.2695
[Epoch 5/50] Evaluation completed with Micro-f1: 0.5457, Macro-f1: 0.4016, Weighted-f1: 0.5163, Loss: 1.4569
Saved models from epoch 5 to 'data/predictions/almnps_8878/thesaurus_affinity/ood_validation/literature/newest.pt'.
Saved model with best loss 1.4569 to 'data/predictions/almnps_8878/thesaurus_affinity/ood_validation/literature/best.pt'.
[Epoch 6/50] Train completed with Micro-f1: 0.9592, Macro-f1: 0.9354, Weighted-f1: 0.9573, Loss: 0.1956
[Epoch 6/50] Evaluation completed with Micro-f1: 0.5519, Macro-f1: 0.4093, Weighted-f1: 0.5244, Loss: 1.5291
Saved models from epoch 6 to 'data/predictions/almnps_8878/thesaurus_affinity/ood_validation/literature/newest.pt'.
[Epoch 7/50] Train completed with Micro-f1: 0.9789, Macro-f1: 0.9630, Weighted-f1: 0.9772, Loss: 0.1338
[Epoch 7/50] Evaluation completed with Micro-f1: 0.5549, Macro-f1: 0.4127, Weighted-f1: 0.5283, Loss: 1.6120
Saved models from epoch 7 to 'data/predictions/almnps_8878/thesaurus_affinity/ood_validation/literature/newest.pt'.
[Epoch 8/50] Train completed with Micro-f1: 0.9813, Macro-f1: 0.9716, Weighted-f1: 0.9805, Loss: 0.1001
[Epoch 8/50] Evaluation completed with Micro-f1: 0.5593, Macro-f1: 0.4178, Weighted-f1: 0.5339, Loss: 1.5461
Saved models from epoch 8 to 'data/predictions/almnps_8878/thesaurus_affinity/ood_validation/literature/newest.pt'.
No improvement since 3 epochs (1.4569 loss). Early stop.
OOD training completed for test topic literature after 8 epochs.
Loaded <torch.utils.data.dataloader.DataLoader object at 0x7f98331c6eb0> (train).
Loaded <torch.utils.data.dataloader.DataLoader object at 0x7f97c8d1ff70> (dev).
Starting training on ['ai', 'literature', 'news', 'politics', 'science'] data with validation domain music.
Read data from folder data/crossre_data/
Loaded <TransformerEmbeddings: dim=768>.
Using classifier: <LinearClassifier: emb_model = <TransformerEmbeddings: dim=768>>
Using criterion: <LabelLoss: loss=XEnt>.
Optimizing using: AdamW with learning rate 2e-05.
[Epoch 1/50] Train completed with Micro-f1: 0.3421, Macro-f1: 0.1958, Weighted-f1: 0.2681, Loss: 2.1929
[Epoch 1/50] Evaluation completed with Micro-f1: 0.4779, Macro-f1: 0.3206, Weighted-f1: 0.4378, Loss: 1.8619
Saved models from epoch 1 to 'data/predictions/almnps_8878/thesaurus_affinity/ood_validation/music/newest.pt'.
Saved model with best loss 1.8619 to 'data/predictions/almnps_8878/thesaurus_affinity/ood_validation/music/best.pt'.
[Epoch 2/50] Train completed with Micro-f1: 0.6886, Macro-f1: 0.5553, Weighted-f1: 0.6593, Loss: 1.1637
[Epoch 2/50] Evaluation completed with Micro-f1: 0.5011, Macro-f1: 0.3487, Weighted-f1: 0.4647, Loss: 1.5842
Saved models from epoch 2 to 'data/predictions/almnps_8878/thesaurus_affinity/ood_validation/music/newest.pt'.
Saved model with best loss 1.5842 to 'data/predictions/almnps_8878/thesaurus_affinity/ood_validation/music/best.pt'.
[Epoch 3/50] Train completed with Micro-f1: 0.8265, Macro-f1: 0.7148, Weighted-f1: 0.8084, Loss: 0.6873
[Epoch 3/50] Evaluation completed with Micro-f1: 0.5171, Macro-f1: 0.3668, Weighted-f1: 0.4834, Loss: 1.5313
Saved models from epoch 3 to 'data/predictions/almnps_8878/thesaurus_affinity/ood_validation/music/newest.pt'.
Saved model with best loss 1.5313 to 'data/predictions/almnps_8878/thesaurus_affinity/ood_validation/music/best.pt'.
[Epoch 4/50] Train completed with Micro-f1: 0.8993, Macro-f1: 0.8264, Weighted-f1: 0.8908, Loss: 0.4352
[Epoch 4/50] Evaluation completed with Micro-f1: 0.5297, Macro-f1: 0.3800, Weighted-f1: 0.4998, Loss: 1.5034
Saved models from epoch 4 to 'data/predictions/almnps_8878/thesaurus_affinity/ood_validation/music/newest.pt'.
Saved model with best loss 1.5034 to 'data/predictions/almnps_8878/thesaurus_affinity/ood_validation/music/best.pt'.
[Epoch 5/50] Train completed with Micro-f1: 0.9544, Macro-f1: 0.9116, Weighted-f1: 0.9491, Loss: 0.2675
[Epoch 5/50] Evaluation completed with Micro-f1: 0.5356, Macro-f1: 0.3881, Weighted-f1: 0.5082, Loss: 1.5220
Saved models from epoch 5 to 'data/predictions/almnps_8878/thesaurus_affinity/ood_validation/music/newest.pt'.
[Epoch 6/50] Train completed with Micro-f1: 0.9756, Macro-f1: 0.9544, Weighted-f1: 0.9748, Loss: 0.1651
[Epoch 6/50] Evaluation completed with Micro-f1: 0.5424, Macro-f1: 0.3947, Weighted-f1: 0.5160, Loss: 1.5345
Saved models from epoch 6 to 'data/predictions/almnps_8878/thesaurus_affinity/ood_validation/music/newest.pt'.
[Epoch 7/50] Train completed with Micro-f1: 0.9928, Macro-f1: 0.9875, Weighted-f1: 0.9918, Loss: 0.0981
[Epoch 7/50] Evaluation completed with Micro-f1: 0.5471, Macro-f1: 0.4006, Weighted-f1: 0.5220, Loss: 1.5657
Saved models from epoch 7 to 'data/predictions/almnps_8878/thesaurus_affinity/ood_validation/music/newest.pt'.
No improvement since 3 epochs (1.5034 loss). Early stop.
OOD training completed for test topic music after 7 epochs.
Loaded <torch.utils.data.dataloader.DataLoader object at 0x7f97c8af7be0> (train).
Loaded <torch.utils.data.dataloader.DataLoader object at 0x7f9703885430> (dev).
Starting training on ['ai', 'literature', 'music', 'politics', 'science'] data with validation domain news.
Read data from folder data/crossre_data/
Loaded <TransformerEmbeddings: dim=768>.
Using classifier: <LinearClassifier: emb_model = <TransformerEmbeddings: dim=768>>
Using criterion: <LabelLoss: loss=XEnt>.
Optimizing using: AdamW with learning rate 2e-05.
[Epoch 1/50] Train completed with Micro-f1: 0.4275, Macro-f1: 0.2780, Weighted-f1: 0.3712, Loss: 1.9719
[Epoch 1/50] Evaluation completed with Micro-f1: 0.5510, Macro-f1: 0.3913, Weighted-f1: 0.5098, Loss: 1.5809
Saved models from epoch 1 to 'data/predictions/almnps_8878/thesaurus_affinity/ood_validation/news/newest.pt'.
Saved model with best loss 1.5809 to 'data/predictions/almnps_8878/thesaurus_affinity/ood_validation/news/best.pt'.
[Epoch 2/50] Train completed with Micro-f1: 0.7508, Macro-f1: 0.6203, Weighted-f1: 0.7234, Loss: 0.9482
[Epoch 2/50] Evaluation completed with Micro-f1: 0.5633, Macro-f1: 0.4077, Weighted-f1: 0.5316, Loss: 1.4294
Saved models from epoch 2 to 'data/predictions/almnps_8878/thesaurus_affinity/ood_validation/news/newest.pt'.
Saved model with best loss 1.4294 to 'data/predictions/almnps_8878/thesaurus_affinity/ood_validation/news/best.pt'.
[Epoch 3/50] Train completed with Micro-f1: 0.8661, Macro-f1: 0.7792, Weighted-f1: 0.8547, Loss: 0.5584
[Epoch 3/50] Evaluation completed with Micro-f1: 0.5743, Macro-f1: 0.4216, Weighted-f1: 0.5452, Loss: 1.3430
Saved models from epoch 3 to 'data/predictions/almnps_8878/thesaurus_affinity/ood_validation/news/newest.pt'.
Saved model with best loss 1.3430 to 'data/predictions/almnps_8878/thesaurus_affinity/ood_validation/news/best.pt'.
[Epoch 4/50] Train completed with Micro-f1: 0.9252, Macro-f1: 0.8715, Weighted-f1: 0.9175, Loss: 0.3412
[Epoch 4/50] Evaluation completed with Micro-f1: 0.5815, Macro-f1: 0.4307, Weighted-f1: 0.5543, Loss: 1.4377
Saved models from epoch 4 to 'data/predictions/almnps_8878/thesaurus_affinity/ood_validation/news/newest.pt'.
[Epoch 5/50] Train completed with Micro-f1: 0.9662, Macro-f1: 0.9367, Weighted-f1: 0.9630, Loss: 0.2003
[Epoch 5/50] Evaluation completed with Micro-f1: 0.5860, Macro-f1: 0.4357, Weighted-f1: 0.5596, Loss: 1.4573
Saved models from epoch 5 to 'data/predictions/almnps_8878/thesaurus_affinity/ood_validation/news/newest.pt'.
[Epoch 6/50] Train completed with Micro-f1: 0.9876, Macro-f1: 0.9765, Weighted-f1: 0.9867, Loss: 0.1197
[Epoch 6/50] Evaluation completed with Micro-f1: 0.5919, Macro-f1: 0.4430, Weighted-f1: 0.5685, Loss: 1.4553
Saved models from epoch 6 to 'data/predictions/almnps_8878/thesaurus_affinity/ood_validation/news/newest.pt'.
No improvement since 3 epochs (1.3430 loss). Early stop.
OOD training completed for test topic news after 6 epochs.
Loaded <torch.utils.data.dataloader.DataLoader object at 0x7f97031adf40> (train).
Loaded <torch.utils.data.dataloader.DataLoader object at 0x7f97035edeb0> (dev).
Starting training on ['ai', 'literature', 'music', 'news', 'science'] data with validation domain politics.
Read data from folder data/crossre_data/
Loaded <TransformerEmbeddings: dim=768>.
Using classifier: <LinearClassifier: emb_model = <TransformerEmbeddings: dim=768>>
Using criterion: <LabelLoss: loss=XEnt>.
Optimizing using: AdamW with learning rate 2e-05.
[Epoch 1/50] Train completed with Micro-f1: 0.3981, Macro-f1: 0.2446, Weighted-f1: 0.3323, Loss: 2.0819
[Epoch 1/50] Evaluation completed with Micro-f1: 0.5141, Macro-f1: 0.3419, Weighted-f1: 0.4656, Loss: 1.7092
Saved models from epoch 1 to 'data/predictions/almnps_8878/thesaurus_affinity/ood_validation/politics/newest.pt'.
Saved model with best loss 1.7092 to 'data/predictions/almnps_8878/thesaurus_affinity/ood_validation/politics/best.pt'.
[Epoch 2/50] Train completed with Micro-f1: 0.7008, Macro-f1: 0.5548, Weighted-f1: 0.6678, Loss: 1.1052
[Epoch 2/50] Evaluation completed with Micro-f1: 0.5347, Macro-f1: 0.3645, Weighted-f1: 0.4880, Loss: 1.4950
Saved models from epoch 2 to 'data/predictions/almnps_8878/thesaurus_affinity/ood_validation/politics/newest.pt'.
Saved model with best loss 1.4950 to 'data/predictions/almnps_8878/thesaurus_affinity/ood_validation/politics/best.pt'.
[Epoch 3/50] Train completed with Micro-f1: 0.8368, Macro-f1: 0.7311, Weighted-f1: 0.8252, Loss: 0.6826
[Epoch 3/50] Evaluation completed with Micro-f1: 0.5487, Macro-f1: 0.3825, Weighted-f1: 0.5075, Loss: 1.3930
Saved models from epoch 3 to 'data/predictions/almnps_8878/thesaurus_affinity/ood_validation/politics/newest.pt'.
Saved model with best loss 1.3930 to 'data/predictions/almnps_8878/thesaurus_affinity/ood_validation/politics/best.pt'.
[Epoch 4/50] Train completed with Micro-f1: 0.8911, Macro-f1: 0.8171, Weighted-f1: 0.8816, Loss: 0.4455
[Epoch 4/50] Evaluation completed with Micro-f1: 0.5586, Macro-f1: 0.3949, Weighted-f1: 0.5211, Loss: 1.3799
Saved models from epoch 4 to 'data/predictions/almnps_8878/thesaurus_affinity/ood_validation/politics/newest.pt'.
Saved model with best loss 1.3799 to 'data/predictions/almnps_8878/thesaurus_affinity/ood_validation/politics/best.pt'.
[Epoch 5/50] Train completed with Micro-f1: 0.9363, Macro-f1: 0.8910, Weighted-f1: 0.9338, Loss: 0.2963
[Epoch 5/50] Evaluation completed with Micro-f1: 0.5647, Macro-f1: 0.4037, Weighted-f1: 0.5304, Loss: 1.3739
Saved models from epoch 5 to 'data/predictions/almnps_8878/thesaurus_affinity/ood_validation/politics/newest.pt'.
Saved model with best loss 1.3739 to 'data/predictions/almnps_8878/thesaurus_affinity/ood_validation/politics/best.pt'.
[Epoch 6/50] Train completed with Micro-f1: 0.9678, Macro-f1: 0.9463, Weighted-f1: 0.9645, Loss: 0.1915
[Epoch 6/50] Evaluation completed with Micro-f1: 0.5720, Macro-f1: 0.4122, Weighted-f1: 0.5395, Loss: 1.3726
Saved models from epoch 6 to 'data/predictions/almnps_8878/thesaurus_affinity/ood_validation/politics/newest.pt'.
Saved model with best loss 1.3726 to 'data/predictions/almnps_8878/thesaurus_affinity/ood_validation/politics/best.pt'.
[Epoch 7/50] Train completed with Micro-f1: 0.9854, Macro-f1: 0.9710, Weighted-f1: 0.9834, Loss: 0.1192
[Epoch 7/50] Evaluation completed with Micro-f1: 0.5777, Macro-f1: 0.4178, Weighted-f1: 0.5465, Loss: 1.4052
Saved models from epoch 7 to 'data/predictions/almnps_8878/thesaurus_affinity/ood_validation/politics/newest.pt'.
[Epoch 8/50] Train completed with Micro-f1: 0.9915, Macro-f1: 0.9868, Weighted-f1: 0.9918, Loss: 0.0885
[Epoch 8/50] Evaluation completed with Micro-f1: 0.5814, Macro-f1: 0.4209, Weighted-f1: 0.5510, Loss: 1.4465
Saved models from epoch 8 to 'data/predictions/almnps_8878/thesaurus_affinity/ood_validation/politics/newest.pt'.
[Epoch 9/50] Train completed with Micro-f1: 0.9955, Macro-f1: 0.9925, Weighted-f1: 0.9953, Loss: 0.0630
[Epoch 9/50] Evaluation completed with Micro-f1: 0.5847, Macro-f1: 0.4248, Weighted-f1: 0.5551, Loss: 1.5060
Saved models from epoch 9 to 'data/predictions/almnps_8878/thesaurus_affinity/ood_validation/politics/newest.pt'.
No improvement since 3 epochs (1.3726 loss). Early stop.
OOD training completed for test topic politics after 9 epochs.
Loaded <torch.utils.data.dataloader.DataLoader object at 0x7f97c8cefee0> (train).
Loaded <torch.utils.data.dataloader.DataLoader object at 0x7f970319edc0> (dev).
Starting training on ['ai', 'literature', 'music', 'news', 'politics'] data with validation domain science.
Read data from folder data/crossre_data/
Loaded <TransformerEmbeddings: dim=768>.
Using classifier: <LinearClassifier: emb_model = <TransformerEmbeddings: dim=768>>
Using criterion: <LabelLoss: loss=XEnt>.
Optimizing using: AdamW with learning rate 2e-05.
[Epoch 1/50] Train completed with Micro-f1: 0.3942, Macro-f1: 0.2498, Weighted-f1: 0.3310, Loss: 2.0638
[Epoch 1/50] Evaluation completed with Micro-f1: 0.5483, Macro-f1: 0.3828, Weighted-f1: 0.5028, Loss: 1.6111
Saved models from epoch 1 to 'data/predictions/almnps_8878/thesaurus_affinity/ood_validation/science/newest.pt'.
Saved model with best loss 1.6111 to 'data/predictions/almnps_8878/thesaurus_affinity/ood_validation/science/best.pt'.
[Epoch 2/50] Train completed with Micro-f1: 0.7187, Macro-f1: 0.5731, Weighted-f1: 0.6897, Loss: 1.0446
[Epoch 2/50] Evaluation completed with Micro-f1: 0.5717, Macro-f1: 0.4086, Weighted-f1: 0.5323, Loss: 1.4041
Saved models from epoch 2 to 'data/predictions/almnps_8878/thesaurus_affinity/ood_validation/science/newest.pt'.
Saved model with best loss 1.4041 to 'data/predictions/almnps_8878/thesaurus_affinity/ood_validation/science/best.pt'.
[Epoch 3/50] Train completed with Micro-f1: 0.8373, Macro-f1: 0.7373, Weighted-f1: 0.8221, Loss: 0.6309
[Epoch 3/50] Evaluation completed with Micro-f1: 0.5812, Macro-f1: 0.4213, Weighted-f1: 0.5470, Loss: 1.3648
Saved models from epoch 3 to 'data/predictions/almnps_8878/thesaurus_affinity/ood_validation/science/newest.pt'.
Saved model with best loss 1.3648 to 'data/predictions/almnps_8878/thesaurus_affinity/ood_validation/science/best.pt'.
[Epoch 4/50] Train completed with Micro-f1: 0.9108, Macro-f1: 0.8453, Weighted-f1: 0.8993, Loss: 0.3830
[Epoch 4/50] Evaluation completed with Micro-f1: 0.5873, Macro-f1: 0.4311, Weighted-f1: 0.5565, Loss: 1.4145
Saved models from epoch 4 to 'data/predictions/almnps_8878/thesaurus_affinity/ood_validation/science/newest.pt'.
[Epoch 5/50] Train completed with Micro-f1: 0.9481, Macro-f1: 0.9044, Weighted-f1: 0.9448, Loss: 0.2501
[Epoch 5/50] Evaluation completed with Micro-f1: 0.5904, Macro-f1: 0.4347, Weighted-f1: 0.5615, Loss: 1.5607
Saved models from epoch 5 to 'data/predictions/almnps_8878/thesaurus_affinity/ood_validation/science/newest.pt'.
[Epoch 6/50] Train completed with Micro-f1: 0.9745, Macro-f1: 0.9528, Weighted-f1: 0.9727, Loss: 0.1559
[Epoch 6/50] Evaluation completed with Micro-f1: 0.5952, Macro-f1: 0.4395, Weighted-f1: 0.5679, Loss: 1.5771
Saved models from epoch 6 to 'data/predictions/almnps_8878/thesaurus_affinity/ood_validation/science/newest.pt'.
No improvement since 3 epochs (1.3648 loss). Early stop.
OOD training completed for test topic science after 6 epochs.
TRAINING COMPLETED with:
	Domains:		['ai', 'literature', 'music', 'news', 'politics', 'science']
	OOD validaation:	True
	Mapping type:		thesaurus_affinity
