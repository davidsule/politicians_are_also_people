Saved arguments to data/predictions\almnps_8878\thesaurus_affinity\ood_validation\args.json.
loading projection weights from C:\Users\petko/gensim-data\word2vec-google-news-300\word2vec-google-news-300.gz
KeyedVectors lifecycle event {'msg': 'loaded (3000000, 300) matrix of type float32 from C:\\Users\\petko/gensim-data\\word2vec-google-news-300\\word2vec-google-news-300.gz', 'binary': True, 'encoding': 'utf8', 'datetime': '2023-05-18T05:04:07.355482', 'gensim': '4.3.0', 'python': '3.9.16 (main, Mar  8 2023, 10:39:24) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22000-SP0', 'event': 'load_word2vec_format'}
Saved category mapping to data/predictions\almnps_8878\thesaurus_affinity\ood_validation\entity2category_mapping.json.
Loaded category mapping: thesaurus_affinity.
Loaded <torch.utils.data.dataloader.DataLoader object at 0x000001E63F339460> (train).
Loaded <torch.utils.data.dataloader.DataLoader object at 0x000001E63F275DF0> (dev).
Starting training on ['literature', 'music', 'news', 'politics', 'science'] data with validation domain ai.
Read data from folder data/crossre_data/
Loaded <TransformerEmbeddings: dim=768>.
Using classifier: <LinearClassifier: emb_model = <TransformerEmbeddings: dim=768>>
Using criterion: <LabelLoss: loss=XEnt>.
Optimizing using: AdamW with learning rate 2e-05.
[Epoch 1/50] Train completed with Micro-f1: 0.4247, Macro-f1: 0.2287, Weighted-f1: 0.3438, Loss: 2.0080
[Epoch 1/50] Evaluation completed with Micro-f1: 0.5056, Macro-f1: 0.3039, Weighted-f1: 0.4319, Loss: 1.7492
Saved models from epoch 1 to 'data/predictions\almnps_8878\thesaurus_affinity\ood_validation\ai\newest.pt'.
Saved model with best loss 1.7492 to 'data/predictions\almnps_8878\thesaurus_affinity\ood_validation\ai\best.pt'.
[Epoch 2/50] Train completed with Micro-f1: 0.7198, Macro-f1: 0.5679, Weighted-f1: 0.6800, Loss: 1.0349
[Epoch 2/50] Evaluation completed with Micro-f1: 0.5369, Macro-f1: 0.3449, Weighted-f1: 0.4782, Loss: 1.4627
Saved models from epoch 2 to 'data/predictions\almnps_8878\thesaurus_affinity\ood_validation\ai\newest.pt'.
Saved model with best loss 1.4627 to 'data/predictions\almnps_8878\thesaurus_affinity\ood_validation\ai\best.pt'.
[Epoch 3/50] Train completed with Micro-f1: 0.8391, Macro-f1: 0.7278, Weighted-f1: 0.8230, Loss: 0.6192
[Epoch 3/50] Evaluation completed with Micro-f1: 0.5471, Macro-f1: 0.3622, Weighted-f1: 0.4937, Loss: 1.4802
Saved models from epoch 3 to 'data/predictions\almnps_8878\thesaurus_affinity\ood_validation\ai\newest.pt'.
[Epoch 4/50] Train completed with Micro-f1: 0.9083, Macro-f1: 0.8326, Weighted-f1: 0.8996, Loss: 0.3947
[Epoch 4/50] Evaluation completed with Micro-f1: 0.5560, Macro-f1: 0.3757, Weighted-f1: 0.5061, Loss: 1.4759
Saved models from epoch 4 to 'data/predictions\almnps_8878\thesaurus_affinity\ood_validation\ai\newest.pt'.
[Epoch 5/50] Train completed with Micro-f1: 0.9518, Macro-f1: 0.9150, Weighted-f1: 0.9500, Loss: 0.2437
[Epoch 5/50] Evaluation completed with Micro-f1: 0.5623, Macro-f1: 0.3855, Weighted-f1: 0.5159, Loss: 1.4768
Saved models from epoch 5 to 'data/predictions\almnps_8878\thesaurus_affinity\ood_validation\ai\newest.pt'.
No improvement since 3 epochs (1.4627 loss). Early stop.
OOD training completed for test topic ai after 5 epochs.
Loaded <torch.utils.data.dataloader.DataLoader object at 0x000001E65B50F8E0> (train).
Loaded <torch.utils.data.dataloader.DataLoader object at 0x000001E63F339D90> (dev).
Starting training on ['ai', 'music', 'news', 'politics', 'science'] data with validation domain literature.
Read data from folder data/crossre_data/
Loaded <TransformerEmbeddings: dim=768>.
Using classifier: <LinearClassifier: emb_model = <TransformerEmbeddings: dim=768>>
Using criterion: <LabelLoss: loss=XEnt>.
Optimizing using: AdamW with learning rate 2e-05.
[Epoch 1/50] Train completed with Micro-f1: 0.3765, Macro-f1: 0.2219, Weighted-f1: 0.3072, Loss: 2.0950
[Epoch 1/50] Evaluation completed with Micro-f1: 0.4759, Macro-f1: 0.3175, Weighted-f1: 0.4230, Loss: 1.7552
Saved models from epoch 1 to 'data/predictions\almnps_8878\thesaurus_affinity\ood_validation\literature\newest.pt'.
Saved model with best loss 1.7552 to 'data/predictions\almnps_8878\thesaurus_affinity\ood_validation\literature\best.pt'.
[Epoch 2/50] Train completed with Micro-f1: 0.6845, Macro-f1: 0.5504, Weighted-f1: 0.6625, Loss: 1.1506
[Epoch 2/50] Evaluation completed with Micro-f1: 0.5063, Macro-f1: 0.3523, Weighted-f1: 0.4599, Loss: 1.5401
Saved models from epoch 2 to 'data/predictions\almnps_8878\thesaurus_affinity\ood_validation\literature\newest.pt'.
Saved model with best loss 1.5401 to 'data/predictions\almnps_8878\thesaurus_affinity\ood_validation\literature\best.pt'.
[Epoch 3/50] Train completed with Micro-f1: 0.8251, Macro-f1: 0.7325, Weighted-f1: 0.8115, Loss: 0.7204
[Epoch 3/50] Evaluation completed with Micro-f1: 0.5263, Macro-f1: 0.3745, Weighted-f1: 0.4850, Loss: 1.4361
Saved models from epoch 3 to 'data/predictions\almnps_8878\thesaurus_affinity\ood_validation\literature\newest.pt'.
Saved model with best loss 1.4361 to 'data/predictions\almnps_8878\thesaurus_affinity\ood_validation\literature\best.pt'.
[Epoch 4/50] Train completed with Micro-f1: 0.8985, Macro-f1: 0.8341, Weighted-f1: 0.8896, Loss: 0.4627
[Epoch 4/50] Evaluation completed with Micro-f1: 0.5390, Macro-f1: 0.3891, Weighted-f1: 0.5024, Loss: 1.4412
Saved models from epoch 4 to 'data/predictions\almnps_8878\thesaurus_affinity\ood_validation\literature\newest.pt'.
[Epoch 5/50] Train completed with Micro-f1: 0.9401, Macro-f1: 0.8918, Weighted-f1: 0.9346, Loss: 0.2925
[Epoch 5/50] Evaluation completed with Micro-f1: 0.5507, Macro-f1: 0.4023, Weighted-f1: 0.5166, Loss: 1.4125
Saved models from epoch 5 to 'data/predictions\almnps_8878\thesaurus_affinity\ood_validation\literature\newest.pt'.
Saved model with best loss 1.4125 to 'data/predictions\almnps_8878\thesaurus_affinity\ood_validation\literature\best.pt'.
[Epoch 6/50] Train completed with Micro-f1: 0.9709, Macro-f1: 0.9458, Weighted-f1: 0.9679, Loss: 0.1919
[Epoch 6/50] Evaluation completed with Micro-f1: 0.5565, Macro-f1: 0.4095, Weighted-f1: 0.5240, Loss: 1.4947
Saved models from epoch 6 to 'data/predictions\almnps_8878\thesaurus_affinity\ood_validation\literature\newest.pt'.
[Epoch 7/50] Train completed with Micro-f1: 0.9798, Macro-f1: 0.9622, Weighted-f1: 0.9783, Loss: 0.1380
[Epoch 7/50] Evaluation completed with Micro-f1: 0.5614, Macro-f1: 0.4165, Weighted-f1: 0.5310, Loss: 1.4699
Saved models from epoch 7 to 'data/predictions\almnps_8878\thesaurus_affinity\ood_validation\literature\newest.pt'.
[Epoch 8/50] Train completed with Micro-f1: 0.9894, Macro-f1: 0.9846, Weighted-f1: 0.9897, Loss: 0.0985
[Epoch 8/50] Evaluation completed with Micro-f1: 0.5658, Macro-f1: 0.4216, Weighted-f1: 0.5368, Loss: 1.5500
Saved models from epoch 8 to 'data/predictions\almnps_8878\thesaurus_affinity\ood_validation\literature\newest.pt'.
No improvement since 3 epochs (1.4125 loss). Early stop.
OOD training completed for test topic literature after 8 epochs.
Loaded <torch.utils.data.dataloader.DataLoader object at 0x000001E647BA1550> (train).
Loaded <torch.utils.data.dataloader.DataLoader object at 0x000001E67DCCA8B0> (dev).
Starting training on ['ai', 'literature', 'news', 'politics', 'science'] data with validation domain music.
Read data from folder data/crossre_data/
Loaded <TransformerEmbeddings: dim=768>.
Using classifier: <LinearClassifier: emb_model = <TransformerEmbeddings: dim=768>>
Using criterion: <LabelLoss: loss=XEnt>.
Optimizing using: AdamW with learning rate 2e-05.
[Epoch 1/50] Train completed with Micro-f1: 0.3286, Macro-f1: 0.1839, Weighted-f1: 0.2560, Loss: 2.2107
[Epoch 1/50] Evaluation completed with Micro-f1: 0.4442, Macro-f1: 0.2801, Weighted-f1: 0.3972, Loss: 1.9100
Saved models from epoch 1 to 'data/predictions\almnps_8878\thesaurus_affinity\ood_validation\music\newest.pt'.
Saved model with best loss 1.9100 to 'data/predictions\almnps_8878\thesaurus_affinity\ood_validation\music\best.pt'.
[Epoch 2/50] Train completed with Micro-f1: 0.6523, Macro-f1: 0.5142, Weighted-f1: 0.6211, Loss: 1.2728
[Epoch 2/50] Evaluation completed with Micro-f1: 0.4609, Macro-f1: 0.3040, Weighted-f1: 0.4179, Loss: 1.6958
Saved models from epoch 2 to 'data/predictions\almnps_8878\thesaurus_affinity\ood_validation\music\newest.pt'.
Saved model with best loss 1.6958 to 'data/predictions\almnps_8878\thesaurus_affinity\ood_validation\music\best.pt'.
[Epoch 3/50] Train completed with Micro-f1: 0.8077, Macro-f1: 0.6977, Weighted-f1: 0.7912, Loss: 0.8102
[Epoch 3/50] Evaluation completed with Micro-f1: 0.4820, Macro-f1: 0.3279, Weighted-f1: 0.4452, Loss: 1.5882
Saved models from epoch 3 to 'data/predictions\almnps_8878\thesaurus_affinity\ood_validation\music\newest.pt'.
Saved model with best loss 1.5882 to 'data/predictions\almnps_8878\thesaurus_affinity\ood_validation\music\best.pt'.
[Epoch 4/50] Train completed with Micro-f1: 0.8787, Macro-f1: 0.8056, Weighted-f1: 0.8678, Loss: 0.5296
[Epoch 4/50] Evaluation completed with Micro-f1: 0.4954, Macro-f1: 0.3450, Weighted-f1: 0.4632, Loss: 1.5140
Saved models from epoch 4 to 'data/predictions\almnps_8878\thesaurus_affinity\ood_validation\music\newest.pt'.
Saved model with best loss 1.5140 to 'data/predictions\almnps_8878\thesaurus_affinity\ood_validation\music\best.pt'.
[Epoch 5/50] Train completed with Micro-f1: 0.9408, Macro-f1: 0.9002, Weighted-f1: 0.9346, Loss: 0.3405
[Epoch 5/50] Evaluation completed with Micro-f1: 0.5055, Macro-f1: 0.3584, Weighted-f1: 0.4758, Loss: 1.5322
Saved models from epoch 5 to 'data/predictions\almnps_8878\thesaurus_affinity\ood_validation\music\newest.pt'.
[Epoch 6/50] Train completed with Micro-f1: 0.9676, Macro-f1: 0.9318, Weighted-f1: 0.9648, Loss: 0.2200
[Epoch 6/50] Evaluation completed with Micro-f1: 0.5132, Macro-f1: 0.3683, Weighted-f1: 0.4856, Loss: 1.5566
Saved models from epoch 6 to 'data/predictions\almnps_8878\thesaurus_affinity\ood_validation\music\newest.pt'.
[Epoch 7/50] Train completed with Micro-f1: 0.9855, Macro-f1: 0.9716, Weighted-f1: 0.9842, Loss: 0.1439
[Epoch 7/50] Evaluation completed with Micro-f1: 0.5198, Macro-f1: 0.3777, Weighted-f1: 0.4941, Loss: 1.5581
Saved models from epoch 7 to 'data/predictions\almnps_8878\thesaurus_affinity\ood_validation\music\newest.pt'.
No improvement since 3 epochs (1.5140 loss). Early stop.
OOD training completed for test topic music after 7 epochs.
Loaded <torch.utils.data.dataloader.DataLoader object at 0x000001E647D2CE80> (train).
Loaded <torch.utils.data.dataloader.DataLoader object at 0x000001E6480C6C40> (dev).
Starting training on ['ai', 'literature', 'music', 'politics', 'science'] data with validation domain news.
Read data from folder data/crossre_data/
Loaded <TransformerEmbeddings: dim=768>.
Using classifier: <LinearClassifier: emb_model = <TransformerEmbeddings: dim=768>>
Using criterion: <LabelLoss: loss=XEnt>.
Optimizing using: AdamW with learning rate 2e-05.
[Epoch 1/50] Train completed with Micro-f1: 0.3892, Macro-f1: 0.2314, Weighted-f1: 0.3308, Loss: 2.0654
[Epoch 1/50] Evaluation completed with Micro-f1: 0.5134, Macro-f1: 0.3325, Weighted-f1: 0.4602, Loss: 1.7256
Saved models from epoch 1 to 'data/predictions\almnps_8878\thesaurus_affinity\ood_validation\news\newest.pt'.
Saved model with best loss 1.7256 to 'data/predictions\almnps_8878\thesaurus_affinity\ood_validation\news\best.pt'.
[Epoch 2/50] Train completed with Micro-f1: 0.7148, Macro-f1: 0.5730, Weighted-f1: 0.6830, Loss: 1.0935
[Epoch 2/50] Evaluation completed with Micro-f1: 0.5383, Macro-f1: 0.3715, Weighted-f1: 0.4951, Loss: 1.4778
Saved models from epoch 2 to 'data/predictions\almnps_8878\thesaurus_affinity\ood_validation\news\newest.pt'.
Saved model with best loss 1.4778 to 'data/predictions\almnps_8878\thesaurus_affinity\ood_validation\news\best.pt'.
[Epoch 3/50] Train completed with Micro-f1: 0.8377, Macro-f1: 0.7381, Weighted-f1: 0.8219, Loss: 0.6623
[Epoch 3/50] Evaluation completed with Micro-f1: 0.5543, Macro-f1: 0.3928, Weighted-f1: 0.5169, Loss: 1.4069
Saved models from epoch 3 to 'data/predictions\almnps_8878\thesaurus_affinity\ood_validation\news\newest.pt'.
Saved model with best loss 1.4069 to 'data/predictions\almnps_8878\thesaurus_affinity\ood_validation\news\best.pt'.
[Epoch 4/50] Train completed with Micro-f1: 0.9031, Macro-f1: 0.8396, Weighted-f1: 0.8938, Loss: 0.4157
[Epoch 4/50] Evaluation completed with Micro-f1: 0.5660, Macro-f1: 0.4085, Weighted-f1: 0.5330, Loss: 1.3774
Saved models from epoch 4 to 'data/predictions\almnps_8878\thesaurus_affinity\ood_validation\news\newest.pt'.
Saved model with best loss 1.3774 to 'data/predictions\almnps_8878\thesaurus_affinity\ood_validation\news\best.pt'.
[Epoch 5/50] Train completed with Micro-f1: 0.9528, Macro-f1: 0.9140, Weighted-f1: 0.9491, Loss: 0.2618
[Epoch 5/50] Evaluation completed with Micro-f1: 0.5722, Macro-f1: 0.4154, Weighted-f1: 0.5407, Loss: 1.4104
Saved models from epoch 5 to 'data/predictions\almnps_8878\thesaurus_affinity\ood_validation\news\newest.pt'.
[Epoch 6/50] Train completed with Micro-f1: 0.9787, Macro-f1: 0.9624, Weighted-f1: 0.9766, Loss: 0.1626
[Epoch 6/50] Evaluation completed with Micro-f1: 0.5780, Macro-f1: 0.4223, Weighted-f1: 0.5474, Loss: 1.4862
Saved models from epoch 6 to 'data/predictions\almnps_8878\thesaurus_affinity\ood_validation\news\newest.pt'.
[Epoch 7/50] Train completed with Micro-f1: 0.9858, Macro-f1: 0.9736, Weighted-f1: 0.9850, Loss: 0.1165
[Epoch 7/50] Evaluation completed with Micro-f1: 0.5826, Macro-f1: 0.4274, Weighted-f1: 0.5532, Loss: 1.5109
Saved models from epoch 7 to 'data/predictions\almnps_8878\thesaurus_affinity\ood_validation\news\newest.pt'.
No improvement since 3 epochs (1.3774 loss). Early stop.
OOD training completed for test topic news after 7 epochs.
Loaded <torch.utils.data.dataloader.DataLoader object at 0x000001E72E497670> (train).
Loaded <torch.utils.data.dataloader.DataLoader object at 0x000001E69EE0B4C0> (dev).
Starting training on ['ai', 'literature', 'music', 'news', 'science'] data with validation domain politics.
Read data from folder data/crossre_data/
Loaded <TransformerEmbeddings: dim=768>.
Using classifier: <LinearClassifier: emb_model = <TransformerEmbeddings: dim=768>>
Using criterion: <LabelLoss: loss=XEnt>.
Optimizing using: AdamW with learning rate 2e-05.
[Epoch 1/50] Train completed with Micro-f1: 0.3821, Macro-f1: 0.2323, Weighted-f1: 0.3265, Loss: 2.1278
[Epoch 1/50] Evaluation completed with Micro-f1: 0.4751, Macro-f1: 0.2848, Weighted-f1: 0.4147, Loss: 1.7887
Saved models from epoch 1 to 'data/predictions\almnps_8878\thesaurus_affinity\ood_validation\politics\newest.pt'.
Saved model with best loss 1.7887 to 'data/predictions\almnps_8878\thesaurus_affinity\ood_validation\politics\best.pt'.
[Epoch 2/50] Train completed with Micro-f1: 0.6732, Macro-f1: 0.5154, Weighted-f1: 0.6348, Loss: 1.1730
[Epoch 2/50] Evaluation completed with Micro-f1: 0.5195, Macro-f1: 0.3354, Weighted-f1: 0.4704, Loss: 1.4800
Saved models from epoch 2 to 'data/predictions\almnps_8878\thesaurus_affinity\ood_validation\politics\newest.pt'.
Saved model with best loss 1.4800 to 'data/predictions\almnps_8878\thesaurus_affinity\ood_validation\politics\best.pt'.
[Epoch 3/50] Train completed with Micro-f1: 0.8296, Macro-f1: 0.7142, Weighted-f1: 0.8101, Loss: 0.7324
[Epoch 3/50] Evaluation completed with Micro-f1: 0.5431, Macro-f1: 0.3652, Weighted-f1: 0.4990, Loss: 1.3935
Saved models from epoch 3 to 'data/predictions\almnps_8878\thesaurus_affinity\ood_validation\politics\newest.pt'.
Saved model with best loss 1.3935 to 'data/predictions\almnps_8878\thesaurus_affinity\ood_validation\politics\best.pt'.
[Epoch 4/50] Train completed with Micro-f1: 0.8895, Macro-f1: 0.8112, Weighted-f1: 0.8827, Loss: 0.4875
[Epoch 4/50] Evaluation completed with Micro-f1: 0.5596, Macro-f1: 0.3854, Weighted-f1: 0.5205, Loss: 1.3277
Saved models from epoch 4 to 'data/predictions\almnps_8878\thesaurus_affinity\ood_validation\politics\newest.pt'.
Saved model with best loss 1.3277 to 'data/predictions\almnps_8878\thesaurus_affinity\ood_validation\politics\best.pt'.
[Epoch 5/50] Train completed with Micro-f1: 0.9302, Macro-f1: 0.8722, Weighted-f1: 0.9249, Loss: 0.3241
[Epoch 5/50] Evaluation completed with Micro-f1: 0.5695, Macro-f1: 0.3971, Weighted-f1: 0.5332, Loss: 1.3488
Saved models from epoch 5 to 'data/predictions\almnps_8878\thesaurus_affinity\ood_validation\politics\newest.pt'.
[Epoch 6/50] Train completed with Micro-f1: 0.9643, Macro-f1: 0.9345, Weighted-f1: 0.9607, Loss: 0.2149
[Epoch 6/50] Evaluation completed with Micro-f1: 0.5772, Macro-f1: 0.4081, Weighted-f1: 0.5431, Loss: 1.3379
Saved models from epoch 6 to 'data/predictions\almnps_8878\thesaurus_affinity\ood_validation\politics\newest.pt'.
[Epoch 7/50] Train completed with Micro-f1: 0.9838, Macro-f1: 0.9677, Weighted-f1: 0.9813, Loss: 0.1398
[Epoch 7/50] Evaluation completed with Micro-f1: 0.5835, Macro-f1: 0.4159, Weighted-f1: 0.5511, Loss: 1.3478
Saved models from epoch 7 to 'data/predictions\almnps_8878\thesaurus_affinity\ood_validation\politics\newest.pt'.
No improvement since 3 epochs (1.3277 loss). Early stop.
OOD training completed for test topic politics after 7 epochs.
Loaded <torch.utils.data.dataloader.DataLoader object at 0x000001E69A5ABF40> (train).
Loaded <torch.utils.data.dataloader.DataLoader object at 0x000001E72E4613D0> (dev).
Starting training on ['ai', 'literature', 'music', 'news', 'politics'] data with validation domain science.
Read data from folder data/crossre_data/
Loaded <TransformerEmbeddings: dim=768>.
Using classifier: <LinearClassifier: emb_model = <TransformerEmbeddings: dim=768>>
Using criterion: <LabelLoss: loss=XEnt>.
Optimizing using: AdamW with learning rate 2e-05.
[Epoch 1/50] Train completed with Micro-f1: 0.3706, Macro-f1: 0.2024, Weighted-f1: 0.2964, Loss: 2.1544
[Epoch 1/50] Evaluation completed with Micro-f1: 0.5278, Macro-f1: 0.3510, Weighted-f1: 0.4818, Loss: 1.6689
Saved models from epoch 1 to 'data/predictions\almnps_8878\thesaurus_affinity\ood_validation\science\newest.pt'.
Saved model with best loss 1.6689 to 'data/predictions\almnps_8878\thesaurus_affinity\ood_validation\science\best.pt'.
[Epoch 2/50] Train completed with Micro-f1: 0.6905, Macro-f1: 0.5248, Weighted-f1: 0.6558, Loss: 1.1476
[Epoch 2/50] Evaluation completed with Micro-f1: 0.5446, Macro-f1: 0.3730, Weighted-f1: 0.5055, Loss: 1.4702
Saved models from epoch 2 to 'data/predictions\almnps_8878\thesaurus_affinity\ood_validation\science\newest.pt'.
Saved model with best loss 1.4702 to 'data/predictions\almnps_8878\thesaurus_affinity\ood_validation\science\best.pt'.
[Epoch 3/50] Train completed with Micro-f1: 0.8207, Macro-f1: 0.7021, Weighted-f1: 0.7967, Loss: 0.7140
[Epoch 3/50] Evaluation completed with Micro-f1: 0.5569, Macro-f1: 0.3906, Weighted-f1: 0.5224, Loss: 1.4010
Saved models from epoch 3 to 'data/predictions\almnps_8878\thesaurus_affinity\ood_validation\science\newest.pt'.
Saved model with best loss 1.4010 to 'data/predictions\almnps_8878\thesaurus_affinity\ood_validation\science\best.pt'.
[Epoch 4/50] Train completed with Micro-f1: 0.8895, Macro-f1: 0.8049, Weighted-f1: 0.8797, Loss: 0.4671
[Epoch 4/50] Evaluation completed with Micro-f1: 0.5651, Macro-f1: 0.4020, Weighted-f1: 0.5340, Loss: 1.4396
Saved models from epoch 4 to 'data/predictions\almnps_8878\thesaurus_affinity\ood_validation\science\newest.pt'.
[Epoch 5/50] Train completed with Micro-f1: 0.9395, Macro-f1: 0.8861, Weighted-f1: 0.9329, Loss: 0.2957
[Epoch 5/50] Evaluation completed with Micro-f1: 0.5713, Macro-f1: 0.4099, Weighted-f1: 0.5417, Loss: 1.4683
Saved models from epoch 5 to 'data/predictions\almnps_8878\thesaurus_affinity\ood_validation\science\newest.pt'.
[Epoch 6/50] Train completed with Micro-f1: 0.9635, Macro-f1: 0.9411, Weighted-f1: 0.9605, Loss: 0.1975
[Epoch 6/50] Evaluation completed with Micro-f1: 0.5777, Macro-f1: 0.4177, Weighted-f1: 0.5495, Loss: 1.4492
Saved models from epoch 6 to 'data/predictions\almnps_8878\thesaurus_affinity\ood_validation\science\newest.pt'.
No improvement since 3 epochs (1.4010 loss). Early stop.
OOD training completed for test topic science after 6 epochs.
TRAINING COMPLETED with:
	Domains:		['ai', 'literature', 'music', 'news', 'politics', 'science']
	OOD validaation:	True
	Mapping type:		thesaurus_affinity
