Saved arguments to data/predictions/almnps_8878/thesaurus_affinity/ood/args.json.
loading projection weights from /home/davidsule/gensim-data/word2vec-google-news-300/word2vec-google-news-300.gz
KeyedVectors lifecycle event {'msg': 'loaded (3000000, 300) matrix of type float32 from /home/davidsule/gensim-data/word2vec-google-news-300/word2vec-google-news-300.gz', 'binary': True, 'encoding': 'utf8', 'datetime': '2023-05-08T05:06:33.742594', 'gensim': '4.3.0', 'python': '3.9.16 | packaged by conda-forge | (main, Feb  1 2023, 21:39:03) \n[GCC 11.3.0]', 'platform': 'Linux-5.15.90.1-microsoft-standard-WSL2-x86_64-with-glibc2.31', 'event': 'load_word2vec_format'}
Saved category mapping to data/predictions/almnps_8878/thesaurus_affinity/ood/mapping.json.
Loaded category mapping: thesaurus_affinity.
Loaded <torch.utils.data.dataloader.DataLoader object at 0x7f59f0ac34f0> (train).
Loaded <torch.utils.data.dataloader.DataLoader object at 0x7f59f0986250> (dev).
Starting training on ['literature', 'music', 'news', 'politics', 'science'] data.
Loaded <TransformerEmbeddings: dim=768>.
Using classifier: <LinearClassifier: emb_model = <TransformerEmbeddings: dim=768>>
Using criterion: <LabelLoss: loss=XEnt>.
Optimizing using: AdamW with learning rate 2e-05.
[Epoch 1/50] Train completed with Micro-f1: 0.4037, Macro-f1: 0.2204, Weighted-f1: 0.3303, Loss: 2.0442
[Epoch 1/50] Evaluation completed with Micro-f1: 0.5079, Macro-f1: 0.3177, Weighted-f1: 0.4388, Loss: 1.7407
Saved models from epoch 1 to 'data/predictions/almnps_8878/thesaurus_affinity/ood/ai/newest.pt'.
Saved model with best loss 1.7407 to 'data/predictions/almnps_8878/thesaurus_affinity/ood/ai/best.pt'.
[Epoch 2/50] Train completed with Micro-f1: 0.7106, Macro-f1: 0.5559, Weighted-f1: 0.6702, Loss: 1.0719
[Epoch 2/50] Evaluation completed with Micro-f1: 0.5369, Macro-f1: 0.3505, Weighted-f1: 0.4805, Loss: 1.4631
Saved models from epoch 2 to 'data/predictions/almnps_8878/thesaurus_affinity/ood/ai/newest.pt'.
Saved model with best loss 1.4631 to 'data/predictions/almnps_8878/thesaurus_affinity/ood/ai/best.pt'.
[Epoch 3/50] Train completed with Micro-f1: 0.8438, Macro-f1: 0.7271, Weighted-f1: 0.8249, Loss: 0.6170
[Epoch 3/50] Evaluation completed with Micro-f1: 0.5508, Macro-f1: 0.3702, Weighted-f1: 0.4999, Loss: 1.4336
Saved models from epoch 3 to 'data/predictions/almnps_8878/thesaurus_affinity/ood/ai/newest.pt'.
Saved model with best loss 1.4336 to 'data/predictions/almnps_8878/thesaurus_affinity/ood/ai/best.pt'.
[Epoch 4/50] Train completed with Micro-f1: 0.9150, Macro-f1: 0.8423, Weighted-f1: 0.9063, Loss: 0.3820
[Epoch 4/50] Evaluation completed with Micro-f1: 0.5585, Macro-f1: 0.3822, Weighted-f1: 0.5108, Loss: 1.4699
Saved models from epoch 4 to 'data/predictions/almnps_8878/thesaurus_affinity/ood/ai/newest.pt'.
[Epoch 5/50] Train completed with Micro-f1: 0.9488, Macro-f1: 0.8969, Weighted-f1: 0.9472, Loss: 0.2446
[Epoch 5/50] Evaluation completed with Micro-f1: 0.5653, Macro-f1: 0.3922, Weighted-f1: 0.5203, Loss: 1.4464
Saved models from epoch 5 to 'data/predictions/almnps_8878/thesaurus_affinity/ood/ai/newest.pt'.
[Epoch 6/50] Train completed with Micro-f1: 0.9734, Macro-f1: 0.9447, Weighted-f1: 0.9719, Loss: 0.1603
[Epoch 6/50] Evaluation completed with Micro-f1: 0.5700, Macro-f1: 0.3988, Weighted-f1: 0.5258, Loss: 1.5395
Saved models from epoch 6 to 'data/predictions/almnps_8878/thesaurus_affinity/ood/ai/newest.pt'.
No improvement since 3 epochs (1.4336 loss). Early stop.
OOD training completed for test topic ai after 6 epochs.
Loaded <torch.utils.data.dataloader.DataLoader object at 0x7f59d845b070> (train).
Loaded <torch.utils.data.dataloader.DataLoader object at 0x7f59f9382f40> (dev).
Starting training on ['ai', 'music', 'news', 'politics', 'science'] data.
Loaded <TransformerEmbeddings: dim=768>.
Using classifier: <LinearClassifier: emb_model = <TransformerEmbeddings: dim=768>>
Using criterion: <LabelLoss: loss=XEnt>.
Optimizing using: AdamW with learning rate 2e-05.
[Epoch 1/50] Train completed with Micro-f1: 0.3622, Macro-f1: 0.1964, Weighted-f1: 0.2901, Loss: 2.1235
[Epoch 1/50] Evaluation completed with Micro-f1: 0.4677, Macro-f1: 0.3192, Weighted-f1: 0.4170, Loss: 1.7919
Saved models from epoch 1 to 'data/predictions/almnps_8878/thesaurus_affinity/ood/literature/newest.pt'.
Saved model with best loss 1.7919 to 'data/predictions/almnps_8878/thesaurus_affinity/ood/literature/best.pt'.
[Epoch 2/50] Train completed with Micro-f1: 0.6935, Macro-f1: 0.5754, Weighted-f1: 0.6656, Loss: 1.1234
[Epoch 2/50] Evaluation completed with Micro-f1: 0.5010, Macro-f1: 0.3534, Weighted-f1: 0.4600, Loss: 1.5355
Saved models from epoch 2 to 'data/predictions/almnps_8878/thesaurus_affinity/ood/literature/newest.pt'.
Saved model with best loss 1.5355 to 'data/predictions/almnps_8878/thesaurus_affinity/ood/literature/best.pt'.
[Epoch 3/50] Train completed with Micro-f1: 0.8249, Macro-f1: 0.7300, Weighted-f1: 0.8101, Loss: 0.6891
[Epoch 3/50] Evaluation completed with Micro-f1: 0.5207, Macro-f1: 0.3760, Weighted-f1: 0.4853, Loss: 1.4775
Saved models from epoch 3 to 'data/predictions/almnps_8878/thesaurus_affinity/ood/literature/newest.pt'.
Saved model with best loss 1.4775 to 'data/predictions/almnps_8878/thesaurus_affinity/ood/literature/best.pt'.
[Epoch 4/50] Train completed with Micro-f1: 0.9015, Macro-f1: 0.8304, Weighted-f1: 0.8952, Loss: 0.4313
[Epoch 4/50] Evaluation completed with Micro-f1: 0.5336, Macro-f1: 0.3888, Weighted-f1: 0.5001, Loss: 1.4612
Saved models from epoch 4 to 'data/predictions/almnps_8878/thesaurus_affinity/ood/literature/newest.pt'.
Saved model with best loss 1.4612 to 'data/predictions/almnps_8878/thesaurus_affinity/ood/literature/best.pt'.
[Epoch 5/50] Train completed with Micro-f1: 0.9372, Macro-f1: 0.8877, Weighted-f1: 0.9304, Loss: 0.2945
[Epoch 5/50] Evaluation completed with Micro-f1: 0.5417, Macro-f1: 0.3988, Weighted-f1: 0.5108, Loss: 1.4731
Saved models from epoch 5 to 'data/predictions/almnps_8878/thesaurus_affinity/ood/literature/newest.pt'.
[Epoch 6/50] Train completed with Micro-f1: 0.9655, Macro-f1: 0.9310, Weighted-f1: 0.9654, Loss: 0.1899
[Epoch 6/50] Evaluation completed with Micro-f1: 0.5481, Macro-f1: 0.4070, Weighted-f1: 0.5195, Loss: 1.4868
Saved models from epoch 6 to 'data/predictions/almnps_8878/thesaurus_affinity/ood/literature/newest.pt'.
[Epoch 7/50] Train completed with Micro-f1: 0.9846, Macro-f1: 0.9724, Weighted-f1: 0.9836, Loss: 0.1277
[Epoch 7/50] Evaluation completed with Micro-f1: 0.5529, Macro-f1: 0.4129, Weighted-f1: 0.5262, Loss: 1.5290
Saved models from epoch 7 to 'data/predictions/almnps_8878/thesaurus_affinity/ood/literature/newest.pt'.
No improvement since 3 epochs (1.4612 loss). Early stop.
OOD training completed for test topic literature after 7 epochs.
Loaded <torch.utils.data.dataloader.DataLoader object at 0x7f5a4853c190> (train).
Loaded <torch.utils.data.dataloader.DataLoader object at 0x7f59d83fdf10> (dev).
Starting training on ['ai', 'literature', 'news', 'politics', 'science'] data.
Loaded <TransformerEmbeddings: dim=768>.
Using classifier: <LinearClassifier: emb_model = <TransformerEmbeddings: dim=768>>
Using criterion: <LabelLoss: loss=XEnt>.
Optimizing using: AdamW with learning rate 2e-05.
[Epoch 1/50] Train completed with Micro-f1: 0.3252, Macro-f1: 0.1702, Weighted-f1: 0.2437, Loss: 2.2221
[Epoch 1/50] Evaluation completed with Micro-f1: 0.4425, Macro-f1: 0.2808, Weighted-f1: 0.3986, Loss: 1.9337
Saved models from epoch 1 to 'data/predictions/almnps_8878/thesaurus_affinity/ood/music/newest.pt'.
Saved model with best loss 1.9337 to 'data/predictions/almnps_8878/thesaurus_affinity/ood/music/best.pt'.
[Epoch 2/50] Train completed with Micro-f1: 0.6510, Macro-f1: 0.5056, Weighted-f1: 0.6137, Loss: 1.2702
[Epoch 2/50] Evaluation completed with Micro-f1: 0.4617, Macro-f1: 0.3041, Weighted-f1: 0.4178, Loss: 1.6994
Saved models from epoch 2 to 'data/predictions/almnps_8878/thesaurus_affinity/ood/music/newest.pt'.
Saved model with best loss 1.6994 to 'data/predictions/almnps_8878/thesaurus_affinity/ood/music/best.pt'.
[Epoch 3/50] Train completed with Micro-f1: 0.7937, Macro-f1: 0.6911, Weighted-f1: 0.7771, Loss: 0.7962
[Epoch 3/50] Evaluation completed with Micro-f1: 0.4868, Macro-f1: 0.3298, Weighted-f1: 0.4479, Loss: 1.5816
Saved models from epoch 3 to 'data/predictions/almnps_8878/thesaurus_affinity/ood/music/newest.pt'.
Saved model with best loss 1.5816 to 'data/predictions/almnps_8878/thesaurus_affinity/ood/music/best.pt'.
[Epoch 4/50] Train completed with Micro-f1: 0.8750, Macro-f1: 0.8001, Weighted-f1: 0.8633, Loss: 0.5254
[Epoch 4/50] Evaluation completed with Micro-f1: 0.5010, Macro-f1: 0.3461, Weighted-f1: 0.4655, Loss: 1.5323
Saved models from epoch 4 to 'data/predictions/almnps_8878/thesaurus_affinity/ood/music/newest.pt'.
Saved model with best loss 1.5323 to 'data/predictions/almnps_8878/thesaurus_affinity/ood/music/best.pt'.
[Epoch 5/50] Train completed with Micro-f1: 0.9364, Macro-f1: 0.8918, Weighted-f1: 0.9294, Loss: 0.3323
[Epoch 5/50] Evaluation completed with Micro-f1: 0.5113, Macro-f1: 0.3574, Weighted-f1: 0.4770, Loss: 1.5735
Saved models from epoch 5 to 'data/predictions/almnps_8878/thesaurus_affinity/ood/music/newest.pt'.
[Epoch 6/50] Train completed with Micro-f1: 0.9676, Macro-f1: 0.9360, Weighted-f1: 0.9650, Loss: 0.2215
[Epoch 6/50] Evaluation completed with Micro-f1: 0.5189, Macro-f1: 0.3676, Weighted-f1: 0.4872, Loss: 1.5775
Saved models from epoch 6 to 'data/predictions/almnps_8878/thesaurus_affinity/ood/music/newest.pt'.
[Epoch 7/50] Train completed with Micro-f1: 0.9834, Macro-f1: 0.9668, Weighted-f1: 0.9837, Loss: 0.1473
[Epoch 7/50] Evaluation completed with Micro-f1: 0.5248, Macro-f1: 0.3765, Weighted-f1: 0.4947, Loss: 1.6330
Saved models from epoch 7 to 'data/predictions/almnps_8878/thesaurus_affinity/ood/music/newest.pt'.
No improvement since 3 epochs (1.5323 loss). Early stop.
OOD training completed for test topic music after 7 epochs.
Loaded <torch.utils.data.dataloader.DataLoader object at 0x7f59d82243a0> (train).
Loaded <torch.utils.data.dataloader.DataLoader object at 0x7f5a4853c190> (dev).
Starting training on ['ai', 'literature', 'music', 'politics', 'science'] data.
Loaded <TransformerEmbeddings: dim=768>.
Using classifier: <LinearClassifier: emb_model = <TransformerEmbeddings: dim=768>>
Using criterion: <LabelLoss: loss=XEnt>.
Optimizing using: AdamW with learning rate 2e-05.
[Epoch 1/50] Train completed with Micro-f1: 0.4119, Macro-f1: 0.2520, Weighted-f1: 0.3489, Loss: 2.0570
[Epoch 1/50] Evaluation completed with Micro-f1: 0.5180, Macro-f1: 0.3376, Weighted-f1: 0.4655, Loss: 1.7075
Saved models from epoch 1 to 'data/predictions/almnps_8878/thesaurus_affinity/ood/news/newest.pt'.
Saved model with best loss 1.7075 to 'data/predictions/almnps_8878/thesaurus_affinity/ood/news/best.pt'.
[Epoch 2/50] Train completed with Micro-f1: 0.7055, Macro-f1: 0.5573, Weighted-f1: 0.6712, Loss: 1.0825
[Epoch 2/50] Evaluation completed with Micro-f1: 0.5374, Macro-f1: 0.3682, Weighted-f1: 0.4915, Loss: 1.4839
Saved models from epoch 2 to 'data/predictions/almnps_8878/thesaurus_affinity/ood/news/newest.pt'.
Saved model with best loss 1.4839 to 'data/predictions/almnps_8878/thesaurus_affinity/ood/news/best.pt'.
[Epoch 3/50] Train completed with Micro-f1: 0.8373, Macro-f1: 0.7381, Weighted-f1: 0.8218, Loss: 0.6547
[Epoch 3/50] Evaluation completed with Micro-f1: 0.5527, Macro-f1: 0.3885, Weighted-f1: 0.5136, Loss: 1.4020
Saved models from epoch 3 to 'data/predictions/almnps_8878/thesaurus_affinity/ood/news/newest.pt'.
Saved model with best loss 1.4020 to 'data/predictions/almnps_8878/thesaurus_affinity/ood/news/best.pt'.
[Epoch 4/50] Train completed with Micro-f1: 0.8999, Macro-f1: 0.8271, Weighted-f1: 0.8902, Loss: 0.4160
[Epoch 4/50] Evaluation completed with Micro-f1: 0.5620, Macro-f1: 0.4020, Weighted-f1: 0.5275, Loss: 1.3821
Saved models from epoch 4 to 'data/predictions/almnps_8878/thesaurus_affinity/ood/news/newest.pt'.
Saved model with best loss 1.3821 to 'data/predictions/almnps_8878/thesaurus_affinity/ood/news/best.pt'.
[Epoch 5/50] Train completed with Micro-f1: 0.9457, Macro-f1: 0.9082, Weighted-f1: 0.9408, Loss: 0.2588
[Epoch 5/50] Evaluation completed with Micro-f1: 0.5691, Macro-f1: 0.4097, Weighted-f1: 0.5361, Loss: 1.4005
Saved models from epoch 5 to 'data/predictions/almnps_8878/thesaurus_affinity/ood/news/newest.pt'.
[Epoch 6/50] Train completed with Micro-f1: 0.9730, Macro-f1: 0.9531, Weighted-f1: 0.9714, Loss: 0.1638
[Epoch 6/50] Evaluation completed with Micro-f1: 0.5746, Macro-f1: 0.4163, Weighted-f1: 0.5424, Loss: 1.4638
Saved models from epoch 6 to 'data/predictions/almnps_8878/thesaurus_affinity/ood/news/newest.pt'.
[Epoch 7/50] Train completed with Micro-f1: 0.9882, Macro-f1: 0.9789, Weighted-f1: 0.9876, Loss: 0.1077
[Epoch 7/50] Evaluation completed with Micro-f1: 0.5793, Macro-f1: 0.4222, Weighted-f1: 0.5495, Loss: 1.5245
Saved models from epoch 7 to 'data/predictions/almnps_8878/thesaurus_affinity/ood/news/newest.pt'.
No improvement since 3 epochs (1.3821 loss). Early stop.
OOD training completed for test topic news after 7 epochs.
Loaded <torch.utils.data.dataloader.DataLoader object at 0x7f59d8148220> (train).
Loaded <torch.utils.data.dataloader.DataLoader object at 0x7f59d824c1c0> (dev).
Starting training on ['ai', 'literature', 'music', 'news', 'science'] data.
Loaded <TransformerEmbeddings: dim=768>.
Using classifier: <LinearClassifier: emb_model = <TransformerEmbeddings: dim=768>>
Using criterion: <LabelLoss: loss=XEnt>.
Optimizing using: AdamW with learning rate 2e-05.
[Epoch 1/50] Train completed with Micro-f1: 0.3774, Macro-f1: 0.2250, Weighted-f1: 0.3228, Loss: 2.1337
[Epoch 1/50] Evaluation completed with Micro-f1: 0.4761, Macro-f1: 0.2813, Weighted-f1: 0.4115, Loss: 1.7922
Saved models from epoch 1 to 'data/predictions/almnps_8878/thesaurus_affinity/ood/politics/newest.pt'.
Saved model with best loss 1.7922 to 'data/predictions/almnps_8878/thesaurus_affinity/ood/politics/best.pt'.
[Epoch 2/50] Train completed with Micro-f1: 0.6711, Macro-f1: 0.5021, Weighted-f1: 0.6280, Loss: 1.2061
[Epoch 2/50] Evaluation completed with Micro-f1: 0.5176, Macro-f1: 0.3341, Weighted-f1: 0.4674, Loss: 1.4865
Saved models from epoch 2 to 'data/predictions/almnps_8878/thesaurus_affinity/ood/politics/newest.pt'.
Saved model with best loss 1.4865 to 'data/predictions/almnps_8878/thesaurus_affinity/ood/politics/best.pt'.
[Epoch 3/50] Train completed with Micro-f1: 0.8045, Macro-f1: 0.6798, Weighted-f1: 0.7831, Loss: 0.7702
[Epoch 3/50] Evaluation completed with Micro-f1: 0.5397, Macro-f1: 0.3604, Weighted-f1: 0.4949, Loss: 1.3953
Saved models from epoch 3 to 'data/predictions/almnps_8878/thesaurus_affinity/ood/politics/newest.pt'.
Saved model with best loss 1.3953 to 'data/predictions/almnps_8878/thesaurus_affinity/ood/politics/best.pt'.
[Epoch 4/50] Train completed with Micro-f1: 0.8873, Macro-f1: 0.8096, Weighted-f1: 0.8781, Loss: 0.4946
[Epoch 4/50] Evaluation completed with Micro-f1: 0.5526, Macro-f1: 0.3772, Weighted-f1: 0.5138, Loss: 1.3523
Saved models from epoch 4 to 'data/predictions/almnps_8878/thesaurus_affinity/ood/politics/newest.pt'.
Saved model with best loss 1.3523 to 'data/predictions/almnps_8878/thesaurus_affinity/ood/politics/best.pt'.
[Epoch 5/50] Train completed with Micro-f1: 0.9297, Macro-f1: 0.8754, Weighted-f1: 0.9251, Loss: 0.3295
[Epoch 5/50] Evaluation completed with Micro-f1: 0.5616, Macro-f1: 0.3883, Weighted-f1: 0.5258, Loss: 1.3860
Saved models from epoch 5 to 'data/predictions/almnps_8878/thesaurus_affinity/ood/politics/newest.pt'.
[Epoch 6/50] Train completed with Micro-f1: 0.9671, Macro-f1: 0.9406, Weighted-f1: 0.9647, Loss: 0.2100
[Epoch 6/50] Evaluation completed with Micro-f1: 0.5680, Macro-f1: 0.3978, Weighted-f1: 0.5344, Loss: 1.4062
Saved models from epoch 6 to 'data/predictions/almnps_8878/thesaurus_affinity/ood/politics/newest.pt'.
[Epoch 7/50] Train completed with Micro-f1: 0.9805, Macro-f1: 0.9665, Weighted-f1: 0.9787, Loss: 0.1444
[Epoch 7/50] Evaluation completed with Micro-f1: 0.5743, Macro-f1: 0.4055, Weighted-f1: 0.5424, Loss: 1.4040
Saved models from epoch 7 to 'data/predictions/almnps_8878/thesaurus_affinity/ood/politics/newest.pt'.
No improvement since 3 epochs (1.3523 loss). Early stop.
OOD training completed for test topic politics after 7 epochs.
Loaded <torch.utils.data.dataloader.DataLoader object at 0x7f59047ab0a0> (train).
Loaded <torch.utils.data.dataloader.DataLoader object at 0x7f59047c7eb0> (dev).
Starting training on ['ai', 'literature', 'music', 'news', 'politics'] data.
Loaded <TransformerEmbeddings: dim=768>.
Using classifier: <LinearClassifier: emb_model = <TransformerEmbeddings: dim=768>>
Using criterion: <LabelLoss: loss=XEnt>.
Optimizing using: AdamW with learning rate 2e-05.
[Epoch 1/50] Train completed with Micro-f1: 0.3685, Macro-f1: 0.1971, Weighted-f1: 0.2902, Loss: 2.1540
[Epoch 1/50] Evaluation completed with Micro-f1: 0.5211, Macro-f1: 0.3414, Weighted-f1: 0.4758, Loss: 1.7008
Saved models from epoch 1 to 'data/predictions/almnps_8878/thesaurus_affinity/ood/science/newest.pt'.
Saved model with best loss 1.7008 to 'data/predictions/almnps_8878/thesaurus_affinity/ood/science/best.pt'.
[Epoch 2/50] Train completed with Micro-f1: 0.6837, Macro-f1: 0.5207, Weighted-f1: 0.6477, Loss: 1.1811
[Epoch 2/50] Evaluation completed with Micro-f1: 0.5409, Macro-f1: 0.3676, Weighted-f1: 0.5019, Loss: 1.4755
Saved models from epoch 2 to 'data/predictions/almnps_8878/thesaurus_affinity/ood/science/newest.pt'.
Saved model with best loss 1.4755 to 'data/predictions/almnps_8878/thesaurus_affinity/ood/science/best.pt'.
[Epoch 3/50] Train completed with Micro-f1: 0.8207, Macro-f1: 0.6976, Weighted-f1: 0.7991, Loss: 0.7349
[Epoch 3/50] Evaluation completed with Micro-f1: 0.5539, Macro-f1: 0.3850, Weighted-f1: 0.5209, Loss: 1.3758
Saved models from epoch 3 to 'data/predictions/almnps_8878/thesaurus_affinity/ood/science/newest.pt'.
Saved model with best loss 1.3758 to 'data/predictions/almnps_8878/thesaurus_affinity/ood/science/best.pt'.
[Epoch 4/50] Train completed with Micro-f1: 0.8874, Macro-f1: 0.7993, Weighted-f1: 0.8794, Loss: 0.4786
[Epoch 4/50] Evaluation completed with Micro-f1: 0.5640, Macro-f1: 0.3984, Weighted-f1: 0.5338, Loss: 1.4050
Saved models from epoch 4 to 'data/predictions/almnps_8878/thesaurus_affinity/ood/science/newest.pt'.
[Epoch 5/50] Train completed with Micro-f1: 0.9364, Macro-f1: 0.8854, Weighted-f1: 0.9286, Loss: 0.3142
[Epoch 5/50] Evaluation completed with Micro-f1: 0.5714, Macro-f1: 0.4077, Weighted-f1: 0.5421, Loss: 1.4064
Saved models from epoch 5 to 'data/predictions/almnps_8878/thesaurus_affinity/ood/science/newest.pt'.
[Epoch 6/50] Train completed with Micro-f1: 0.9682, Macro-f1: 0.9475, Weighted-f1: 0.9659, Loss: 0.2026
[Epoch 6/50] Evaluation completed with Micro-f1: 0.5778, Macro-f1: 0.4145, Weighted-f1: 0.5493, Loss: 1.4079
Saved models from epoch 6 to 'data/predictions/almnps_8878/thesaurus_affinity/ood/science/newest.pt'.
No improvement since 3 epochs (1.3758 loss). Early stop.
OOD training completed for test topic science after 6 epochs.
TRAINING COMPLETED with:
	Domains:		['ai', 'literature', 'music', 'news', 'politics', 'science']
	OOD validaation:	True
	Mapping type:		thesaurus_affinity
