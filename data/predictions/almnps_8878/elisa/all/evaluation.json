{"related-to": {"precision": 0.04822695035460993, "recall": 0.0821256038647343, "f1-score": 0.060768543342269894, "support": 414}, "artifact": {"precision": 0.07856191744340879, "recall": 0.09291338582677165, "f1-score": 0.08513708513708512, "support": 635}, "cause-effect": {"precision": 0.0, "recall": 0.0, "f1-score": 0.0, "support": 86}, "compare": {"precision": 0.0, "recall": 0.0, "f1-score": 0.0, "support": 82}, "general-affiliation": {"precision": 0.1487138263665595, "recall": 0.1493139628732849, "f1-score": 0.14901329037454694, "support": 1239}, "named": {"precision": 0.06622516556291391, "recall": 0.07168458781362007, "f1-score": 0.06884681583476764, "support": 558}, "opposite": {"precision": 0.0, "recall": 0.0, "f1-score": 0.0, "support": 128}, "origin": {"precision": 0.022388059701492536, "recall": 0.019736842105263157, "f1-score": 0.020979020979020976, "support": 304}, "part-of": {"precision": 0.0940959409594096, "recall": 0.07073509015256588, "f1-score": 0.08076009501187649, "support": 721}, "physical": {"precision": 0.16424840394660475, "recall": 0.19450171821305842, "f1-score": 0.17809943360604152, "support": 1455}, "role": {"precision": 0.23050259965337955, "recall": 0.19890329012961117, "f1-score": 0.21354027294621353, "support": 2006}, "social": {"precision": 0.0, "recall": 0.0, "f1-score": 0.0, "support": 105}, "temporal": {"precision": 0.04241071428571429, "recall": 0.04656862745098039, "f1-score": 0.04439252336448598, "support": 408}, "topic": {"precision": 0.034482758620689655, "recall": 0.0058823529411764705, "f1-score": 0.010050251256281407, "support": 170}, "type-of": {"precision": 0.05555555555555555, "recall": 0.026785714285714284, "f1-score": 0.03614457831325301, "support": 336}, "usage": {"precision": 0.0, "recall": 0.0, "f1-score": 0.0, "support": 200}, "win-defeat": {"precision": 0.028037383177570093, "recall": 0.028391167192429023, "f1-score": 0.02821316614420063, "support": 317}, "micro avg": {"precision": 0.12632671896631287, "recall": 0.11948930597992143, "f1-score": 0.12281292059219381, "support": 9164}, "macro avg": {"precision": 0.05961466327222989, "recall": 0.058090726049953516, "f1-score": 0.05740853390059077, "support": 9164}, "weighted avg": {"precision": 0.12197749803924801, "recall": 0.11948930597992143, "f1-score": 0.11951933375719508, "support": 9164}, "samples avg": {"precision": 0.12632671896631287, "recall": 0.11927011229041688, "f1-score": 0.12161590524534685, "support": 9164}}