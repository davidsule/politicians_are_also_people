{"related-to": {"precision": 0.056555269922879174, "recall": 0.05314009661835749, "f1-score": 0.05479452054794521, "support": 414}, "artifact": {"precision": 0.0625, "recall": 0.06456692913385827, "f1-score": 0.0635166537567777, "support": 635}, "cause-effect": {"precision": 0.0, "recall": 0.0, "f1-score": 0.0, "support": 86}, "compare": {"precision": 0.0, "recall": 0.0, "f1-score": 0.0, "support": 82}, "general-affiliation": {"precision": 0.14315209910529939, "recall": 0.1678773204196933, "f1-score": 0.15453194650817237, "support": 1239}, "named": {"precision": 0.06765067650676507, "recall": 0.0985663082437276, "f1-score": 0.08023340627279357, "support": 558}, "opposite": {"precision": 0.0, "recall": 0.0, "f1-score": 0.0, "support": 128}, "origin": {"precision": 0.022813688212927757, "recall": 0.019736842105263157, "f1-score": 0.021164021164021166, "support": 304}, "part-of": {"precision": 0.08928571428571429, "recall": 0.09015256588072122, "f1-score": 0.08971704623878538, "support": 721}, "physical": {"precision": 0.1589775561097257, "recall": 0.17525773195876287, "f1-score": 0.16672115070284407, "support": 1455}, "role": {"precision": 0.23475409836065575, "recall": 0.17846460618145563, "f1-score": 0.20277541772868876, "support": 2006}, "social": {"precision": 0.0, "recall": 0.0, "f1-score": 0.0, "support": 105}, "temporal": {"precision": 0.05084745762711865, "recall": 0.058823529411764705, "f1-score": 0.05454545454545455, "support": 408}, "topic": {"precision": 0.031746031746031744, "recall": 0.011764705882352941, "f1-score": 0.017167381974248927, "support": 170}, "type-of": {"precision": 0.041025641025641026, "recall": 0.023809523809523808, "f1-score": 0.030131826741996236, "support": 336}, "usage": {"precision": 0.0, "recall": 0.0, "f1-score": 0.0, "support": 200}, "win-defeat": {"precision": 0.04778156996587031, "recall": 0.04416403785488959, "f1-score": 0.0459016393442623, "support": 317}, "micro avg": {"precision": 0.1220581449007845, "recall": 0.11545176778699258, "f1-score": 0.1186630776132795, "support": 9164}, "macro avg": {"precision": 0.059240576639331105, "recall": 0.058019070441198264, "f1-score": 0.05771767444270531, "support": 9164}, "weighted avg": {"precision": 0.12078015035690258, "recall": 0.11545176778699258, "f1-score": 0.1167141049675111, "support": 9164}, "samples avg": {"precision": 0.1220581449007845, "recall": 0.11488617135825256, "f1-score": 0.11727041993539455, "support": 9164}}