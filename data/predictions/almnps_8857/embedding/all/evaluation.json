{"related-to": {"precision": 0.04643449419568822, "recall": 0.06763285024154589, "f1-score": 0.05506391347099311, "support": 414}, "artifact": {"precision": 0.06887052341597796, "recall": 0.07874015748031496, "f1-score": 0.07347538574577517, "support": 635}, "cause-effect": {"precision": 0.0, "recall": 0.0, "f1-score": 0.0, "support": 86}, "compare": {"precision": 0.0, "recall": 0.0, "f1-score": 0.0, "support": 82}, "general-affiliation": {"precision": 0.16148148148148148, "recall": 0.17594834543987087, "f1-score": 0.1684047894940131, "support": 1239}, "named": {"precision": 0.06235565819861432, "recall": 0.04838709677419355, "f1-score": 0.054490413723511606, "support": 558}, "opposite": {"precision": 0.0, "recall": 0.0, "f1-score": 0.0, "support": 128}, "origin": {"precision": 0.046153846153846156, "recall": 0.019736842105263157, "f1-score": 0.02764976958525345, "support": 304}, "part-of": {"precision": 0.08009153318077804, "recall": 0.0970873786407767, "f1-score": 0.0877742946708464, "support": 721}, "physical": {"precision": 0.15928210880538418, "recall": 0.19518900343642612, "f1-score": 0.1754169240271773, "support": 1455}, "role": {"precision": 0.25412735849056606, "recall": 0.21485543369890328, "f1-score": 0.2328471096704484, "support": 2006}, "social": {"precision": 0.0, "recall": 0.0, "f1-score": 0.0, "support": 105}, "temporal": {"precision": 0.06378986866791744, "recall": 0.08333333333333333, "f1-score": 0.0722635494155154, "support": 408}, "topic": {"precision": 0.10526315789473684, "recall": 0.011764705882352941, "f1-score": 0.021164021164021163, "support": 170}, "type-of": {"precision": 0.007407407407407408, "recall": 0.002976190476190476, "f1-score": 0.004246284501061571, "support": 336}, "usage": {"precision": 0.0, "recall": 0.0, "f1-score": 0.0, "support": 200}, "win-defeat": {"precision": 0.021739130434782608, "recall": 0.01892744479495268, "f1-score": 0.02023608768971332, "support": 317}, "micro avg": {"precision": 0.1334794646977388, "recall": 0.12625491051942384, "f1-score": 0.129766711529834, "support": 9164}, "macro avg": {"precision": 0.06335273931336359, "recall": 0.05968110484141905, "f1-score": 0.058413679009313525, "support": 9164}, "weighted avg": {"precision": 0.12706673792993164, "recall": 0.12625491051942384, "f1-score": 0.1247762157557649, "support": 9164}, "samples avg": {"precision": 0.1334794646977388, "recall": 0.12442316566682049, "f1-score": 0.12742270419935392, "support": 9164}}