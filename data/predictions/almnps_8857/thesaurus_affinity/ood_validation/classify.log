Saved arguments to data/predictions\almnps_8857\thesaurus_affinity\ood_validation\args.json.
loading projection weights from C:\Users\petko/gensim-data\word2vec-google-news-300\word2vec-google-news-300.gz
KeyedVectors lifecycle event {'msg': 'loaded (3000000, 300) matrix of type float32 from C:\\Users\\petko/gensim-data\\word2vec-google-news-300\\word2vec-google-news-300.gz', 'binary': True, 'encoding': 'utf8', 'datetime': '2023-05-18T02:37:47.170726', 'gensim': '4.3.0', 'python': '3.9.16 (main, Mar  8 2023, 10:39:24) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22000-SP0', 'event': 'load_word2vec_format'}
Saved category mapping to data/predictions\almnps_8857\thesaurus_affinity\ood_validation\entity2category_mapping.json.
Loaded category mapping: thesaurus_affinity.
Loaded <torch.utils.data.dataloader.DataLoader object at 0x0000022E114A7BE0> (train).
Loaded <torch.utils.data.dataloader.DataLoader object at 0x0000022E114A7EE0> (dev).
Starting training on ['literature', 'music', 'news', 'politics', 'science'] data with validation domain ai.
Read data from folder data/crossre_data/
Loaded <TransformerEmbeddings: dim=768>.
Using classifier: <LinearClassifier: emb_model = <TransformerEmbeddings: dim=768>>
Using criterion: <LabelLoss: loss=XEnt>.
Optimizing using: AdamW with learning rate 2e-05.
[Epoch 1/50] Train completed with Micro-f1: 0.4155, Macro-f1: 0.2316, Weighted-f1: 0.3432, Loss: 2.0273
[Epoch 1/50] Evaluation completed with Micro-f1: 0.4989, Macro-f1: 0.3126, Weighted-f1: 0.4358, Loss: 1.7412
Saved models from epoch 1 to 'data/predictions\almnps_8857\thesaurus_affinity\ood_validation\ai\newest.pt'.
Saved model with best loss 1.7412 to 'data/predictions\almnps_8857\thesaurus_affinity\ood_validation\ai\best.pt'.
[Epoch 2/50] Train completed with Micro-f1: 0.7280, Macro-f1: 0.5616, Weighted-f1: 0.6884, Loss: 1.0493
[Epoch 2/50] Evaluation completed with Micro-f1: 0.5184, Macro-f1: 0.3416, Weighted-f1: 0.4599, Loss: 1.5451
Saved models from epoch 2 to 'data/predictions\almnps_8857\thesaurus_affinity\ood_validation\ai\newest.pt'.
Saved model with best loss 1.5451 to 'data/predictions\almnps_8857\thesaurus_affinity\ood_validation\ai\best.pt'.
[Epoch 3/50] Train completed with Micro-f1: 0.8197, Macro-f1: 0.7066, Weighted-f1: 0.8025, Loss: 0.6830
[Epoch 3/50] Evaluation completed with Micro-f1: 0.5333, Macro-f1: 0.3637, Weighted-f1: 0.4821, Loss: 1.5196
Saved models from epoch 3 to 'data/predictions\almnps_8857\thesaurus_affinity\ood_validation\ai\newest.pt'.
Saved model with best loss 1.5196 to 'data/predictions\almnps_8857\thesaurus_affinity\ood_validation\ai\best.pt'.
[Epoch 4/50] Train completed with Micro-f1: 0.8945, Macro-f1: 0.8104, Weighted-f1: 0.8822, Loss: 0.4381
[Epoch 4/50] Evaluation completed with Micro-f1: 0.5457, Macro-f1: 0.3779, Weighted-f1: 0.4995, Loss: 1.4618
Saved models from epoch 4 to 'data/predictions\almnps_8857\thesaurus_affinity\ood_validation\ai\newest.pt'.
Saved model with best loss 1.4618 to 'data/predictions\almnps_8857\thesaurus_affinity\ood_validation\ai\best.pt'.
[Epoch 5/50] Train completed with Micro-f1: 0.9467, Macro-f1: 0.9008, Weighted-f1: 0.9436, Loss: 0.2799
[Epoch 5/50] Evaluation completed with Micro-f1: 0.5547, Macro-f1: 0.3910, Weighted-f1: 0.5122, Loss: 1.4787
Saved models from epoch 5 to 'data/predictions\almnps_8857\thesaurus_affinity\ood_validation\ai\newest.pt'.
[Epoch 6/50] Train completed with Micro-f1: 0.9667, Macro-f1: 0.9416, Weighted-f1: 0.9623, Loss: 0.1793
[Epoch 6/50] Evaluation completed with Micro-f1: 0.5623, Macro-f1: 0.4008, Weighted-f1: 0.5232, Loss: 1.5301
Saved models from epoch 6 to 'data/predictions\almnps_8857\thesaurus_affinity\ood_validation\ai\newest.pt'.
[Epoch 7/50] Train completed with Micro-f1: 0.9831, Macro-f1: 0.9714, Weighted-f1: 0.9825, Loss: 0.1209
[Epoch 7/50] Evaluation completed with Micro-f1: 0.5684, Macro-f1: 0.4088, Weighted-f1: 0.5312, Loss: 1.5698
Saved models from epoch 7 to 'data/predictions\almnps_8857\thesaurus_affinity\ood_validation\ai\newest.pt'.
No improvement since 3 epochs (1.4618 loss). Early stop.
OOD training completed for test topic ai after 7 epochs.
Loaded <torch.utils.data.dataloader.DataLoader object at 0x0000022E5427E5B0> (train).
Loaded <torch.utils.data.dataloader.DataLoader object at 0x0000022E114A7550> (dev).
Starting training on ['ai', 'music', 'news', 'politics', 'science'] data with validation domain literature.
Read data from folder data/crossre_data/
Loaded <TransformerEmbeddings: dim=768>.
Using classifier: <LinearClassifier: emb_model = <TransformerEmbeddings: dim=768>>
Using criterion: <LabelLoss: loss=XEnt>.
Optimizing using: AdamW with learning rate 2e-05.
[Epoch 1/50] Train completed with Micro-f1: 0.3733, Macro-f1: 0.2160, Weighted-f1: 0.3063, Loss: 2.0840
[Epoch 1/50] Evaluation completed with Micro-f1: 0.4983, Macro-f1: 0.3437, Weighted-f1: 0.4643, Loss: 1.7374
Saved models from epoch 1 to 'data/predictions\almnps_8857\thesaurus_affinity\ood_validation\literature\newest.pt'.
Saved model with best loss 1.7374 to 'data/predictions\almnps_8857\thesaurus_affinity\ood_validation\literature\best.pt'.
[Epoch 2/50] Train completed with Micro-f1: 0.6871, Macro-f1: 0.5683, Weighted-f1: 0.6617, Loss: 1.1323
[Epoch 2/50] Evaluation completed with Micro-f1: 0.5166, Macro-f1: 0.3630, Weighted-f1: 0.4848, Loss: 1.4903
Saved models from epoch 2 to 'data/predictions\almnps_8857\thesaurus_affinity\ood_validation\literature\newest.pt'.
Saved model with best loss 1.4903 to 'data/predictions\almnps_8857\thesaurus_affinity\ood_validation\literature\best.pt'.
[Epoch 3/50] Train completed with Micro-f1: 0.8337, Macro-f1: 0.7256, Weighted-f1: 0.8205, Loss: 0.6911
[Epoch 3/50] Evaluation completed with Micro-f1: 0.5282, Macro-f1: 0.3779, Weighted-f1: 0.5005, Loss: 1.4547
Saved models from epoch 3 to 'data/predictions\almnps_8857\thesaurus_affinity\ood_validation\literature\newest.pt'.
Saved model with best loss 1.4547 to 'data/predictions\almnps_8857\thesaurus_affinity\ood_validation\literature\best.pt'.
[Epoch 4/50] Train completed with Micro-f1: 0.8993, Macro-f1: 0.8294, Weighted-f1: 0.8924, Loss: 0.4327
[Epoch 4/50] Evaluation completed with Micro-f1: 0.5395, Macro-f1: 0.3904, Weighted-f1: 0.5127, Loss: 1.4393
Saved models from epoch 4 to 'data/predictions\almnps_8857\thesaurus_affinity\ood_validation\literature\newest.pt'.
Saved model with best loss 1.4393 to 'data/predictions\almnps_8857\thesaurus_affinity\ood_validation\literature\best.pt'.
[Epoch 5/50] Train completed with Micro-f1: 0.9420, Macro-f1: 0.9082, Weighted-f1: 0.9377, Loss: 0.2839
[Epoch 5/50] Evaluation completed with Micro-f1: 0.5478, Macro-f1: 0.4004, Weighted-f1: 0.5233, Loss: 1.4207
Saved models from epoch 5 to 'data/predictions\almnps_8857\thesaurus_affinity\ood_validation\literature\newest.pt'.
Saved model with best loss 1.4207 to 'data/predictions\almnps_8857\thesaurus_affinity\ood_validation\literature\best.pt'.
[Epoch 6/50] Train completed with Micro-f1: 0.9695, Macro-f1: 0.9461, Weighted-f1: 0.9675, Loss: 0.1873
[Epoch 6/50] Evaluation completed with Micro-f1: 0.5538, Macro-f1: 0.4069, Weighted-f1: 0.5306, Loss: 1.4367
Saved models from epoch 6 to 'data/predictions\almnps_8857\thesaurus_affinity\ood_validation\literature\newest.pt'.
[Epoch 7/50] Train completed with Micro-f1: 0.9820, Macro-f1: 0.9672, Weighted-f1: 0.9807, Loss: 0.1245
[Epoch 7/50] Evaluation completed with Micro-f1: 0.5562, Macro-f1: 0.4108, Weighted-f1: 0.5338, Loss: 1.5741
Saved models from epoch 7 to 'data/predictions\almnps_8857\thesaurus_affinity\ood_validation\literature\newest.pt'.
[Epoch 8/50] Train completed with Micro-f1: 0.9905, Macro-f1: 0.9849, Weighted-f1: 0.9899, Loss: 0.0903
[Epoch 8/50] Evaluation completed with Micro-f1: 0.5600, Macro-f1: 0.4154, Weighted-f1: 0.5385, Loss: 1.5989
Saved models from epoch 8 to 'data/predictions\almnps_8857\thesaurus_affinity\ood_validation\literature\newest.pt'.
No improvement since 3 epochs (1.4207 loss). Early stop.
OOD training completed for test topic literature after 8 epochs.
Loaded <torch.utils.data.dataloader.DataLoader object at 0x0000022E1A49B610> (train).
Loaded <torch.utils.data.dataloader.DataLoader object at 0x0000022E5428C9D0> (dev).
Starting training on ['ai', 'literature', 'news', 'politics', 'science'] data with validation domain music.
Read data from folder data/crossre_data/
Loaded <TransformerEmbeddings: dim=768>.
Using classifier: <LinearClassifier: emb_model = <TransformerEmbeddings: dim=768>>
Using criterion: <LabelLoss: loss=XEnt>.
Optimizing using: AdamW with learning rate 2e-05.
[Epoch 1/50] Train completed with Micro-f1: 0.3401, Macro-f1: 0.2002, Weighted-f1: 0.2706, Loss: 2.1569
[Epoch 1/50] Evaluation completed with Micro-f1: 0.4447, Macro-f1: 0.3039, Weighted-f1: 0.4072, Loss: 1.8873
Saved models from epoch 1 to 'data/predictions\almnps_8857\thesaurus_affinity\ood_validation\music\newest.pt'.
Saved model with best loss 1.8873 to 'data/predictions\almnps_8857\thesaurus_affinity\ood_validation\music\best.pt'.
[Epoch 2/50] Train completed with Micro-f1: 0.6631, Macro-f1: 0.5225, Weighted-f1: 0.6345, Loss: 1.2460
[Epoch 2/50] Evaluation completed with Micro-f1: 0.4753, Macro-f1: 0.3315, Weighted-f1: 0.4392, Loss: 1.6707
Saved models from epoch 2 to 'data/predictions\almnps_8857\thesaurus_affinity\ood_validation\music\newest.pt'.
Saved model with best loss 1.6707 to 'data/predictions\almnps_8857\thesaurus_affinity\ood_validation\music\best.pt'.
[Epoch 3/50] Train completed with Micro-f1: 0.8039, Macro-f1: 0.7034, Weighted-f1: 0.7890, Loss: 0.8022
[Epoch 3/50] Evaluation completed with Micro-f1: 0.4928, Macro-f1: 0.3467, Weighted-f1: 0.4593, Loss: 1.5698
Saved models from epoch 3 to 'data/predictions\almnps_8857\thesaurus_affinity\ood_validation\music\newest.pt'.
Saved model with best loss 1.5698 to 'data/predictions\almnps_8857\thesaurus_affinity\ood_validation\music\best.pt'.
[Epoch 4/50] Train completed with Micro-f1: 0.8753, Macro-f1: 0.7928, Weighted-f1: 0.8671, Loss: 0.5299
[Epoch 4/50] Evaluation completed with Micro-f1: 0.5042, Macro-f1: 0.3579, Weighted-f1: 0.4733, Loss: 1.5269
Saved models from epoch 4 to 'data/predictions\almnps_8857\thesaurus_affinity\ood_validation\music\newest.pt'.
Saved model with best loss 1.5269 to 'data/predictions\almnps_8857\thesaurus_affinity\ood_validation\music\best.pt'.
[Epoch 5/50] Train completed with Micro-f1: 0.9371, Macro-f1: 0.8940, Weighted-f1: 0.9298, Loss: 0.3400
[Epoch 5/50] Evaluation completed with Micro-f1: 0.5121, Macro-f1: 0.3663, Weighted-f1: 0.4834, Loss: 1.5418
Saved models from epoch 5 to 'data/predictions\almnps_8857\thesaurus_affinity\ood_validation\music\newest.pt'.
[Epoch 6/50] Train completed with Micro-f1: 0.9676, Macro-f1: 0.9391, Weighted-f1: 0.9666, Loss: 0.2261
[Epoch 6/50] Evaluation completed with Micro-f1: 0.5191, Macro-f1: 0.3720, Weighted-f1: 0.4912, Loss: 1.5480
Saved models from epoch 6 to 'data/predictions\almnps_8857\thesaurus_affinity\ood_validation\music\newest.pt'.
[Epoch 7/50] Train completed with Micro-f1: 0.9855, Macro-f1: 0.9711, Weighted-f1: 0.9844, Loss: 0.1460
[Epoch 7/50] Evaluation completed with Micro-f1: 0.5249, Macro-f1: 0.3796, Weighted-f1: 0.4985, Loss: 1.5710
Saved models from epoch 7 to 'data/predictions\almnps_8857\thesaurus_affinity\ood_validation\music\newest.pt'.
No improvement since 3 epochs (1.5269 loss). Early stop.
OOD training completed for test topic music after 7 epochs.
Loaded <torch.utils.data.dataloader.DataLoader object at 0x0000022E71D23790> (train).
Loaded <torch.utils.data.dataloader.DataLoader object at 0x0000022E1A4A2070> (dev).
Starting training on ['ai', 'literature', 'music', 'politics', 'science'] data with validation domain news.
Read data from folder data/crossre_data/
Loaded <TransformerEmbeddings: dim=768>.
Using classifier: <LinearClassifier: emb_model = <TransformerEmbeddings: dim=768>>
Using criterion: <LabelLoss: loss=XEnt>.
Optimizing using: AdamW with learning rate 2e-05.
[Epoch 1/50] Train completed with Micro-f1: 0.3992, Macro-f1: 0.2367, Weighted-f1: 0.3320, Loss: 2.0373
[Epoch 1/50] Evaluation completed with Micro-f1: 0.5252, Macro-f1: 0.3567, Weighted-f1: 0.4825, Loss: 1.6361
Saved models from epoch 1 to 'data/predictions\almnps_8857\thesaurus_affinity\ood_validation\news\newest.pt'.
Saved model with best loss 1.6361 to 'data/predictions\almnps_8857\thesaurus_affinity\ood_validation\news\best.pt'.
[Epoch 2/50] Train completed with Micro-f1: 0.7244, Macro-f1: 0.5885, Weighted-f1: 0.6954, Loss: 1.0509
[Epoch 2/50] Evaluation completed with Micro-f1: 0.5485, Macro-f1: 0.3827, Weighted-f1: 0.5091, Loss: 1.4395
Saved models from epoch 2 to 'data/predictions\almnps_8857\thesaurus_affinity\ood_validation\news\newest.pt'.
Saved model with best loss 1.4395 to 'data/predictions\almnps_8857\thesaurus_affinity\ood_validation\news\best.pt'.
[Epoch 3/50] Train completed with Micro-f1: 0.8382, Macro-f1: 0.7453, Weighted-f1: 0.8244, Loss: 0.6219
[Epoch 3/50] Evaluation completed with Micro-f1: 0.5657, Macro-f1: 0.4055, Weighted-f1: 0.5313, Loss: 1.3776
Saved models from epoch 3 to 'data/predictions\almnps_8857\thesaurus_affinity\ood_validation\news\newest.pt'.
Saved model with best loss 1.3776 to 'data/predictions\almnps_8857\thesaurus_affinity\ood_validation\news\best.pt'.
[Epoch 4/50] Train completed with Micro-f1: 0.9092, Macro-f1: 0.8435, Weighted-f1: 0.8995, Loss: 0.3847
[Epoch 4/50] Evaluation completed with Micro-f1: 0.5777, Macro-f1: 0.4193, Weighted-f1: 0.5456, Loss: 1.3203
Saved models from epoch 4 to 'data/predictions\almnps_8857\thesaurus_affinity\ood_validation\news\newest.pt'.
Saved model with best loss 1.3203 to 'data/predictions\almnps_8857\thesaurus_affinity\ood_validation\news\best.pt'.
[Epoch 5/50] Train completed with Micro-f1: 0.9485, Macro-f1: 0.9078, Weighted-f1: 0.9438, Loss: 0.2457
[Epoch 5/50] Evaluation completed with Micro-f1: 0.5847, Macro-f1: 0.4283, Weighted-f1: 0.5548, Loss: 1.4225
Saved models from epoch 5 to 'data/predictions\almnps_8857\thesaurus_affinity\ood_validation\news\newest.pt'.
[Epoch 6/50] Train completed with Micro-f1: 0.9735, Macro-f1: 0.9478, Weighted-f1: 0.9706, Loss: 0.1508
[Epoch 6/50] Evaluation completed with Micro-f1: 0.5906, Macro-f1: 0.4351, Weighted-f1: 0.5622, Loss: 1.4936
Saved models from epoch 6 to 'data/predictions\almnps_8857\thesaurus_affinity\ood_validation\news\newest.pt'.
[Epoch 7/50] Train completed with Micro-f1: 0.9863, Macro-f1: 0.9735, Weighted-f1: 0.9856, Loss: 0.1015
[Epoch 7/50] Evaluation completed with Micro-f1: 0.5955, Macro-f1: 0.4419, Weighted-f1: 0.5683, Loss: 1.5587
Saved models from epoch 7 to 'data/predictions\almnps_8857\thesaurus_affinity\ood_validation\news\newest.pt'.
No improvement since 3 epochs (1.3203 loss). Early stop.
OOD training completed for test topic news after 7 epochs.
Loaded <torch.utils.data.dataloader.DataLoader object at 0x0000022E1A583FA0> (train).
Loaded <torch.utils.data.dataloader.DataLoader object at 0x0000022E71D26D60> (dev).
Starting training on ['ai', 'literature', 'music', 'news', 'science'] data with validation domain politics.
Read data from folder data/crossre_data/
Loaded <TransformerEmbeddings: dim=768>.
Using classifier: <LinearClassifier: emb_model = <TransformerEmbeddings: dim=768>>
Using criterion: <LabelLoss: loss=XEnt>.
Optimizing using: AdamW with learning rate 2e-05.
[Epoch 1/50] Train completed with Micro-f1: 0.3724, Macro-f1: 0.2130, Weighted-f1: 0.3105, Loss: 2.1494
[Epoch 1/50] Evaluation completed with Micro-f1: 0.5073, Macro-f1: 0.3255, Weighted-f1: 0.4541, Loss: 1.7539
Saved models from epoch 1 to 'data/predictions\almnps_8857\thesaurus_affinity\ood_validation\politics\newest.pt'.
Saved model with best loss 1.7539 to 'data/predictions\almnps_8857\thesaurus_affinity\ood_validation\politics\best.pt'.
[Epoch 2/50] Train completed with Micro-f1: 0.6925, Macro-f1: 0.5285, Weighted-f1: 0.6541, Loss: 1.1867
[Epoch 2/50] Evaluation completed with Micro-f1: 0.5349, Macro-f1: 0.3543, Weighted-f1: 0.4871, Loss: 1.4634
Saved models from epoch 2 to 'data/predictions\almnps_8857\thesaurus_affinity\ood_validation\politics\newest.pt'.
Saved model with best loss 1.4634 to 'data/predictions\almnps_8857\thesaurus_affinity\ood_validation\politics\best.pt'.
[Epoch 3/50] Train completed with Micro-f1: 0.8099, Macro-f1: 0.7047, Weighted-f1: 0.7917, Loss: 0.7521
[Epoch 3/50] Evaluation completed with Micro-f1: 0.5550, Macro-f1: 0.3777, Weighted-f1: 0.5118, Loss: 1.3657
Saved models from epoch 3 to 'data/predictions\almnps_8857\thesaurus_affinity\ood_validation\politics\newest.pt'.
Saved model with best loss 1.3657 to 'data/predictions\almnps_8857\thesaurus_affinity\ood_validation\politics\best.pt'.
[Epoch 4/50] Train completed with Micro-f1: 0.8813, Macro-f1: 0.8018, Weighted-f1: 0.8692, Loss: 0.4917
[Epoch 4/50] Evaluation completed with Micro-f1: 0.5638, Macro-f1: 0.3885, Weighted-f1: 0.5230, Loss: 1.3811
Saved models from epoch 4 to 'data/predictions\almnps_8857\thesaurus_affinity\ood_validation\politics\newest.pt'.
[Epoch 5/50] Train completed with Micro-f1: 0.9291, Macro-f1: 0.8804, Weighted-f1: 0.9230, Loss: 0.3220
[Epoch 5/50] Evaluation completed with Micro-f1: 0.5718, Macro-f1: 0.3976, Weighted-f1: 0.5338, Loss: 1.3489
Saved models from epoch 5 to 'data/predictions\almnps_8857\thesaurus_affinity\ood_validation\politics\newest.pt'.
Saved model with best loss 1.3489 to 'data/predictions\almnps_8857\thesaurus_affinity\ood_validation\politics\best.pt'.
[Epoch 6/50] Train completed with Micro-f1: 0.9555, Macro-f1: 0.9259, Weighted-f1: 0.9519, Loss: 0.2207
[Epoch 6/50] Evaluation completed with Micro-f1: 0.5785, Macro-f1: 0.4048, Weighted-f1: 0.5414, Loss: 1.4153
Saved models from epoch 6 to 'data/predictions\almnps_8857\thesaurus_affinity\ood_validation\politics\newest.pt'.
[Epoch 7/50] Train completed with Micro-f1: 0.9827, Macro-f1: 0.9632, Weighted-f1: 0.9817, Loss: 0.1374
[Epoch 7/50] Evaluation completed with Micro-f1: 0.5844, Macro-f1: 0.4116, Weighted-f1: 0.5494, Loss: 1.4050
Saved models from epoch 7 to 'data/predictions\almnps_8857\thesaurus_affinity\ood_validation\politics\newest.pt'.
[Epoch 8/50] Train completed with Micro-f1: 0.9888, Macro-f1: 0.9769, Weighted-f1: 0.9878, Loss: 0.0919
[Epoch 8/50] Evaluation completed with Micro-f1: 0.5890, Macro-f1: 0.4165, Weighted-f1: 0.5547, Loss: 1.4777
Saved models from epoch 8 to 'data/predictions\almnps_8857\thesaurus_affinity\ood_validation\politics\newest.pt'.
No improvement since 3 epochs (1.3489 loss). Early stop.
OOD training completed for test topic politics after 8 epochs.
Loaded <torch.utils.data.dataloader.DataLoader object at 0x0000022E5D76CE80> (train).
Loaded <torch.utils.data.dataloader.DataLoader object at 0x0000022E71D8DE80> (dev).
Starting training on ['ai', 'literature', 'music', 'news', 'politics'] data with validation domain science.
Read data from folder data/crossre_data/
Loaded <TransformerEmbeddings: dim=768>.
Using classifier: <LinearClassifier: emb_model = <TransformerEmbeddings: dim=768>>
Using criterion: <LabelLoss: loss=XEnt>.
Optimizing using: AdamW with learning rate 2e-05.
[Epoch 1/50] Train completed with Micro-f1: 0.3852, Macro-f1: 0.2152, Weighted-f1: 0.3096, Loss: 2.1126
[Epoch 1/50] Evaluation completed with Micro-f1: 0.5254, Macro-f1: 0.3450, Weighted-f1: 0.4758, Loss: 1.6481
Saved models from epoch 1 to 'data/predictions\almnps_8857\thesaurus_affinity\ood_validation\science\newest.pt'.
Saved model with best loss 1.6481 to 'data/predictions\almnps_8857\thesaurus_affinity\ood_validation\science\best.pt'.
[Epoch 2/50] Train completed with Micro-f1: 0.6848, Macro-f1: 0.5208, Weighted-f1: 0.6474, Loss: 1.1430
[Epoch 2/50] Evaluation completed with Micro-f1: 0.5480, Macro-f1: 0.3725, Weighted-f1: 0.5053, Loss: 1.4201
Saved models from epoch 2 to 'data/predictions\almnps_8857\thesaurus_affinity\ood_validation\science\newest.pt'.
Saved model with best loss 1.4201 to 'data/predictions\almnps_8857\thesaurus_affinity\ood_validation\science\best.pt'.
[Epoch 3/50] Train completed with Micro-f1: 0.8186, Macro-f1: 0.7007, Weighted-f1: 0.7987, Loss: 0.7127
[Epoch 3/50] Evaluation completed with Micro-f1: 0.5628, Macro-f1: 0.3912, Weighted-f1: 0.5242, Loss: 1.3227
Saved models from epoch 3 to 'data/predictions\almnps_8857\thesaurus_affinity\ood_validation\science\newest.pt'.
Saved model with best loss 1.3227 to 'data/predictions\almnps_8857\thesaurus_affinity\ood_validation\science\best.pt'.
[Epoch 4/50] Train completed with Micro-f1: 0.8900, Macro-f1: 0.7975, Weighted-f1: 0.8806, Loss: 0.4613
[Epoch 4/50] Evaluation completed with Micro-f1: 0.5712, Macro-f1: 0.4025, Weighted-f1: 0.5367, Loss: 1.3582
Saved models from epoch 4 to 'data/predictions\almnps_8857\thesaurus_affinity\ood_validation\science\newest.pt'.
[Epoch 5/50] Train completed with Micro-f1: 0.9401, Macro-f1: 0.8917, Weighted-f1: 0.9347, Loss: 0.2950
[Epoch 5/50] Evaluation completed with Micro-f1: 0.5786, Macro-f1: 0.4127, Weighted-f1: 0.5459, Loss: 1.3765
Saved models from epoch 5 to 'data/predictions\almnps_8857\thesaurus_affinity\ood_validation\science\newest.pt'.
[Epoch 6/50] Train completed with Micro-f1: 0.9640, Macro-f1: 0.9443, Weighted-f1: 0.9615, Loss: 0.2041
[Epoch 6/50] Evaluation completed with Micro-f1: 0.5837, Macro-f1: 0.4194, Weighted-f1: 0.5533, Loss: 1.4028
Saved models from epoch 6 to 'data/predictions\almnps_8857\thesaurus_affinity\ood_validation\science\newest.pt'.
No improvement since 3 epochs (1.3227 loss). Early stop.
OOD training completed for test topic science after 6 epochs.
TRAINING COMPLETED with:
	Domains:		['ai', 'literature', 'music', 'news', 'politics', 'science']
	OOD validaation:	True
	Mapping type:		thesaurus_affinity
