Saved arguments to data/predictions/almnps_8857/thesaurus_affinity/ood/args.json.
loading projection weights from /home/davidsule/gensim-data/word2vec-google-news-300/word2vec-google-news-300.gz
KeyedVectors lifecycle event {'msg': 'loaded (3000000, 300) matrix of type float32 from /home/davidsule/gensim-data/word2vec-google-news-300/word2vec-google-news-300.gz', 'binary': True, 'encoding': 'utf8', 'datetime': '2023-05-08T14:01:17.920911', 'gensim': '4.3.0', 'python': '3.9.16 | packaged by conda-forge | (main, Feb  1 2023, 21:39:03) \n[GCC 11.3.0]', 'platform': 'Linux-5.15.90.1-microsoft-standard-WSL2-x86_64-with-glibc2.31', 'event': 'load_word2vec_format'}
Saved category mapping to data/predictions/almnps_8857/thesaurus_affinity/ood/mapping.json.
Loaded category mapping: thesaurus_affinity.
Loaded <torch.utils.data.dataloader.DataLoader object at 0x7f99df2ba070> (train).
Loaded <torch.utils.data.dataloader.DataLoader object at 0x7f99e1005610> (dev).
Starting training on ['literature', 'music', 'news', 'politics', 'science'] data.
Loaded <TransformerEmbeddings: dim=768>.
Using classifier: <LinearClassifier: emb_model = <TransformerEmbeddings: dim=768>>
Using criterion: <LabelLoss: loss=XEnt>.
Optimizing using: AdamW with learning rate 2e-05.
[Epoch 1/50] Train completed with Micro-f1: 0.3791, Macro-f1: 0.2035, Weighted-f1: 0.3122, Loss: 2.0861
[Epoch 1/50] Evaluation completed with Micro-f1: 0.4916, Macro-f1: 0.3066, Weighted-f1: 0.4296, Loss: 1.7628
Saved models from epoch 1 to 'data/predictions/almnps_8857/thesaurus_affinity/ood/ai/newest.pt'.
Saved model with best loss 1.7628 to 'data/predictions/almnps_8857/thesaurus_affinity/ood/ai/best.pt'.
[Epoch 2/50] Train completed with Micro-f1: 0.7044, Macro-f1: 0.5432, Weighted-f1: 0.6662, Loss: 1.0889
[Epoch 2/50] Evaluation completed with Micro-f1: 0.5179, Macro-f1: 0.3413, Weighted-f1: 0.4604, Loss: 1.5285
Saved models from epoch 2 to 'data/predictions/almnps_8857/thesaurus_affinity/ood/ai/newest.pt'.
Saved model with best loss 1.5285 to 'data/predictions/almnps_8857/thesaurus_affinity/ood/ai/best.pt'.
[Epoch 3/50] Train completed with Micro-f1: 0.8186, Macro-f1: 0.7061, Weighted-f1: 0.7996, Loss: 0.6850
[Epoch 3/50] Evaluation completed with Micro-f1: 0.5376, Macro-f1: 0.3640, Weighted-f1: 0.4844, Loss: 1.4476
Saved models from epoch 3 to 'data/predictions/almnps_8857/thesaurus_affinity/ood/ai/newest.pt'.
Saved model with best loss 1.4476 to 'data/predictions/almnps_8857/thesaurus_affinity/ood/ai/best.pt'.
[Epoch 4/50] Train completed with Micro-f1: 0.9073, Macro-f1: 0.8360, Weighted-f1: 0.8984, Loss: 0.4169
[Epoch 4/50] Evaluation completed with Micro-f1: 0.5485, Macro-f1: 0.3770, Weighted-f1: 0.5002, Loss: 1.3950
Saved models from epoch 4 to 'data/predictions/almnps_8857/thesaurus_affinity/ood/ai/newest.pt'.
Saved model with best loss 1.3950 to 'data/predictions/almnps_8857/thesaurus_affinity/ood/ai/best.pt'.
[Epoch 5/50] Train completed with Micro-f1: 0.9483, Macro-f1: 0.8988, Weighted-f1: 0.9425, Loss: 0.2738
[Epoch 5/50] Evaluation completed with Micro-f1: 0.5578, Macro-f1: 0.3891, Weighted-f1: 0.5133, Loss: 1.4374
Saved models from epoch 5 to 'data/predictions/almnps_8857/thesaurus_affinity/ood/ai/newest.pt'.
[Epoch 6/50] Train completed with Micro-f1: 0.9672, Macro-f1: 0.9415, Weighted-f1: 0.9635, Loss: 0.1824
[Epoch 6/50] Evaluation completed with Micro-f1: 0.5664, Macro-f1: 0.4004, Weighted-f1: 0.5254, Loss: 1.4405
Saved models from epoch 6 to 'data/predictions/almnps_8857/thesaurus_affinity/ood/ai/newest.pt'.
[Epoch 7/50] Train completed with Micro-f1: 0.9857, Macro-f1: 0.9743, Weighted-f1: 0.9842, Loss: 0.1209
[Epoch 7/50] Evaluation completed with Micro-f1: 0.5718, Macro-f1: 0.4083, Weighted-f1: 0.5323, Loss: 1.5284
Saved models from epoch 7 to 'data/predictions/almnps_8857/thesaurus_affinity/ood/ai/newest.pt'.
No improvement since 3 epochs (1.3950 loss). Early stop.
OOD training completed for test topic ai after 7 epochs.
Loaded <torch.utils.data.dataloader.DataLoader object at 0x7f99c836cf70> (train).
Loaded <torch.utils.data.dataloader.DataLoader object at 0x7f99e9a03f10> (dev).
Starting training on ['ai', 'music', 'news', 'politics', 'science'] data.
Loaded <TransformerEmbeddings: dim=768>.
Using classifier: <LinearClassifier: emb_model = <TransformerEmbeddings: dim=768>>
Using criterion: <LabelLoss: loss=XEnt>.
Optimizing using: AdamW with learning rate 2e-05.
[Epoch 1/50] Train completed with Micro-f1: 0.3730, Macro-f1: 0.2166, Weighted-f1: 0.3070, Loss: 2.0799
[Epoch 1/50] Evaluation completed with Micro-f1: 0.4923, Macro-f1: 0.3360, Weighted-f1: 0.4568, Loss: 1.7387
Saved models from epoch 1 to 'data/predictions/almnps_8857/thesaurus_affinity/ood/literature/newest.pt'.
Saved model with best loss 1.7387 to 'data/predictions/almnps_8857/thesaurus_affinity/ood/literature/best.pt'.
[Epoch 2/50] Train completed with Micro-f1: 0.6895, Macro-f1: 0.5636, Weighted-f1: 0.6586, Loss: 1.1454
[Epoch 2/50] Evaluation completed with Micro-f1: 0.5106, Macro-f1: 0.3570, Weighted-f1: 0.4765, Loss: 1.5238
Saved models from epoch 2 to 'data/predictions/almnps_8857/thesaurus_affinity/ood/literature/newest.pt'.
Saved model with best loss 1.5238 to 'data/predictions/almnps_8857/thesaurus_affinity/ood/literature/best.pt'.
[Epoch 3/50] Train completed with Micro-f1: 0.8276, Macro-f1: 0.7291, Weighted-f1: 0.8137, Loss: 0.7061
[Epoch 3/50] Evaluation completed with Micro-f1: 0.5238, Macro-f1: 0.3754, Weighted-f1: 0.4948, Loss: 1.5011
Saved models from epoch 3 to 'data/predictions/almnps_8857/thesaurus_affinity/ood/literature/newest.pt'.
Saved model with best loss 1.5011 to 'data/predictions/almnps_8857/thesaurus_affinity/ood/literature/best.pt'.
[Epoch 4/50] Train completed with Micro-f1: 0.8969, Macro-f1: 0.8261, Weighted-f1: 0.8889, Loss: 0.4514
[Epoch 4/50] Evaluation completed with Micro-f1: 0.5328, Macro-f1: 0.3870, Weighted-f1: 0.5054, Loss: 1.5291
Saved models from epoch 4 to 'data/predictions/almnps_8857/thesaurus_affinity/ood/literature/newest.pt'.
[Epoch 5/50] Train completed with Micro-f1: 0.9375, Macro-f1: 0.8855, Weighted-f1: 0.9318, Loss: 0.2960
[Epoch 5/50] Evaluation completed with Micro-f1: 0.5404, Macro-f1: 0.3963, Weighted-f1: 0.5151, Loss: 1.5303
Saved models from epoch 5 to 'data/predictions/almnps_8857/thesaurus_affinity/ood/literature/newest.pt'.
[Epoch 6/50] Train completed with Micro-f1: 0.9693, Macro-f1: 0.9416, Weighted-f1: 0.9667, Loss: 0.1999
[Epoch 6/50] Evaluation completed with Micro-f1: 0.5465, Macro-f1: 0.4034, Weighted-f1: 0.5232, Loss: 1.5919
Saved models from epoch 6 to 'data/predictions/almnps_8857/thesaurus_affinity/ood/literature/newest.pt'.
No improvement since 3 epochs (1.5011 loss). Early stop.
OOD training completed for test topic literature after 6 epochs.
Loaded <torch.utils.data.dataloader.DataLoader object at 0x7f9900f68bb0> (train).
Loaded <torch.utils.data.dataloader.DataLoader object at 0x7f99c8268b50> (dev).
Starting training on ['ai', 'literature', 'news', 'politics', 'science'] data.
Loaded <TransformerEmbeddings: dim=768>.
Using classifier: <LinearClassifier: emb_model = <TransformerEmbeddings: dim=768>>
Using criterion: <LabelLoss: loss=XEnt>.
Optimizing using: AdamW with learning rate 2e-05.
[Epoch 1/50] Train completed with Micro-f1: 0.3262, Macro-f1: 0.1878, Weighted-f1: 0.2562, Loss: 2.1794
[Epoch 1/50] Evaluation completed with Micro-f1: 0.4361, Macro-f1: 0.2881, Weighted-f1: 0.3982, Loss: 1.9100
Saved models from epoch 1 to 'data/predictions/almnps_8857/thesaurus_affinity/ood/music/newest.pt'.
Saved model with best loss 1.9100 to 'data/predictions/almnps_8857/thesaurus_affinity/ood/music/best.pt'.
[Epoch 2/50] Train completed with Micro-f1: 0.6621, Macro-f1: 0.5338, Weighted-f1: 0.6254, Loss: 1.2501
[Epoch 2/50] Evaluation completed with Micro-f1: 0.4615, Macro-f1: 0.3140, Weighted-f1: 0.4284, Loss: 1.6888
Saved models from epoch 2 to 'data/predictions/almnps_8857/thesaurus_affinity/ood/music/newest.pt'.
Saved model with best loss 1.6888 to 'data/predictions/almnps_8857/thesaurus_affinity/ood/music/best.pt'.
[Epoch 3/50] Train completed with Micro-f1: 0.8007, Macro-f1: 0.6915, Weighted-f1: 0.7845, Loss: 0.8100
[Epoch 3/50] Evaluation completed with Micro-f1: 0.4809, Macro-f1: 0.3350, Weighted-f1: 0.4527, Loss: 1.5810
Saved models from epoch 3 to 'data/predictions/almnps_8857/thesaurus_affinity/ood/music/newest.pt'.
Saved model with best loss 1.5810 to 'data/predictions/almnps_8857/thesaurus_affinity/ood/music/best.pt'.
[Epoch 4/50] Train completed with Micro-f1: 0.8908, Macro-f1: 0.8167, Weighted-f1: 0.8804, Loss: 0.5169
[Epoch 4/50] Evaluation completed with Micro-f1: 0.4936, Macro-f1: 0.3476, Weighted-f1: 0.4660, Loss: 1.5699
Saved models from epoch 4 to 'data/predictions/almnps_8857/thesaurus_affinity/ood/music/newest.pt'.
Saved model with best loss 1.5699 to 'data/predictions/almnps_8857/thesaurus_affinity/ood/music/best.pt'.
[Epoch 5/50] Train completed with Micro-f1: 0.9269, Macro-f1: 0.8801, Weighted-f1: 0.9225, Loss: 0.3493
[Epoch 5/50] Evaluation completed with Micro-f1: 0.5022, Macro-f1: 0.3574, Weighted-f1: 0.4770, Loss: 1.5975
Saved models from epoch 5 to 'data/predictions/almnps_8857/thesaurus_affinity/ood/music/newest.pt'.
[Epoch 6/50] Train completed with Micro-f1: 0.9650, Macro-f1: 0.9360, Weighted-f1: 0.9608, Loss: 0.2274
[Epoch 6/50] Evaluation completed with Micro-f1: 0.5115, Macro-f1: 0.3660, Weighted-f1: 0.4874, Loss: 1.5827
Saved models from epoch 6 to 'data/predictions/almnps_8857/thesaurus_affinity/ood/music/newest.pt'.
[Epoch 7/50] Train completed with Micro-f1: 0.9836, Macro-f1: 0.9728, Weighted-f1: 0.9820, Loss: 0.1498
[Epoch 7/50] Evaluation completed with Micro-f1: 0.5171, Macro-f1: 0.3734, Weighted-f1: 0.4948, Loss: 1.6413
Saved models from epoch 7 to 'data/predictions/almnps_8857/thesaurus_affinity/ood/music/newest.pt'.
No improvement since 3 epochs (1.5699 loss). Early stop.
OOD training completed for test topic music after 7 epochs.
Loaded <torch.utils.data.dataloader.DataLoader object at 0x7f99c815c070> (train).
Loaded <torch.utils.data.dataloader.DataLoader object at 0x7f9900f68d00> (dev).
Starting training on ['ai', 'literature', 'music', 'politics', 'science'] data.
Loaded <TransformerEmbeddings: dim=768>.
Using classifier: <LinearClassifier: emb_model = <TransformerEmbeddings: dim=768>>
Using criterion: <LabelLoss: loss=XEnt>.
Optimizing using: AdamW with learning rate 2e-05.
[Epoch 1/50] Train completed with Micro-f1: 0.3984, Macro-f1: 0.2476, Weighted-f1: 0.3406, Loss: 2.0474
[Epoch 1/50] Evaluation completed with Micro-f1: 0.5290, Macro-f1: 0.3556, Weighted-f1: 0.4789, Loss: 1.6820
Saved models from epoch 1 to 'data/predictions/almnps_8857/thesaurus_affinity/ood/news/newest.pt'.
Saved model with best loss 1.6820 to 'data/predictions/almnps_8857/thesaurus_affinity/ood/news/best.pt'.
[Epoch 2/50] Train completed with Micro-f1: 0.7193, Macro-f1: 0.5862, Weighted-f1: 0.6876, Loss: 1.0468
[Epoch 2/50] Evaluation completed with Micro-f1: 0.5527, Macro-f1: 0.3853, Weighted-f1: 0.5103, Loss: 1.4382
Saved models from epoch 2 to 'data/predictions/almnps_8857/thesaurus_affinity/ood/news/newest.pt'.
Saved model with best loss 1.4382 to 'data/predictions/almnps_8857/thesaurus_affinity/ood/news/best.pt'.
[Epoch 3/50] Train completed with Micro-f1: 0.8336, Macro-f1: 0.7334, Weighted-f1: 0.8175, Loss: 0.6486
[Epoch 3/50] Evaluation completed with Micro-f1: 0.5672, Macro-f1: 0.4033, Weighted-f1: 0.5298, Loss: 1.3658
Saved models from epoch 3 to 'data/predictions/almnps_8857/thesaurus_affinity/ood/news/newest.pt'.
Saved model with best loss 1.3658 to 'data/predictions/almnps_8857/thesaurus_affinity/ood/news/best.pt'.
[Epoch 4/50] Train completed with Micro-f1: 0.9039, Macro-f1: 0.8396, Weighted-f1: 0.8945, Loss: 0.4064
[Epoch 4/50] Evaluation completed with Micro-f1: 0.5761, Macro-f1: 0.4131, Weighted-f1: 0.5398, Loss: 1.3748
Saved models from epoch 4 to 'data/predictions/almnps_8857/thesaurus_affinity/ood/news/newest.pt'.
[Epoch 5/50] Train completed with Micro-f1: 0.9448, Macro-f1: 0.9068, Weighted-f1: 0.9408, Loss: 0.2589
[Epoch 5/50] Evaluation completed with Micro-f1: 0.5833, Macro-f1: 0.4225, Weighted-f1: 0.5501, Loss: 1.3910
Saved models from epoch 5 to 'data/predictions/almnps_8857/thesaurus_affinity/ood/news/newest.pt'.
[Epoch 6/50] Train completed with Micro-f1: 0.9711, Macro-f1: 0.9539, Weighted-f1: 0.9688, Loss: 0.1732
[Epoch 6/50] Evaluation completed with Micro-f1: 0.5880, Macro-f1: 0.4293, Weighted-f1: 0.5569, Loss: 1.3562
Saved models from epoch 6 to 'data/predictions/almnps_8857/thesaurus_affinity/ood/news/newest.pt'.
Saved model with best loss 1.3562 to 'data/predictions/almnps_8857/thesaurus_affinity/ood/news/best.pt'.
[Epoch 7/50] Train completed with Micro-f1: 0.9848, Macro-f1: 0.9727, Weighted-f1: 0.9844, Loss: 0.1150
[Epoch 7/50] Evaluation completed with Micro-f1: 0.5913, Macro-f1: 0.4339, Weighted-f1: 0.5617, Loss: 1.4910
Saved models from epoch 7 to 'data/predictions/almnps_8857/thesaurus_affinity/ood/news/newest.pt'.
[Epoch 8/50] Train completed with Micro-f1: 0.9910, Macro-f1: 0.9804, Weighted-f1: 0.9905, Loss: 0.0773
[Epoch 8/50] Evaluation completed with Micro-f1: 0.5941, Macro-f1: 0.4370, Weighted-f1: 0.5652, Loss: 1.5988
Saved models from epoch 8 to 'data/predictions/almnps_8857/thesaurus_affinity/ood/news/newest.pt'.
[Epoch 9/50] Train completed with Micro-f1: 0.9910, Macro-f1: 0.9843, Weighted-f1: 0.9906, Loss: 0.0600
[Epoch 9/50] Evaluation completed with Micro-f1: 0.5959, Macro-f1: 0.4398, Weighted-f1: 0.5681, Loss: 1.6744
Saved models from epoch 9 to 'data/predictions/almnps_8857/thesaurus_affinity/ood/news/newest.pt'.
No improvement since 3 epochs (1.3562 loss). Early stop.
OOD training completed for test topic news after 9 epochs.
Loaded <torch.utils.data.dataloader.DataLoader object at 0x7f99daa14550> (train).
Loaded <torch.utils.data.dataloader.DataLoader object at 0x7f99c805ee20> (dev).
Starting training on ['ai', 'literature', 'music', 'news', 'science'] data.
Loaded <TransformerEmbeddings: dim=768>.
Using classifier: <LinearClassifier: emb_model = <TransformerEmbeddings: dim=768>>
Using criterion: <LabelLoss: loss=XEnt>.
Optimizing using: AdamW with learning rate 2e-05.
[Epoch 1/50] Train completed with Micro-f1: 0.3456, Macro-f1: 0.1927, Weighted-f1: 0.2881, Loss: 2.1948
[Epoch 1/50] Evaluation completed with Micro-f1: 0.4967, Macro-f1: 0.3166, Weighted-f1: 0.4472, Loss: 1.7671
Saved models from epoch 1 to 'data/predictions/almnps_8857/thesaurus_affinity/ood/politics/newest.pt'.
Saved model with best loss 1.7671 to 'data/predictions/almnps_8857/thesaurus_affinity/ood/politics/best.pt'.
[Epoch 2/50] Train completed with Micro-f1: 0.6842, Macro-f1: 0.5158, Weighted-f1: 0.6448, Loss: 1.1981
[Epoch 2/50] Evaluation completed with Micro-f1: 0.5259, Macro-f1: 0.3511, Weighted-f1: 0.4830, Loss: 1.4787
Saved models from epoch 2 to 'data/predictions/almnps_8857/thesaurus_affinity/ood/politics/newest.pt'.
Saved model with best loss 1.4787 to 'data/predictions/almnps_8857/thesaurus_affinity/ood/politics/best.pt'.
[Epoch 3/50] Train completed with Micro-f1: 0.8164, Macro-f1: 0.7093, Weighted-f1: 0.7996, Loss: 0.7453
[Epoch 3/50] Evaluation completed with Micro-f1: 0.5472, Macro-f1: 0.3747, Weighted-f1: 0.5082, Loss: 1.3807
Saved models from epoch 3 to 'data/predictions/almnps_8857/thesaurus_affinity/ood/politics/newest.pt'.
Saved model with best loss 1.3807 to 'data/predictions/almnps_8857/thesaurus_affinity/ood/politics/best.pt'.
[Epoch 4/50] Train completed with Micro-f1: 0.8908, Macro-f1: 0.8200, Weighted-f1: 0.8802, Loss: 0.4904
[Epoch 4/50] Evaluation completed with Micro-f1: 0.5570, Macro-f1: 0.3860, Weighted-f1: 0.5202, Loss: 1.3715
Saved models from epoch 4 to 'data/predictions/almnps_8857/thesaurus_affinity/ood/politics/newest.pt'.
Saved model with best loss 1.3715 to 'data/predictions/almnps_8857/thesaurus_affinity/ood/politics/best.pt'.
[Epoch 5/50] Train completed with Micro-f1: 0.9332, Macro-f1: 0.8858, Weighted-f1: 0.9283, Loss: 0.3250
[Epoch 5/50] Evaluation completed with Micro-f1: 0.5653, Macro-f1: 0.3953, Weighted-f1: 0.5312, Loss: 1.3559
Saved models from epoch 5 to 'data/predictions/almnps_8857/thesaurus_affinity/ood/politics/newest.pt'.
Saved model with best loss 1.3559 to 'data/predictions/almnps_8857/thesaurus_affinity/ood/politics/best.pt'.
[Epoch 6/50] Train completed with Micro-f1: 0.9632, Macro-f1: 0.9425, Weighted-f1: 0.9592, Loss: 0.2206
[Epoch 6/50] Evaluation completed with Micro-f1: 0.5722, Macro-f1: 0.4036, Weighted-f1: 0.5388, Loss: 1.4020
Saved models from epoch 6 to 'data/predictions/almnps_8857/thesaurus_affinity/ood/politics/newest.pt'.
[Epoch 7/50] Train completed with Micro-f1: 0.9844, Macro-f1: 0.9691, Weighted-f1: 0.9831, Loss: 0.1438
[Epoch 7/50] Evaluation completed with Micro-f1: 0.5780, Macro-f1: 0.4094, Weighted-f1: 0.5461, Loss: 1.3949
Saved models from epoch 7 to 'data/predictions/almnps_8857/thesaurus_affinity/ood/politics/newest.pt'.
[Epoch 8/50] Train completed with Micro-f1: 0.9849, Macro-f1: 0.9752, Weighted-f1: 0.9839, Loss: 0.1003
[Epoch 8/50] Evaluation completed with Micro-f1: 0.5820, Macro-f1: 0.4140, Weighted-f1: 0.5516, Loss: 1.4361
Saved models from epoch 8 to 'data/predictions/almnps_8857/thesaurus_affinity/ood/politics/newest.pt'.
No improvement since 3 epochs (1.3559 loss). Early stop.
OOD training completed for test topic politics after 8 epochs.
Loaded <torch.utils.data.dataloader.DataLoader object at 0x7f98f4e23310> (train).
Loaded <torch.utils.data.dataloader.DataLoader object at 0x7f99c81c70a0> (dev).
Starting training on ['ai', 'literature', 'music', 'news', 'politics'] data.
Loaded <TransformerEmbeddings: dim=768>.
Using classifier: <LinearClassifier: emb_model = <TransformerEmbeddings: dim=768>>
Using criterion: <LabelLoss: loss=XEnt>.
Optimizing using: AdamW with learning rate 2e-05.
[Epoch 1/50] Train completed with Micro-f1: 0.3951, Macro-f1: 0.2276, Weighted-f1: 0.3258, Loss: 2.0821
[Epoch 1/50] Evaluation completed with Micro-f1: 0.5447, Macro-f1: 0.3642, Weighted-f1: 0.4963, Loss: 1.5949
Saved models from epoch 1 to 'data/predictions/almnps_8857/thesaurus_affinity/ood/science/newest.pt'.
Saved model with best loss 1.5949 to 'data/predictions/almnps_8857/thesaurus_affinity/ood/science/best.pt'.
[Epoch 2/50] Train completed with Micro-f1: 0.6905, Macro-f1: 0.5172, Weighted-f1: 0.6530, Loss: 1.1212
[Epoch 2/50] Evaluation completed with Micro-f1: 0.5652, Macro-f1: 0.3917, Weighted-f1: 0.5235, Loss: 1.3874
Saved models from epoch 2 to 'data/predictions/almnps_8857/thesaurus_affinity/ood/science/newest.pt'.
Saved model with best loss 1.3874 to 'data/predictions/almnps_8857/thesaurus_affinity/ood/science/best.pt'.
[Epoch 3/50] Train completed with Micro-f1: 0.8234, Macro-f1: 0.7146, Weighted-f1: 0.8057, Loss: 0.6972
[Epoch 3/50] Evaluation completed with Micro-f1: 0.5766, Macro-f1: 0.4071, Weighted-f1: 0.5399, Loss: 1.3197
Saved models from epoch 3 to 'data/predictions/almnps_8857/thesaurus_affinity/ood/science/newest.pt'.
Saved model with best loss 1.3197 to 'data/predictions/almnps_8857/thesaurus_affinity/ood/science/best.pt'.
[Epoch 4/50] Train completed with Micro-f1: 0.8890, Macro-f1: 0.7982, Weighted-f1: 0.8796, Loss: 0.4611
[Epoch 4/50] Evaluation completed with Micro-f1: 0.5847, Macro-f1: 0.4176, Weighted-f1: 0.5518, Loss: 1.3159
Saved models from epoch 4 to 'data/predictions/almnps_8857/thesaurus_affinity/ood/science/newest.pt'.
Saved model with best loss 1.3159 to 'data/predictions/almnps_8857/thesaurus_affinity/ood/science/best.pt'.
[Epoch 5/50] Train completed with Micro-f1: 0.9411, Macro-f1: 0.9002, Weighted-f1: 0.9373, Loss: 0.2922
[Epoch 5/50] Evaluation completed with Micro-f1: 0.5917, Macro-f1: 0.4270, Weighted-f1: 0.5616, Loss: 1.3515
Saved models from epoch 5 to 'data/predictions/almnps_8857/thesaurus_affinity/ood/science/newest.pt'.
[Epoch 6/50] Train completed with Micro-f1: 0.9698, Macro-f1: 0.9511, Weighted-f1: 0.9665, Loss: 0.1926
[Epoch 6/50] Evaluation completed with Micro-f1: 0.5973, Macro-f1: 0.4328, Weighted-f1: 0.5682, Loss: 1.3874
Saved models from epoch 6 to 'data/predictions/almnps_8857/thesaurus_affinity/ood/science/newest.pt'.
[Epoch 7/50] Train completed with Micro-f1: 0.9823, Macro-f1: 0.9645, Weighted-f1: 0.9798, Loss: 0.1332
[Epoch 7/50] Evaluation completed with Micro-f1: 0.6010, Macro-f1: 0.4392, Weighted-f1: 0.5747, Loss: 1.4726
Saved models from epoch 7 to 'data/predictions/almnps_8857/thesaurus_affinity/ood/science/newest.pt'.
No improvement since 3 epochs (1.3159 loss). Early stop.
OOD training completed for test topic science after 7 epochs.
TRAINING COMPLETED with:
	Domains:		['ai', 'literature', 'music', 'news', 'politics', 'science']
	OOD validaation:	True
	Mapping type:		thesaurus_affinity
