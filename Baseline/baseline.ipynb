{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'CrossRE/crossre_data/'\n",
    "topics = ['ai', 'literature', 'music', 'news', 'politics', 'science']\n",
    "label2idx = {'part-of':0, 'physical':1, 'usage':2, 'role':3, 'social':4, \n",
    "             'general-affiliation':5, 'compare':6, 'temporal':7, 'artifact':8, \n",
    "             'origin':9, 'topic':10, 'opposite':11, 'cause-effect':12,\n",
    "             'win-defeat':13, 'type-of':14, 'named':15, 'related-to':16}\n",
    "\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "import preprocessing\n",
    "\n",
    "from collections import defaultdict\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Data\n",
    "\n",
    "We Created our own data loader, identical to the one on the CrossRE projects, with the only difference being that we sample across all 6 domains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data = pd.DataFrame(columns=['doc_key', 'sentence', 'ner', 'relations'])\n",
    "# dev_data = pd.DataFrame(columns=['doc_key', 'sentence', 'ner', 'relations'])\n",
    "# test_data = pd.DataFrame(columns=['doc_key', 'sentence', 'ner', 'relations'])\n",
    "# for t in topics:\n",
    "#     train_data = pd.concat((train_data, pd.read_json(f'{data_path}{t}-train.json', lines=True)), axis=0, ignore_index=True)\n",
    "#     dev_data = pd.concat((dev_data, pd.read_json(f'{data_path}{t}-dev.json', lines=True)), axis=0, ignore_index=True)\n",
    "#     test_data = pd.concat((test_data, pd.read_json(f'{data_path}{t}-test.json', lines=True)), axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_data = preprocessing.prepare_data(f'{data_path}{topics[0]}-train.json', label2idx, 32)\n",
    "def get_all_crossre(data_path, topics, batch_size = 32, dataset='train'):\n",
    "    sentences, entities_1, entities_2, relations = [], [], [], []\n",
    "    for t in topics:\n",
    "        s, e_1, e_2, r = preprocessing.read_json_file(f'{data_path}{t}-{dataset}.json', label2idx)\n",
    "        sentences += s\n",
    "        entities_1 += e_1\n",
    "        entities_2 += e_2\n",
    "        relations += r\n",
    "\n",
    "    return DataLoader(preprocessing.DatasetMapper(sentences, entities_1, entities_2, relations), batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = get_all_crossre(data_path, topics, batch_size)\n",
    "dev_data = get_all_crossre(data_path, topics, batch_size, dataset='dev')\n",
    "# test_data = get_all_crossre(data_path, topics, dataset='test')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Predictions\n",
    "\n",
    "Once we had our own data loader working (see above) we had to make some changes to the scripts main.py & preprocessing.py to support the functionality and extra / different parameters of our data loader (i.e. we do not query data paths for train-dev-test but instead require path to data folder + list of topics + dataset name (\"train\", \"dev\" or \"test\"))\n",
    "\n",
    "We ran the following 2 commands while being within the CrossRE folder:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "python main.py --exp_path predictions/cross_domain --data_path crossre_data/ -rs 4012\n",
    "\n",
    "python main.py --exp_path predictions/cross_domain --data_path crossre_data/ -rs 4012 --prediction_only\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first command trains the model with default parameters (50 epochs, batch size 32 etc.)\n",
    "\n",
    "The second command finds the test data and produces predictions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
